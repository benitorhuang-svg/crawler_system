Directory structure:
└── crawler_system/
    ├── README.md
    ├── docker-compose-producer-network.yml
    ├── docker-compose-worker-network.yml
    ├── Dockerfile
    ├── genenv.py
    ├── local.ini
    ├── mysql-network.yml
    ├── pyproject.toml
    ├── rabbitmq-network.yml
    ├── requirements.txt
    ├── .python-version
    ├── crawler/
    │   ├── __init__.py
    │   ├── check_crawler_config.py
    │   ├── config.py
    │   ├── logging_config.py
    │   ├── worker.py
    │   ├── database/
    │   │   ├── connection.py
    │   │   ├── models.py
    │   │   ├── repository.py
    │   │   ├── schemas.py
    │   │   └── scripts/
    │   │       ├── fix_salary_data.py
    │   │       ├── get_category_ids.py
    │   │       ├── pandas_sql_config.py
    │   │       └── temp_count_db.py
    │   ├── geocoding/
    │   │   ├── client.py
    │   │   └── task.py
    │   ├── project_104/
    │   │   ├── 104人力銀行_crawl.ipynb
    │   │   ├── client_104.py
    │   │   ├── config_104.py
    │   │   ├── local_fetch_104_url_data.py
    │   │   ├── page_api_data_104.txt
    │   │   ├── parser_apidata_104.py
    │   │   ├── producer_category_104.py
    │   │   ├── producer_jobs_104.py
    │   │   ├── producer_urls_104.py
    │   │   ├── single_url_api_data_104.py
    │   │   ├── single_url_api_data_104.txt
    │   │   ├── task_category_104.py
    │   │   ├── task_jobs_104.py
    │   │   └── task_urls_104.py
    │   ├── project_1111/
    │   │   ├── 1111_人力銀行_crawl.ipynb
    │   │   ├── client_1111.py
    │   │   ├── config_1111.py
    │   │   ├── page_api_data_1111.txt
    │   │   ├── parser_apidata_1111.py
    │   │   ├── producer_category_1111.py
    │   │   ├── producer_jobs_1111.py
    │   │   ├── producer_urls_1111.py
    │   │   ├── task_category_1111.py
    │   │   ├── task_jobs_1111.py
    │   │   └── task_urls_1111.py
    │   ├── project_cakeresume/
    │   │   ├── cake_me_crawl.ipynb
    │   │   ├── Cake_me_jobcat_json.txt
    │   │   ├── client_cakeresume.py
    │   │   ├── config_cakeresume.py
    │   │   ├── demo_platform_cakeresume_tasks.py
    │   │   ├── producer_category_cakeresume.py
    │   │   ├── producer_jobs_cakeresume.py
    │   │   ├── producer_urls_cakeresume.py
    │   │   ├── task_category_cakeresume.py
    │   │   ├── task_jobs_cakeresume.py
    │   │   └── task_urls_cakeresume.py
    │   ├── project_yes123/
    │   │   ├── client_yes123.py
    │   │   ├── config_yes123.py
    │   │   ├── producer_category_yes123.py
    │   │   ├── producer_jobs_yes123.py
    │   │   ├── producer_urls_yes123.py
    │   │   ├── task_category_yes123.py
    │   │   ├── task_jobs_yes123.py
    │   │   ├── task_urls_yes123.py
    │   │   └── yes123_人力銀行_crawl.ipynb
    │   └── utils/
    │       └── salary_parser.py
    └── docs/
        ├── development_manual.md
        ├── project_104_docker_manual.md
        ├── project_104_local_test_plan.md
        ├── project_1111_local_test_plan.md
        ├── project_cakeresume_local_test_plan.md
        └── project_yes123_local_test_plan.md

================================================
FILE: README.md
================================================
# crawler

# 環境設定

#### 安裝 uv
<!-- https://docs.astral.sh/uv/getting-started/installation/#standalone-installer -->
    curl -LsSf https://astral.sh/uv/install.sh | sh


#### 建立初始化環境
   uv init
   uv venv


#### 安裝 repo 套件
    uv add <package name>
    uv add ruff


#### 將專案套件化 / 輸出到 requirements.txt
    uv pip install -e .
    uv pip compile pyproject.toml -o requirements.txt


#### 建立環境變數
    ENV=DEV python genenv.py
    ENV=DOCKER python genenv.py
    ENV=PRODUCTION python genenv.py


#### 排版
    ruff check
    ruff check --fix


# 測試 rabbitmq docker image
    docker compose -f rabbitmq-network.yml up -d
        - rabbitmq UI : http://localhost:15672 worker worker
        - flowler UI  : http://localhost:5555
    docker compose -f rabbitmq-network.yml down


#### 查看 docker container 服務運作狀況
    docker ps -a

#### 查看 服務 log
    docker logs container_name    


# 本地執行檔案測試
### Worker 啟動預設執行 celery 的 queue 的工人
    uv run celery -A crawler.worker worker --loglevel=info
    uv run celery -A crawler.worker worker -Q crawler_category_104 --loglevel=info
### Producer 發送任務
    uv run python crawler/project_104/producer_104_jobs.py

<!-- 任務流程：producer 負責生成並分發任務，而 worker 負責接收並執行這些任務 -->
     python -m crawler.worker
     python -m crawler.project_104.producer_category_104




# 建立專案的 Docker image

####  docker image build / push
    docker build -f Dockerfile -t benitorhuang/crawler_jobs:0.0.1 .
    docker push benitorhuang/crawler_jobs:0.0.1



# docker compose 容器網路連線
<!-- 建立 docker image 連線通道 -->
### 測試 docker compose : rabbitmq、 worker 和 producer 的網路連線。
docker compose -f rabbitmq-network.yml up -d
docker compose -f docker-compose-worker-network.yml up -d
docker compose -f docker-compose-producer-network.yml up -d



#### 建立 network
    docker network create my_network

# 建立 mysql service / 啟動 / 關閉 / 上傳資料
    docker compose -f mysql.yml up -d
    docker compose -f mysql.yml down
    python -m crawler.database.test_upload_data_to_mysql
    python -m crawler.database.test_upload_duplicate_data


####   worker/producer  <啟動 / 關閉>
    celery -A crawler.worker worker --loglevel=info

    ENV=DEV python genenv.py
    python -m crawler.project_104.task_category_104
    python -m crawler.project_104.task_urls_104
    python -m crawler.project_104.task_jobs_104


    docker compose -f docker-compose-worker-network.yml up -d
    docker compose -f docker-compose-worker-network.yml down
    docker compose -f docker-compose-producer-network.yml up -d
    docker compose -f docker-compose-producer-network.yml down
    
#### 加上環境版本 DOCKER_IMAGE_VERSION=***
    DOCKER_IMAGE_VERSION=0.0.1 docker compose -f docker-compose-worker-network-version.yml up -d
    DOCKER_IMAGE_VERSION=0.0.1 docker compose -f docker-compose-worker-network-version.yml down
    DOCKER_IMAGE_VERSION=0.0.3 docker compose -f docker-compose-producer-network-version.yml up -d
    DOCKER_IMAGE_VERSION=0.0.3 docker compose -f docker-compose-producer-network-version.yml down




================================================
FILE: docker-compose-producer-network.yml
================================================
# version: '3.0'

services:
  producer_104_jobs:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_jobs_producer"
    command: python -m crawler.project_104.producer_jobs_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

  producer_104_category:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_category_producer"
    command: python -m crawler.project_104.producer_category_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

  producer_104_urls:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_urls_producer"
    command: python -m crawler.project_104.producer_urls_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

networks:
  my_network:
    external: true


================================================
FILE: docker-compose-worker-network.yml
================================================
# version: '3.0'  # 使用 Docker Compose 的版本 3.0，適合大部分部署場景

services:
  crawler_104:  # 定義一個服務，名稱為 crawler_twse
    image: benitorhuang/crawler_jobs:0.0.2 

    hostname: "crawler_104_category"
    command: celery -A crawler.worker worker --loglevel=info --hostname=%h -Q producer_jobs_104,producer_urls_104,producer_category_104  
    # 啟動容器後執行的命令，這裡是啟動 Celery worker，指定 app 為 crawler.worker，設定日誌等級為 info，
    # 使用主機名稱當作 worker 名稱（%h），並將此 worker 加入名為 "twse" 的任務佇列 (queue)

    restart: always  # 若容器停止或崩潰，自動重新啟動
    environment:
      - TZ=Asia/Taipei  # 設定時區為台北（UTC+8）
    networks:
      - my_network  # 將此服務連接到 my_network 網路

networks:
  my_network:
    # 加入已經存在的網路
    external: true



================================================
FILE: Dockerfile
================================================
# Stage 1: Builder
FROM python:3.13-slim-bullseye AS builder

WORKDIR /app

RUN apt-get update && apt-get install -y build-essential curl && rm -rf /var/lib/apt/lists/*

COPY requirements.txt uv.lock .
RUN curl -LsSf https://astral.sh/uv/install.sh | sh && /root/.local/bin/uv pip install -r requirements.txt --system


# Stage 2: Runner
FROM python:3.13-slim-bullseye AS runner

WORKDIR /app

# Copy only the installed packages from the builder stage
COPY --from=builder /usr/local/lib/python3.13/site-packages /usr/local/lib/python3.13/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copy the application code
COPY . .

ENV PYTHONPATH="/app"
ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8

CMD ["/bin/bash"]


================================================
FILE: genenv.py
================================================
import os
from configparser import ConfigParser
import structlog
import sys

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)


def generate_env_file():
    config_path = "local.ini"

    if not os.path.exists(config_path):
        logger.critical(
            "local.ini not found. Please ensure it exists in the project root.",
            path=config_path,
        )
        sys.exit(1)

    local_config = ConfigParser()
    try:
        local_config.read(config_path)
    except Exception as e:
        logger.critical(
            "Failed to read local.ini configuration file.",
            path=config_path,
            error=e,
            exc_info=True,
        )
        sys.exit(1)

    # Determine which section to use based on APP_ENV environment variable
    app_env = os.environ.get("APP_ENV", "").upper()

    selected_section_name = "DEFAULT"  # Default fallback
    if app_env and app_env in local_config:
        selected_section_name = app_env
    elif "DEFAULT" not in local_config:
        logger.critical(
            "Neither APP_ENV specified section nor 'DEFAULT' section found in local.ini.",
            app_env=app_env,
        )
        sys.exit(1)

    section = local_config[selected_section_name]
    logger.info("Using configuration section.", section_name=selected_section_name)

    env_content = ""
    for key, value in section.items():
        env_content += f"{key.upper()}={value}\n"

    env_file_path = ".env"
    try:
        with open(env_file_path, "w", encoding="utf8") as env_file:
            env_file.write(env_content)
        logger.info(
            ".env file generated successfully.",
            path=env_file_path,
            section_used=selected_section_name,
        )
    except Exception as e:
        logger.critical(
            "Failed to write .env file.", path=env_file_path, error=e, exc_info=True
        )
        sys.exit(1)


if __name__ == "__main__":
    generate_env_file()



================================================
FILE: local.ini
================================================
# dev 環境
[DEV]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = 127.0.0.1
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = 127.0.0.1
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = console
# Producer Batching
PRODUCER_BATCH_SIZE = 20
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5


# Docker 環境
[DOCKER]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = rabbitmq
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = localhost
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = console
# Producer Batching
PRODUCER_BATCH_SIZE = 20
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5


# Prod 環境
[PRODUCTION]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = rabbitmq
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = 127.0.0.1
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = json
# Producer Batching
PRODUCER_BATCH_SIZE = 20
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5



================================================
FILE: mysql-network.yml
================================================
version: '3.8'

services:
  crawler_jobs_mysql:
    image: mysql:8.0
    # 設定 mysql 使用原生認證的密碼 hash
    command: mysqld --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_ACCOUNT: ${MYSQL_ACCOUNT}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    ports:
      - "3306:3306"

    restart: always
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - my_network



  crawler_jobs_phpmyadmin:
    image: phpmyadmin/phpmyadmin
    links: 
          - crawler_jobs_mysql:db
    restart: always
    ports:
      - "8080:80"
    networks:
      - my_network

networks:
  my_network:
    external: true

volumes:
  mysql_data:


================================================
FILE: pyproject.toml
================================================
[project]
name = "crawler-jobs"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "celery>=5.5.3",
    "ruff>=0.12.5",
    "structlog>=25.4.0",
    "python-dotenv",
    "requests>=2.32.4",
    "pymysql==1.1.0",
    "tenacity>=8.2.3",
    "pydantic>=2.0.0",
    "sqlalchemy>=2.0.41",
    "gitingest>=0.1.5",
    "pandas>=2.3.1",
    "pytest>=8.4.1",
    "beautifulsoup4",
    "lxml",
]

[tool.ruff]
exclude = ["*.ipynb"]

[tool.pytest.ini_options]
python_files = "pytest_*.py"


================================================
FILE: rabbitmq-network.yml
================================================
version: '3'
services:

  rabbitmq:
    # 使用 RabbitMQ 官方管理版的輕量 Alpine 版本映像檔
    image: 'rabbitmq:3.12-management-alpine'
    restart: always  # 若容器停止或崩潰，自動重新啟動
    ports: 
      - '5672:5672'       # 對外開放 RabbitMQ 的 AMQP 通訊埠（應用程式通訊埠）
      - '15672:15672'     # 對外開放 RabbitMQ 的管理介面（Web UI）埠口
    environment:
      RABBITMQ_DEFAULT_USER: "worker"       # 預設使用者名稱設定為 worker
      RABBITMQ_DEFAULT_PASS: "worker"       # 預設密碼設定為 worker
      RABBITMQ_DEFAULT_VHOST: "/"            # 預設虛擬主機 (Virtual Host)，用於隔離不同環境的訊息隊列
    networks:
      - my_network                           # 將服務加入名為 my_network 的自訂網路

  flower:
    # 使用 Flower 映像來監控 Celery 的任務佇列狀況
    image: mher/flower:0.9.5
    command: ["flower", "--broker=amqp://worker:worker@rabbitmq", "--port=5555"]  
    restart: always  # 若容器停止或崩潰，自動重新啟動
    # 啟動 Flower，設定 RabbitMQ 為 broker，並監聽 5555 埠口
    ports: 
      - 5555:5555                           # 映射 Flower 的監控介面埠口到宿主機
    depends_on:
      - rabbitmq                           # 確保 RabbitMQ 先啟動後，Flower 再啟動
    networks:
      - my_network                         # 將服務加入 my_network 網路

networks:
  my_network:
    # 加入已經存在的網路
    external: true


================================================
FILE: requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    uv pip compile pyproject.toml -o requirements.txt
amqp==5.3.1
    # via kombu
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via
    #   httpx
    #   starlette
    #   watchfiles
beautifulsoup4==4.13.4
    # via crawler-jobs (pyproject.toml)
billiard==4.2.1
    # via celery
celery==5.5.3
    # via crawler-jobs (pyproject.toml)
certifi==2025.7.14
    # via
    #   httpcore
    #   httpx
    #   requests
    #   sentry-sdk
charset-normalizer==3.4.2
    # via requests
click==8.2.1
    # via
    #   celery
    #   click-didyoumean
    #   click-plugins
    #   click-repl
    #   gitingest
    #   rich-toolkit
    #   typer
    #   uvicorn
click-didyoumean==0.3.1
    # via celery
click-plugins==1.1.1.2
    # via celery
click-repl==0.3.0
    # via celery
deprecated==1.2.18
    # via limits
dnspython==2.7.0
    # via email-validator
email-validator==2.2.0
    # via
    #   fastapi
    #   pydantic
fastapi==0.116.1
    # via gitingest
fastapi-cli==0.0.8
    # via fastapi
fastapi-cloud-cli==0.1.5
    # via fastapi-cli
gitingest==0.1.5
    # via crawler-jobs (pyproject.toml)
greenlet==3.2.3
    # via sqlalchemy
h11==0.16.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.9
    # via httpx
httptools==0.6.4
    # via uvicorn
httpx==0.28.1
    # via
    #   fastapi
    #   fastapi-cloud-cli
idna==3.10
    # via
    #   anyio
    #   email-validator
    #   httpx
    #   requests
iniconfig==2.1.0
    # via pytest
jinja2==3.1.6
    # via fastapi
kombu==5.5.4
    # via celery
limits==5.4.0
    # via slowapi
lxml==6.0.0
    # via crawler-jobs (pyproject.toml)
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
numpy==2.3.2
    # via pandas
packaging==25.0
    # via
    #   kombu
    #   limits
    #   pytest
pandas==2.3.1
    # via crawler-jobs (pyproject.toml)
pathspec==0.12.1
    # via gitingest
pluggy==1.6.0
    # via pytest
prompt-toolkit==3.0.51
    # via click-repl
pydantic==2.11.7
    # via
    #   crawler-jobs (pyproject.toml)
    #   fastapi
    #   fastapi-cloud-cli
    #   gitingest
pydantic-core==2.33.2
    # via pydantic
pygments==2.19.2
    # via
    #   pytest
    #   rich
pymysql==1.1.0
    # via crawler-jobs (pyproject.toml)
pytest==8.4.1
    # via crawler-jobs (pyproject.toml)
python-dateutil==2.9.0.post0
    # via
    #   celery
    #   pandas
python-dotenv==1.1.1
    # via
    #   crawler-jobs (pyproject.toml)
    #   gitingest
    #   uvicorn
python-multipart==0.0.20
    # via fastapi
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via uvicorn
regex==2024.11.6
    # via tiktoken
requests==2.32.4
    # via
    #   crawler-jobs (pyproject.toml)
    #   tiktoken
rich==14.1.0
    # via
    #   rich-toolkit
    #   typer
rich-toolkit==0.14.9
    # via
    #   fastapi-cli
    #   fastapi-cloud-cli
rignore==0.6.4
    # via fastapi-cloud-cli
ruff==0.12.5
    # via crawler-jobs (pyproject.toml)
sentry-sdk==2.33.2
    # via fastapi-cloud-cli
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
slowapi==0.1.9
    # via gitingest
sniffio==1.3.1
    # via anyio
soupsieve==2.7
    # via beautifulsoup4
sqlalchemy==2.0.41
    # via crawler-jobs (pyproject.toml)
starlette==0.47.2
    # via
    #   fastapi
    #   gitingest
structlog==25.4.0
    # via crawler-jobs (pyproject.toml)
tenacity==9.1.2
    # via crawler-jobs (pyproject.toml)
tiktoken==0.9.0
    # via gitingest
tomli==2.2.1
    # via gitingest
typer==0.16.0
    # via
    #   fastapi-cli
    #   fastapi-cloud-cli
typing-extensions==4.14.1
    # via
    #   beautifulsoup4
    #   fastapi
    #   limits
    #   pydantic
    #   pydantic-core
    #   rich-toolkit
    #   sqlalchemy
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via pydantic
tzdata==2025.2
    # via
    #   kombu
    #   pandas
urllib3==2.5.0
    # via
    #   requests
    #   sentry-sdk
uvicorn==0.35.0
    # via
    #   fastapi
    #   fastapi-cli
    #   fastapi-cloud-cli
    #   gitingest
uvloop==0.21.0
    # via uvicorn
vine==5.1.0
    # via
    #   amqp
    #   celery
    #   kombu
watchfiles==1.1.0
    # via uvicorn
wcwidth==0.2.13
    # via prompt-toolkit
websockets==15.0.1
    # via uvicorn
wrapt==1.17.2
    # via deprecated



================================================
FILE: .python-version
================================================
3.13



================================================
FILE: crawler/__init__.py
================================================




================================================
FILE: crawler/check_crawler_config.py
================================================
import structlog

from .logging_config import configure_logging
from .config import (
    RABBITMQ_HOST,
    RABBITMQ_PORT,
)

configure_logging()

logger = structlog.get_logger(__name__)  # Corrected: add __name__

logger.info(
    "RabbitMQ configuration check.",  # Improved log message
    rabbitmq_host=RABBITMQ_HOST,
    rabbitmq_port=RABBITMQ_PORT,
    worker_account="***masked***",  # Masked sensitive info
    worker_password="***masked***",  # Masked sensitive info
)



================================================
FILE: crawler/config.py
================================================
import os
import configparser
import structlog

logger = structlog.get_logger(__name__)

config = configparser.ConfigParser()
config_path = os.path.join(os.path.dirname(__file__), "..", "local.ini")

try:
    config.read(config_path)
except Exception as e:
    logger.critical(f"無法讀取 local.ini 設定檔: {e}", exc_info=True)
    raise RuntimeError("無法讀取設定檔。") from e

# Determine which section to use based on APP_ENV environment variable
# Default to 'DOCKER' if APP_ENV is not set or invalid
app_env = os.environ.get("APP_ENV", "DOCKER").upper()
if app_env not in config:
    logger.warning(
        f"環境變數 APP_ENV={app_env} 無效或未找到對應區塊，預設使用 [DOCKER] 設定。"
    )
    app_env = "DOCKER"

config_section = config[app_env]

WORKER_ACCOUNT = config_section.get("WORKER_ACCOUNT", "worker")
WORKER_PASSWORD = config_section.get("WORKER_PASSWORD", "worker")

RABBITMQ_HOST = config_section.get("RABBITMQ_HOST", "127.0.0.1")
RABBITMQ_PORT = int(config_section.get("RABBITMQ_PORT", "5672"))

MYSQL_HOST = config_section.get("MYSQL_HOST", "crawler_jobs_mysql")
MYSQL_PORT = int(config_section.get("MYSQL_PORT", "3306"))
MYSQL_ACCOUNT = config_section.get("MYSQL_ACCOUNT", "root")
MYSQL_ROOT_PASSWORD = config_section.get("MYSQL_ROOT_PASSWORD", "root_password")
MYSQL_PASSWORD = config_section.get("MYSQL_PASSWORD", "root_password")
MYSQL_DATABASE = os.environ.get('CRAWLER_DB_NAME') or config_section.get("MYSQL_DATABASE", "crawler_db")
LOG_LEVEL = config_section.get("LOG_LEVEL", "INFO").upper()
LOG_FORMATTER = config_section.get("LOG_FORMATTER", "json").lower()

PRODUCER_BATCH_SIZE = int(config_section.get("PRODUCER_BATCH_SIZE", "100"))
PRODUCER_DISPATCH_INTERVAL_SECONDS = float(
    config_section.get("PRODUCER_DISPATCH_INTERVAL_SECONDS", "1.0")
)

URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = int(
    config_section.get("URL_CRAWLER_REQUEST_TIMEOUT_SECONDS", "20")
)
URL_CRAWLER_UPLOAD_BATCH_SIZE = int(
    config_section.get("URL_CRAWLER_UPLOAD_BATCH_SIZE", "30")
)
URL_CRAWLER_SLEEP_MIN_SECONDS = float(
    config_section.get("URL_CRAWLER_SLEEP_MIN_SECONDS", "0.5")
)
URL_CRAWLER_SLEEP_MAX_SECONDS = float(
    config_section.get("URL_CRAWLER_SLEEP_MAX_SECONDS", "1.5")
)



================================================
FILE: crawler/logging_config.py
================================================
import logging
import structlog
import sys

# 從集中的設定模組導入日誌級別和格式化工具
from crawler.config import LOG_LEVEL, LOG_FORMATTER


def configure_logging():
    """
    配置應用程式的日誌系統，整合 structlog 和標準 logging。
    日誌級別和格式化工具從 crawler.config 獲取。
    """
    numeric_log_level = getattr(logging, LOG_LEVEL, logging.INFO)

    # 檢查是否已經配置過，避免重複添加 handler
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        # 1. 配置 structlog 的處理器鏈
        #    - TimeStamper: 添加時間戳
        #    - add_logger_name, add_log_level: 添加日誌器名稱和級別
        #    - wrap_for_formatter: 為標準庫的 formatter 準備
        processors = [
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ]
        structlog.configure(
            processors=processors,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # 2. 根據 LOG_FORMATTER 選擇渲染器
        #    - console: 美觀、適合開發但效能較差
        #    - key_value: 結構化、易讀且效能好
        #    - json: 機器可讀、效能最佳，適合生產環境
        if LOG_FORMATTER == "console":
            renderer = structlog.dev.ConsoleRenderer()
        elif LOG_FORMATTER == "key_value":
            renderer = structlog.processors.KeyValueRenderer(key_order=['timestamp', 'level', 'event'])
        else:
            renderer = structlog.processors.JSONRenderer()

        # 3. 配置標準 logging 的 formatter 和 handler
        formatter = structlog.stdlib.ProcessorFormatter(
            processor=renderer,
            foreign_pre_chain=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.processors.TimeStamper(fmt="iso"),
            ],
        )

        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)

        # 4. 配置 root logger
        root_logger.addHandler(handler)
        root_logger.setLevel(numeric_log_level)

        logger = structlog.get_logger(__name__)
        logger.info(
            "日誌系統配置完成",
            configured_level=LOG_LEVEL,
            formatter=LOG_FORMATTER,
        )



================================================
FILE: crawler/worker.py
================================================
from celery import Celery
import structlog
import logging # Import logging module
import sys # Import sys module

from crawler.config import (
    RABBITMQ_HOST,
    RABBITMQ_PORT,
    WORKER_ACCOUNT,
    WORKER_PASSWORD,
    LOG_LEVEL, # Import LOG_LEVEL
)

# configure_logging() # Removed direct call
logger = structlog.get_logger(__name__)

logger.info(
    "RabbitMQ configuration",
    rabbitmq_host=RABBITMQ_HOST,
    rabbitmq_port=RABBITMQ_PORT,
    worker_account="***masked***",
    worker_password="***masked***",
)

app = Celery(
    "task",
    include=[
        "crawler.project_104.task_category_104",
        "crawler.project_104.task_jobs_104",
        "crawler.project_104.task_urls_104",
    ],
    # Configure broker connection settings for robustness
    broker_connection_retry_on_startup=True,
    broker_connection_max_retries=10,
    broker_connection_timeout=30,
)

# Set the broker URL using app.conf
app.conf.broker_url = (
    f"pyamqp://{WORKER_ACCOUNT}:{WORKER_PASSWORD}@{RABBITMQ_HOST}:{RABBITMQ_PORT}/"
)

# Initialize database when Celery app is ready
@app.on_after_configure.connect
def setup_database_connection(sender, **kwargs):
    from crawler.database.connection import initialize_database
    initialize_database()
    logger.info("Celery app configured and database initialized.")

# Configure Celery's logging to use structlog
@app.on_after_configure.connect
def setup_logging(sender, **kwargs):
    # 確保只配置一次
    if not logging.getLogger().handlers:
        # 配置 structlog 的處理器鏈
        structlog.configure(
            processors=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
            ],
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # 配置標準 logging 的 formatter 和 handler
        formatter = structlog.stdlib.ProcessorFormatter(
            processor=structlog.dev.ConsoleRenderer(),
            foreign_pre_chain=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
            ],
        )

        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)

        # 配置 root logger
        root_logger = logging.getLogger()
        root_logger.addHandler(handler)
        root_logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

        # 確保 Celery 自己的日誌也通過 structlog 處理
        logging.getLogger('celery').addHandler(handler)
        logging.getLogger('celery').setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

        logger.info("Celery 日誌系統配置完成", configured_level=LOG_LEVEL)

app.conf.task_routes = {
    "crawler.project_104.task_jobs_104.fetch_url_data_104": {"queue": "producer_jobs_104"},
    "crawler.project_104.task_urls_104.crawl_and_store_category_urls": {
        "queue": "producer_urls_104"
    },
    "crawler.project_104.task_category_104.fetch_url_data_104": {"queue": "producer_category_104"},
}



================================================
FILE: crawler/database/connection.py
================================================
import os
import logging
import structlog
from contextlib import contextmanager

from tenacity import retry, stop_after_attempt, wait_exponential, before_log, RetryError
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from crawler.config import (
    MYSQL_HOST,
    MYSQL_PORT,
    MYSQL_ACCOUNT,
    MYSQL_PASSWORD,
    MYSQL_DATABASE as DEFAULT_DB_NAME,
)
from crawler.database.models import Base

logger = structlog.get_logger(__name__)
metadata = Base.metadata
_engine = None  # Singleton engine instance

SessionLocal = sessionmaker(autocommit=False, autoflush=False)


def get_db_name() -> str:
    """
    Determines the database name to use.
    It prioritizes the CRAWLER_DB_NAME environment variable for testing purposes.
    Otherwise, it falls back to the default database name from the config.
    """
    return os.environ.get('CRAWLER_DB_NAME', DEFAULT_DB_NAME)


@contextmanager
def get_session():
    """
    Provides a transactional database session via a context manager.
    Handles commit, rollback, and closing automatically.
    """
    engine = get_engine()  # Ensure engine is initialized
    SessionLocal.configure(bind=engine)
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        logger.error("Session encountered an error, performing rollback.", exc_info=True)
        session.rollback()
        raise
    finally:
        session.close()


def get_engine():
    """
    Retrieves the SQLAlchemy engine instance, creating it if it doesn't exist.
    This is a singleton to ensure one engine instance per application lifecycle.
    """
    global _engine
    if _engine is None:
        try:
            _engine = _connect_with_retry()
        except RetryError as e:
            logger.critical(
                "Database connection failed after multiple retries. Application cannot start.",
                error=e,
                exc_info=True,
            )
            raise RuntimeError("Database connection failed. Please check the database service.") from e
        except Exception as e:
            logger.critical(
                "An unexpected error occurred while creating the database engine.",
                error=e,
                exc_info=True,
            )
            raise RuntimeError("Fatal error creating the database engine.") from e
    return _engine


@retry(
    stop=stop_after_attempt(8),
    wait=wait_exponential(multiplier=1, min=2, max=20),
    before=before_log(logger, logging.INFO),
    reraise=True,
)
def _connect_with_retry() -> create_engine:
    """
    (Internal) Performs the actual database connection with retry logic.
    """
    db_name = get_db_name()
    logger.info(f"Attempting to connect to database: {db_name}@{MYSQL_HOST}:{MYSQL_PORT}")

    db_url = (
        f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{db_name}?charset=utf8mb4"
    )

    engine = create_engine(
        db_url,
        pool_recycle=3600,
        echo=False,
        connect_args={"connect_timeout": 10},
        isolation_level="READ COMMITTED",
    )

    # Test the connection; this will trigger tenacity's retry if it fails
    with engine.connect() as conn:
        conn.execute(text("SELECT 1"))

    logger.info("Database engine created successfully, connection test passed.")
    return engine


def initialize_database():
    """
    Initializes the database. If the target is 'test_db', it ensures
    the database exists before creating tables. For other databases,
    it simply creates tables based on the models.
    """
    db_name = get_db_name()
    logger.info(f"Initializing database: {db_name}")

    # If using the test database, ensure it exists first.
    if db_name == 'test_db':
        server_db_url = (
            f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
            f"{MYSQL_HOST}:{MYSQL_PORT}/?charset=utf8mb4"
        )
        server_engine = create_engine(server_db_url)
        try:
            with server_engine.connect() as connection:
                connection.execute(text(f"CREATE DATABASE IF NOT EXISTS {db_name};"))
                connection.commit()
            logger.info(f"Ensured database '{db_name}' exists.")
        finally:
            server_engine.dispose()

    # Now, connect to the specific database and create all tables
    try:
        engine = get_engine()
        metadata.create_all(engine)
        logger.info(f"Database tables for '{db_name}' initialized successfully.")
    except Exception as e:
        logger.critical(
            f"Failed to initialize tables for database '{db_name}'.",
            error=e,
            exc_info=True,
        )
        raise


================================================
FILE: crawler/database/models.py
================================================
from sqlalchemy import Column, Integer, String, Text, DateTime, Enum, ForeignKey, UniqueConstraint
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime, timezone

from crawler.database.schemas import (
    SourcePlatform,
    JobStatus,
    CrawlStatus,
    SalaryType,
    JobType,
)

Base = declarative_base()


# SQLAlchemy Models
class CategorySource(Base):
    __tablename__ = "tb_category_source"
    id = Column(Integer, primary_key=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False)
    source_category_id = Column(String(255), nullable=False, unique=True)
    source_category_name = Column(String(255), nullable=False)
    parent_source_id = Column(String(255))

    url_associations = relationship("UrlCategory", back_populates="category")


class Url(Base):
    __tablename__ = "tb_urls"
    source_url = Column(String(512), primary_key=True)
    source = Column(Enum(SourcePlatform), nullable=False, index=True)
    status = Column(
        Enum(JobStatus), nullable=False, index=True, default=JobStatus.ACTIVE
    )
    details_crawl_status = Column(
        String(20), nullable=False, index=True, default=CrawlStatus.PENDING.value
    )
    crawled_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    details_crawled_at = Column(DateTime)

    category_associations = relationship("UrlCategory", back_populates="url")


class UrlCategory(Base):
    __tablename__ = "tb_url_categories"
    source_url = Column(String(512), ForeignKey("tb_urls.source_url"), primary_key=True)
    source_category_id = Column(String(255), ForeignKey("tb_category_source.source_category_id"), primary_key=True)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc), nullable=False)

    url = relationship("Url", back_populates="category_associations")
    category = relationship("CategorySource", back_populates="url_associations")


class Job(Base):
    __tablename__ = "tb_jobs"
    id = Column(Integer, primary_key=True, autoincrement=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False, index=True)
    source_job_id = Column(String(255), index=True, nullable=False)
    url = Column(String(512), index=True, nullable=False)
    status = Column(Enum(JobStatus), nullable=False)
    title = Column(String(255), nullable=False)
    description = Column(Text)
    job_type = Column(Enum(JobType))
    location_text = Column(String(255))
    posted_at = Column(DateTime)
    salary_text = Column(String(255))
    salary_min = Column(Integer)
    salary_max = Column(Integer)
    salary_type = Column(Enum(SalaryType))
    experience_required_text = Column(String(255))
    education_required_text = Column(String(255))
    company_source_id = Column(String(255))
    company_name = Column(String(255))
    company_url = Column(String(512))
    created_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (UniqueConstraint('source_platform', 'url', name='_source_platform_url_uc'),)


class JobLocation(Base):
    __tablename__ = "tb_job_locations"
    id = Column(Integer, primary_key=True, autoincrement=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False, index=True)
    source_job_id = Column(String(255), nullable=False, index=True)
    latitude = Column(String(255))
    longitude = Column(String(255))
    created_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (UniqueConstraint('source_platform', 'source_job_id', name='_source_platform_job_id_uc'),)


================================================
FILE: crawler/database/repository.py
================================================
import structlog
from typing import List, Dict, Any, Optional, Set
from datetime import datetime, timezone, timedelta
import pandas as pd

from sqlalchemy import select, update
from sqlalchemy.dialects.mysql import insert
from sqlalchemy.orm import DeclarativeBase

from crawler.database.connection import get_session

from crawler.database.models import (
    CategorySource,
    Url,
    Job,
    UrlCategory,
    JobLocation,
)
from crawler.database.schemas import (
    SourcePlatform,
    JobStatus,
    CrawlStatus,
    JobPydantic,
    CategorySourcePydantic,
    JobLocationPydantic,
)

logger = structlog.get_logger(__name__)


def _generic_upsert(
    model: DeclarativeBase, data_list: List[Dict[str, Any]], update_columns: List[str]
) -> int:
    """
    通用的 UPSERT 函式，用於將數據同步到資料庫。
    如果記錄已存在則更新指定欄位；否則插入新記錄。
    """
    if not data_list:
        return 0

    with get_session() as session:
        stmt = insert(model).values(data_list)
        
        if update_columns: # 只有當有需要更新的欄位時才構建 update_dict
            update_dict = {col: getattr(stmt.inserted, col) for col in update_columns}
            stmt = stmt.on_duplicate_key_update(**update_dict)
        else:
            # 對於沒有額外更新欄位的 UPSERT (例如只有複合主鍵的關聯表)，
            # 仍然需要呼叫 on_duplicate_key_update 以觸發 ON DUPLICATE KEY 行為
            # 這裡選擇更新第一個非主鍵欄位為其自身，以滿足語法要求但不實際更新數據
            # 如果沒有非主鍵欄位，則選擇第一個主鍵欄位
            first_column_name = next(iter(model.__table__.columns)).name
            stmt = stmt.on_duplicate_key_update(**{first_column_name: getattr(stmt.inserted, first_column_name)})

        result = session.execute(stmt)
        return result.rowcount # 返回受影響的行數








def sync_source_categories(
    platform: SourcePlatform, flattened_data: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    將抓取到的職務分類數據同步到資料庫。
    執行 UPSERT 操作，如果分類已存在則更新，否則插入。
    """
    if not flattened_data:
        logger.info(
            "No flattened data to sync for categories.", platform=platform.value
        )
        return {"total": 0, "affected": 0}

    update_cols = ["source_category_name", "parent_source_id"]
    affected_rows = _generic_upsert(CategorySource, flattened_data, update_cols)

    logger.info(
        "Categories synced successfully.",
        platform=platform.value,
        total_categories=len(flattened_data),
        affected_rows=affected_rows,
    )
    return {"total": len(flattened_data), "affected": affected_rows}


def get_source_categories(
    platform: SourcePlatform, source_ids: Optional[List[str]] = None
) -> List[CategorySourcePydantic]:
    """
    從資料庫獲取指定平台和可選的 source_ids 的職務分類。
    返回 CategorySourcePydantic 實例列表，以便在 Session 關閉後安全使用。
    """
    with get_session() as session:
        stmt = select(CategorySource).where(CategorySource.source_platform == platform)
        if source_ids:
            stmt = stmt.where(CategorySource.source_category_id.in_(source_ids))

        categories = [
            CategorySourcePydantic.model_validate(cat)
            for cat in session.scalars(stmt).all()
        ]
        logger.debug(
            "Fetched source categories.",
            platform=platform.value,
            count=len(categories),
            source_ids=source_ids,
        )
        return categories


def get_all_categories_for_platform(
    platform: SourcePlatform,
) -> List[CategorySourcePydantic]:
    """
    從資料庫獲取指定平台的所有職務分類。
    返回 CategorySourcePydantic 實例列表。
    """
    with get_session() as session:
        stmt = select(CategorySource).where(CategorySource.source_platform == platform)
        categories = [
            CategorySourcePydantic.model_validate(cat)
            for cat in session.scalars(stmt).all()
        ]
        logger.debug(
            "Fetched all categories for platform.",
            platform=platform.value,
            count=len(categories),
        )
        return categories


def upsert_urls(platform: SourcePlatform, urls: List[str]) -> None:
    """
    Synchronizes a list of URLs for a given platform with the database.
    Performs an UPSERT operation. URLs are marked as ACTIVE and PENDING.
    """
    if not urls:
        logger.info("No URLs to upsert.", platform=platform.value)
        return

    now = datetime.now(timezone.utc)
    url_models_to_upsert = [
        {
            "source_url": url,
            "source": platform,
            "status": JobStatus.ACTIVE.value,
            "details_crawl_status": CrawlStatus.PENDING.value,
            "crawled_at": now,
            "updated_at": now,
        }
        for url in urls
    ]

    update_cols = ["status", "updated_at", "details_crawl_status"]
    affected_rows = _generic_upsert(Url, url_models_to_upsert, update_cols)

    logger.info("URLs upserted successfully.", platform=platform.value, count=len(urls), affected_rows=affected_rows)


def get_urls_by_crawl_status(
    platform: SourcePlatform, statuses: List[CrawlStatus], limit: int
) -> List[str]:
    """
    從資料庫獲取指定平台和指定爬取狀態列表的 URL。
    返回 URL 字串列表。
    """
    with get_session() as session:
        statement = (
            select(Url.source_url)
            .where(Url.source == platform, Url.details_crawl_status.in_(statuses))
            .limit(limit)
        )
        urls = list(session.scalars(statement).all())
        logger.debug(
            "Fetched URLs by crawl status.",
            platform=platform.value,
            statuses=[s.value for s in statuses],
            count=len(urls),
            limit=limit,
        )
        return urls


def update_urls_status(urls: List[str], status: CrawlStatus) -> None:
    """
    批量更新一組 URL 的爬取狀態。
    """
    if not urls:
        logger.info("No URLs provided to update status.")
        return

    now = datetime.now(timezone.utc)
    with get_session() as session:
        stmt = (
            update(Url)
            .where(Url.source_url.in_(urls))
            .values(details_crawl_status=status, updated_at=now)
        )
        session.execute(stmt)
        logger.info(
            "Successfully updated URL statuses.",
            status=status.value,
            count=len(urls),
        )


def upsert_jobs(jobs: List[JobPydantic]) -> None:
    """
    將 Job 對象列表同步到資料庫。
    執行 UPSERT 操作，如果職位已存在則更新，否則插入。
    """
    if not jobs:
        logger.info("No jobs to upsert.", count=0)
        return

    now = datetime.now(timezone.utc)
    job_dicts_to_upsert = [
        {
            **job.model_dump(
                exclude_none=False,
                exclude={"extracted_skills", "extracted_languages", "extracted_licenses"}
            ),
            "updated_at": now,
            "created_at": job.created_at or now,
        }
        for job in jobs
    ]

    # --- DEBUG PRINT --- START
    for job_dict in job_dicts_to_upsert:
        logger.info("Job dict before upsert", job_id=job_dict.get("source_job_id"), salary_type=job_dict.get("salary_type"))
    # --- DEBUG PRINT --- END

    # 動態生成更新欄位列表，排除主鍵
    update_cols = [
        column.name for column in Job.__table__.columns if not column.primary_key
    ]
    affected_rows = _generic_upsert(Job, job_dicts_to_upsert, update_cols)

    logger.info("Jobs upserted successfully.", count=len(job_dicts_to_upsert), affected_rows=affected_rows)


def upsert_job_locations(job_locations: List[JobLocationPydantic]) -> None:
    """
    將 JobLocation 對象列表同步到資料庫。
    執行 UPSERT 操作，如果經緯度資訊已存在則更新，否則插入。
    """
    if not job_locations:
        logger.info("No job locations to upsert.", count=0)
        return

    now = datetime.now(timezone.utc)
    location_dicts_to_upsert = [
        {
            **loc.model_dump(exclude_none=False),
            "updated_at": now,
            "created_at": loc.created_at or now,
        }
        for loc in job_locations
    ]

    update_cols = ["latitude", "longitude", "updated_at"]
    affected_rows = _generic_upsert(JobLocation, location_dicts_to_upsert, update_cols)

    logger.info("Job locations upserted successfully.", count=len(location_dicts_to_upsert), affected_rows=affected_rows)


def upsert_url_categories(url_category_data: List[Dict[str, Any]]) -> None:
    """
    將 URL 與其所屬的分類關聯數據同步到資料庫。
    執行 UPSERT 操作，如果關聯已存在則更新，否則插入。
    """
    if not url_category_data:
        logger.info("No URL category data to upsert.")
        return

    affected_rows = _generic_upsert(UrlCategory, url_category_data, []) # UrlCategory has composite primary key, no update columns needed for UPSERT

    logger.info("URL categories upserted successfully.", count=len(url_category_data), affected_rows=affected_rows)


def mark_urls_as_crawled(processed_urls: Dict[CrawlStatus, List[str]]) -> None:
    """
    根據處理狀態標記 URL 為已爬取。
    """
    if not processed_urls:
        logger.info("No URLs to mark as crawled.")
        return

    now = datetime.now(timezone.utc)
    with get_session() as session:
        for status, urls in processed_urls.items():
            if urls:
                stmt = (
                    update(Url)
                    .where(Url.source_url.in_(urls))
                    .values(details_crawl_status=status, details_crawled_at=now)
                )
                session.execute(stmt)
                logger.info(
                    "URLs marked as crawled.", status=status.value, count=len(urls)
                )

def get_all_category_source_ids_pandas(platform: SourcePlatform) -> Set[str]:
    """
    使用 Pandas 獲取指定平台所有職務分類的 source_category_id。
    """
    with get_session() as session:
        query = select(CategorySource.source_category_id).where(CategorySource.source_platform == platform)
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


def get_all_crawled_category_ids_pandas(platform: SourcePlatform) -> Set[str]:
    """
    使用 Pandas 獲取指定平台所有已爬取 URL 的 source_category_id。
    """
    with get_session() as session:
        query = select(UrlCategory.source_category_id).join(Url, UrlCategory.source_url == Url.source_url).where(Url.source == platform)
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


def get_stale_crawled_category_ids_pandas(platform: SourcePlatform, n_days: int) -> Set[str]:
    """
    使用 Pandas 獲取指定平台中，上次爬取時間超過 n_days 的 source_category_id。
    """
    threshold_date = datetime.now(timezone.utc) - timedelta(days=n_days)
    with get_session() as session:
        query = (
            select(UrlCategory.source_category_id)
            .join(Url, UrlCategory.source_url == Url.source_url)
            .where(
                Url.source == platform,
                UrlCategory.created_at < threshold_date
            )
            .distinct()
        )
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


================================================
FILE: crawler/database/schemas.py
================================================
from datetime import datetime, timezone
from typing import Optional
import enum

from pydantic import BaseModel, Field


class SourcePlatform(str, enum.Enum):
    """資料來源平台。用於在資料庫中標識數據的來源。"""

    PLATFORM_104 = "platform_104"
    PLATFORM_1111 = "platform_1111"
    PLATFORM_CAKERESUME = "platform_cakeresume"
    PLATFORM_YES123 = "platform_yes123"


class JobStatus(str, enum.Enum):
    """職缺或 URL 的活躍狀態。"""

    ACTIVE = "active"
    INACTIVE = "inactive"


class CrawlStatus(str, enum.Enum):
    """職缺詳情頁的抓取狀態。"""

    PENDING = "PENDING"
    QUEUED = "QUEUED"
    PROCESSING = "PROCESSING"
    SUCCESS = "SUCCESS"
    FAILED = "FAILED"


class SalaryType(str, enum.Enum):
    """標準化的薪資給付週期。"""

    MONTHLY = "MONTHLY"
    HOURLY = "HOURLY"
    YEARLY = "YEARLY"
    DAILY = "DAILY"
    BY_CASE = "BY_CASE"
    NEGOTIABLE = "NEGOTIABLE"


class JobType(str, enum.Enum):
    """標準化的工作類型。"""

    FULL_TIME = "FULL_TIME"
    PART_TIME = "PART_TIME"
    CONTRACT = "CONTRACT"
    INTERNSHIP = "INTERNSHIP"
    TEMPORARY = "TEMPORARY"
    OTHER = "OTHER"


class CategorySourcePydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_category_id: str
    source_category_name: str
    parent_source_id: Optional[str] = None

    class Config:
        from_attributes = True


class UrlPydantic(BaseModel):
    source_url: str
    source: SourcePlatform
    status: JobStatus = JobStatus.ACTIVE
    details_crawl_status: CrawlStatus = CrawlStatus.PENDING
    crawled_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    details_crawled_at: Optional[datetime] = None

    class Config:
        from_attributes = True


class UrlCategoryPydantic(BaseModel):
    source_url: str
    source_category_id: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

    class Config:
        from_attributes = True


class JobPydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_job_id: str
    url: str
    status: JobStatus
    title: str
    description: Optional[str] = None
    job_type: Optional[JobType] = None
    location_text: Optional[str] = None
    posted_at: Optional[datetime] = None
    salary_text: Optional[str] = None
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    salary_type: Optional[SalaryType] = None
    experience_required_text: Optional[str] = None
    education_required_text: Optional[str] = None
    company_source_id: Optional[str] = None
    company_name: Optional[str] = None
    company_url: Optional[str] = None
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    class Config:
        from_attributes = True


class JobLocationPydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_job_id: str
    latitude: Optional[str] = None
    longitude: Optional[str] = None
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    class Config:
        from_attributes = True



================================================
FILE: crawler/database/scripts/fix_salary_data.py
================================================
from crawler.database.connection import get_session
from crawler.database.models import Job
from crawler.database.schemas import SourcePlatform
from crawler.utils.salary_parser import parse_salary_text
from crawler.project_1111.parser_apidata_1111 import derive_salary_type

try:
    with get_session() as session:
        # Query for jobs that match the original criteria
        # You can modify this filter as needed for actual data updates
        jobs_to_update = session.query(Job).filter(
            Job.source_platform == SourcePlatform.PLATFORM_1111,
            Job.salary_text.isnot(None) # Ensure salary_text is not null
        ).all()

        print(f"Found {len(jobs_to_update)} jobs to process.")

        for job in jobs_to_update:
            min_salary, max_salary = parse_salary_text(job.salary_text)
            # Assuming job.job_type is already populated from the crawler
            salary_type = derive_salary_type(job.salary_text, min_salary, job.job_type)

            if min_salary is not None or salary_type is not None: # Update if either min_salary is parsed or type is derived
                job.salary_min = min_salary
                job.salary_max = max_salary
                job.salary_type = salary_type
                print(f"Updating job_id: {job.id} - salary_text: '{job.salary_text}' - salary_min: {min_salary}, salary_max: {max_salary}, salary_type: {salary_type}")
            else:
                print(f"Skipping job_id: {job.id} - Could not parse salary from '{job.salary_text}'")

        print("Successfully processed salary information for matching jobs.")

except Exception as e:
    print(f"An error occurred: {e}")



================================================
FILE: crawler/database/scripts/get_category_ids.py
================================================
import os
import sys
import structlog
import pandas as pd

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.database.connection import get_session, initialize_database
from crawler.database.models import CategorySource
from crawler.logging_config import configure_logging

# Configure logging at the script level
configure_logging()
logger = structlog.get_logger(__name__)


def get_source_category_ids():
    """
    Fetches all category IDs and names from the database and returns them as a Pandas DataFrame.
    """
    try:
        with get_session() as session:
            categories = session.query(CategorySource).all()
            data = [
                {
                    "parent_source_id": cat.parent_source_id,
                    "source_category_id": cat.source_category_id,
                    "source_category_name": cat.source_category_name,
                }
                for cat in categories
            ]
            df = pd.DataFrame(data)
            logger.info("Successfully fetched source category IDs.", count=len(df))
            return df
    except Exception as e:
        logger.error("Error fetching source_category_ids with ORM.", error=e, exc_info=True)
        return pd.DataFrame()  # Return an empty DataFrame on error


if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.get_category_ids
    
    # Ensure the database is initialized before fetching data
    initialize_database()

    ids_df = get_source_category_ids()
    if not ids_df.empty:
        logger.info(
            "Source Category IDs fetched successfully.",
            dataframe_head=ids_df.head().to_dict("records")
        )
    else:
        logger.warning("Could not fetch any source category IDs.")


================================================
FILE: crawler/database/scripts/pandas_sql_config.py
================================================
import os
import sys
import pandas as pd
import structlog
from sqlalchemy import create_engine

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.logging_config import configure_logging
from crawler.config import (
    MYSQL_HOST,
    MYSQL_PORT,
    MYSQL_ACCOUNT,
    MYSQL_PASSWORD,
    MYSQL_DATABASE,
)

configure_logging()
logger = structlog.get_logger(__name__)


def main():
    """
    Demonstrates connecting to the database and reading data using Pandas.
    """
    db_url = (
        f"mysql+mysqlconnector://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}"
    )

    engine = create_engine(db_url)

    try:
        table_name = "tb_category_source"
        logger.info(
            "Attempting to read data from database using Pandas.", table=table_name
        )
        df = pd.read_sql(f"SELECT * FROM {table_name} LIMIT 5", engine)

        logger.info(
            "Successfully read data from database.", table=table_name, rows_read=len(df)
        )
        logger.info("DataFrame head:", dataframe_head=df.head().to_dict("records"))

    except Exception as e:
        logger.error(
            "An error occurred during database connection or query.",
            error=e,
            exc_info=True,
        )

    finally:
        engine.dispose()
        logger.info("Database engine disposed.")


if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.pandas_sql_config
    main()


================================================
FILE: crawler/database/scripts/temp_count_db.py
================================================
import os
import sys
import structlog
from sqlalchemy import text

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.database.connection import get_session, initialize_database
from crawler.logging_config import configure_logging

# Configure logging at the script level
configure_logging()
logger = structlog.get_logger(__name__)


def main():
    """
    Connects to the database and counts the records in key tables.
    """
    logger.info("Starting database count check.")
    try:
        # Ensure the database and tables exist before counting
        initialize_database()
        
        with get_session() as session:
            category_count = session.execute(
                text("SELECT COUNT(*) FROM tb_category_source")
            ).scalar_one_or_none() or 0
            url_count = session.execute(text("SELECT COUNT(*) FROM tb_urls")).scalar_one_or_none() or 0

            logger.info(
                "Database record counts retrieved successfully.",
                category_count=category_count,
                url_count=url_count,
            )

    except Exception as e:
        logger.error(
            "An error occurred while counting database records.", error=e, exc_info=True
        )

if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.temp_count_db
    main()


================================================
FILE: crawler/geocoding/client.py
================================================
import structlog
from typing import Optional, Dict
import random
import time

from crawler.config import (
    URL_CRAWLER_SLEEP_MIN_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
)
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

def geocode_address(address: str) -> Optional[Dict[str, float]]:
    """
    模擬地理編碼服務，將地址轉換為經緯度。
    在實際應用中，這裡會呼叫第三方地理編碼 API (例如 Google Maps Geocoding API)。
    """
    logger.info("Attempting to geocode address.", address=address)
    
    # 模擬網路延遲
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    time.sleep(sleep_time)

    # 簡單的模擬經緯度，實際應根據地址返回真實數據
    if "台北" in address:
        return {"latitude": 25.032969, "longitude": 121.564559} # 台北市政府
    elif "台中" in address:
        return {"latitude": 24.137135, "longitude": 120.687138} # 台中市政府
    elif "高雄" in address:
        return {"latitude": 22.627278, "longitude": 120.301435} # 高雄市政府
    else:
        logger.warning("Address not found in mock geocoding service.", address=address)
        return None



================================================
FILE: crawler/geocoding/task.py
================================================
import structlog

from crawler.worker import app
from crawler.database.models import SourcePlatform, JobLocationPydantic
from crawler.database.repository import upsert_job_locations
from crawler.geocoding.client import geocode_address

logger = structlog.get_logger(__name__)


@app.task()
def geocode_job_location(
    source_platform: str, source_job_id: str, location_text: str
) -> None:
    """
    Celery 任務：接收職缺的地理位置文字，呼叫地理編碼服務，
    並將經緯度資訊儲存到資料庫。
    """
    logger.info(
        "Starting geocoding for job location.",
        source_platform=source_platform,
        source_job_id=source_job_id,
        location_text=location_text,
    )

    try:
        geocoded_data = geocode_address(location_text)

        if geocoded_data:
            # 只有當任務不是在 eager 模式下執行時才寫入資料庫
            if not app.conf.task_always_eager:
                job_location_pydantic = JobLocationPydantic(
                    source_platform=SourcePlatform(source_platform),
                    source_job_id=source_job_id,
                    latitude=str(geocoded_data["latitude"]),
                    longitude=str(geocoded_data["longitude"]),
                )
                upsert_job_locations([job_location_pydantic])
                logger.info(
                    "Geocoding successful and data upserted.",
                    source_platform=source_platform,
                    source_job_id=source_job_id,
                    latitude=geocoded_data["latitude"],
                    longitude=geocoded_data["longitude"],
                )
            else:
                logger.info(
                    "Geocoding successful (eager mode), skipping database upsert.",
                    source_platform=source_platform,
                    source_job_id=source_job_id,
                    latitude=geocoded_data["latitude"],
                    longitude=geocoded_data["longitude"],
                )
        else:
            logger.warning(
                "Geocoding failed for location.",
                source_platform=source_platform,
                source_job_id=source_job_id,
                location_text=location_text,
            )

    except Exception as e:
        logger.error(
            "An unexpected error occurred during geocoding task.",
            source_platform=source_platform,
            source_job_id=source_job_id,
            location_text=location_text,
            error=e,
            exc_info=True,
        )


if __name__ == "__main__":
    from crawler.database.connection import initialize_database
    initialize_database()

    # 測試範例
    geocode_job_location("platform_104", "test_job_1", "台北市")
    geocode_job_location("platform_1111", "test_job_2", "台中市")
    geocode_job_location("platform_cakeresume", "test_job_3", "高雄市")
    geocode_job_location("platform_yes123", "test_job_4", "未知地點")



================================================
FILE: crawler/project_104/104人力銀行_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import random
import json
import requests
from tqdm import tqdm
import pandas as pd
from collections import deque

WEB_NAME = '104_人力銀行'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Referer': 'https://www.104.com.tw/jobs/search',
    }


## 取得網站所有職業總覽

# 1. 取得 JSON 資料
# jobcat 檔案名稱
file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
url_JobCat = "https://static.104.com.tw/category-tool/json/JobCat.json"

response_jobcat = requests.get(url_JobCat, headers=HEADERS, timeout=10)
response_jobcat.raise_for_status()
jobcat_data = response_jobcat.json()
with open(file_jobcat_json, "w", encoding="utf-8") as f:
    json.dump(jobcat_data, f, ensure_ascii=False, indent=4)
print(f"職業總覽資料已儲存為 {file_jobcat_json}")


# 2. 直接將 requests 取得的資料傳入遞迴函式
def flatten_jobcat_recursive(node_list, parent_des=None, parent_no=None):
    flat_list = []
    for node in node_list:
        row = {
            "parent_code": parent_no,
            "parent_name": parent_des,
            "job_code": node.get("no"),
            "job_name": node.get("des"),
        }
        flat_list.append(row)
        if "n" in node and node["n"]:
            children_list = flatten_jobcat_recursive(
                node_list=node["n"],
                parent_des=node.get("des"),
                parent_no=node.get("no"),
            )
            flat_list.extend(children_list)
    return flat_list


# 3. 執行結果轉為 DataFrame
flattened_data = flatten_jobcat_recursive(jobcat_data)
df_jobcat = pd.DataFrame(flattened_data)
df_jobcat = df_jobcat[df_jobcat["parent_code"].notnull()]
df_jobcat_sorted = df_jobcat.sort_values(by="job_code")
df_jobcat_sorted.to_excel(f"{WEB_NAME}_category.xlsx", index=False)
print(f"職業總覽資料已轉換為 '{WEB_NAME}_category.xlsx'")

# 篩選出 IT 相關的工作
# 篩選出 job_code 以 '140' 開頭的行
mask = df_jobcat_sorted["job_code"].astype(str).str.startswith("2007")
df_it_jobs = df_jobcat_sorted[mask]
# df_it_jobs
# Output:
#   職業總覽資料已儲存為 104_人力銀行_jobcat_json.txt

#   職業總覽資料已轉換為 '104_人力銀行_category.xlsx'


# 產生 104 人力銀行網址 https://www.104.com.tw 根據提供的 (關鍵字和職缺類別) 轉換為職缺網址

def catch_104_url (KEYWORDS, CATEGORY, ORDER=None):
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (int, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。
    """

    BASE_URL = "https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s"

    param_fragments = []
    if ORDER is not None:
        param_fragments.append(f"order={ORDER}")
    if KEYWORDS:
        param_fragments.append(f"keyword={KEYWORDS}")
    if CATEGORY:
        param_fragments.append(f"jobcat={CATEGORY}")
    query_string = "&".join(param_fragments)

    return f"{BASE_URL}{query_string}&page="


KEYWORDS_STR = "雲端工程師"
JOBCAT_CODE = "2007000000"
ORDER_SETTING = 16     # 15 (符合度高)、  16 (最近更新)


# # 測試範例
# url_1 = catch_104_url (KEYWORDS_STR, JOBCAT_CODE, ORDER_SETTING)
# print (url_1)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&order=15&keyword=雲端工程師&jobcat=2007000000&page=

# url_2 = catch_104_url (KEYWORDS_STR, "")
# print (url_2)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&keyword=雲端工程師&page=

# url_3 = catch_104_url ("", JOBCAT_CODE, ORDER_SETTING)
# print (url_3)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&order=15&jobcat=2007000000&page=

# url_4 = catch_104_url ("","")
# print (url_4)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&page=

#  從 api 網址獲取工作職缺的網址

def fetch_104_job_url (_CODE, KEYWORD):
    """
    這個函數會遍歷多個頁面，並從每個頁面中提取工作職缺的網址，將其存儲在一個集合中以避免重複。
    使用 tqdm 顯示進度條，並在每次請求之間隨機延遲以避免過於頻繁的請求。

    參數:
    _CODE (str): 職缺類別的代碼。
    KEYWORD (str): 搜尋的關鍵字。

    返回:
    list: 包含所有獲取到的工作職缺網址的列表。

    """

    BASE_URL = "https://www.104.com.tw/jobs/search/api/jobs"

    PAGE = 1
    MAX_PAGE = 10
    PAGE_SIZE = 30
    ORDER_SETTING = 15   # 15 (符合度高)、  16 (最近更新)
    
    MAX_LENGTH = 4
    recent_counts = deque (maxlen=MAX_LENGTH)
    job_url_set = set ()  # 用於存儲唯一的職缺網址

    with requests.Session () as session, tqdm (total=MAX_PAGE, desc="104 職缺列表", unit="PAGE", leave=True) as pbar:
        while True:
            params = {
                'jobsource': 'm_joblist_search',
                'page': PAGE,
                'pagesize': PAGE_SIZE,
                'order': ORDER_SETTING, 
                'jobcat': _CODE,
                'keyword': KEYWORD,
            }

            response = requests.get (BASE_URL, headers=HEADERS, params=params, timeout=20)
            api_job_urls = response.json ()['data']
            for job_url in api_job_urls:
                job_url_set.add (job_url ['link']['job'])

            # 檢查是否有新資料
            total_jobs = len (job_url_set) 
            recent_counts.append (total_jobs)
            if len (recent_counts) == MAX_LENGTH and len (set (recent_counts)) == 1:
                print (f"連續 {MAX_LENGTH} 次沒有新資料，提前結束。")
                break
            
            time.sleep (random.uniform (0.5, 1.5))
            
            pbar.set_postfix_str (f"目前頁面 {PAGE}, 最大頁數: {MAX_PAGE}")
            pbar.update (1)

            PAGE = PAGE + 1  # 更新頁碼
            if PAGE >= MAX_PAGE:
                MAX_PAGE = PAGE + 1 
                pbar.total = MAX_PAGE

    modified_job_url_set = {f"https://www.104.com.tw/job/ajax/content/{url.split ('/')[-1]}" for url in job_url_set}
    print (f"共獲取到 {len (job_url_set)} 筆職缺資料。")
    return list (modified_job_url_set)


# 測試範例
# JOBCAT_CODE = "2007000000"
# KEYWORDS = "雲端工程師"
# jobs_url = fetch_104_job_url(JOBCAT_CODE, KEYWORDS)
# jobs_url [0]

# 從指定的職缺網址獲取職缺的相關數據

def fetch_104_job_data(job_url):
    """
    這個函數會發送 GET 請求到提供的職缺網址，並使用 BeautifulSoup 解析返回的 HTML 文檔。
    它會從頁面中的 JavaScript 代碼中提取職缺的元數據，並將其轉換為 Pandas DataFrame 格式。

    參數:
    job_url (str): 職缺的網址。

    返回:
    pd.DataFrame: 包含職缺詳細信息的 DataFrame，包括職缺網址、公司名稱、公司網址及其他職缺網址。
    """
    response = requests.get(job_url, headers=HEADERS)
    jobMetaData = response.json()["data"]
    df = pd.json_normalize(jobMetaData)
    return df


# 測試範例
# job_url = "https://www.104.com.tw/job/ajax/content/8k4lp"   # jobs_url [0]
# job_data = fetch_104_job_data(job_url)
# job_data

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime ('%Y-%m-%d', time.localtime (time.time ()))
JOBCAT_CODE = "2007000000"
KEYWORDS = "雲端工程師"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{JOBCAT_CODE}"

print ( f"開始執行 {FILE_NAME}" )
job_data_list = []
job_urls = fetch_104_job_url(JOBCAT_CODE, KEYWORDS)        # 列出 104 人力銀行 - 職缺網址列表 


all_jobs_df = pd.DataFrame ()  # 初始化一個空的 DataFrame

for url in tqdm (job_urls, desc="Fetching job data", unit="job"):
    df_job_data = fetch_104_job_data(url)
    all_jobs_df = pd.concat ([all_jobs_df, df_job_data], ignore_index=True)


print(all_jobs_df.shape)

all_jobs_df.head(1)
# Output:
#   開始執行 (2025-06-16)_104_人力銀行_雲端工程師_2007000000

#   104 職缺列表:  95%|█████████▌| 38/40 [01:00<00:03,  1.59s/PAGE, 目前頁面 38, 最大頁數: 39]

#   連續 4 次沒有新資料，提前結束。

#   共獲取到 1034 筆職缺資料。

#   Fetching job data: 100%|██████████| 1034/1034 [03:30<00:00,  4.91job/s]
#   (1034, 79)

#   

#     switch                                           custLogo postalCode  \

#   0     on  https://static.104.com.tw/b_profile/cust_pictu...        105   

#   

#     closeDate industry       custNo  \

#   0             人力仲介代徵  11111119000   

#   

#                                              reportUrl  industryNo employees  \

#   0  https://www.104.com.tw/feedback?category=2&cus...  1009001001      820人   

#   

#      chinaCorp  ... jobDetail.startWorkingDay jobDetail.hireType  \

#   0      False  ...                        不限                  0   

#   

#     jobDetail.delegatedRecruit jobDetail.needEmp jobDetail.landmark  \

#   0                                           1人                      

#   

#     jobDetail.remoteWork interactionRecord.lastProcessedResumeAtTime  \

#   0                 None                                        None   

#   

#     interactionRecord.nowTimestamp  jobDetail.remoteWork.type  \

#   0                     1750026676                        NaN   

#   

#     jobDetail.remoteWork.description  

#   0                              NaN  

#   

#   [1 rows x 79 columns]

# all_jobs_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
# Output:
#   已將所有職缺資料儲存到 (2025-06-16)_104_人力銀行_雲端工程師_2007000000.xlsx


all_jobs_df.columns
# Output:
#   Index(['switch', 'custLogo', 'postalCode', 'closeDate', 'industry', 'custNo',

#          'reportUrl', 'industryNo', 'employees', 'chinaCorp',

#          'corpImageRight.corpImageRight.imageUrl',

#          'corpImageRight.corpImageRight.link', 'header.corpImageTop.imageUrl',

#          'header.corpImageTop.link', 'header.jobName', 'header.appearDate',

#          'header.custName', 'header.custUrl', 'header.analysisType',

#          'header.analysisUrl', 'header.isSaved', 'header.isApplied',

#          'header.applyDate', 'header.userApplyCount', 'header.isActivelyHiring',

#          'contact.hrName', 'contact.email', 'contact.visit', 'contact.phone',

#          'contact.other', 'contact.reply', 'environmentPic.environmentPic',

#          'environmentPic.corpImageBottom.imageUrl',

#          'environmentPic.corpImageBottom.link', 'condition.acceptRole.role',

#          'condition.acceptRole.disRole.needHandicapCompendium',

#          'condition.acceptRole.disRole.disability', 'condition.workExp',

#          'condition.edu', 'condition.major', 'condition.language',

#          'condition.localLanguage', 'condition.specialty', 'condition.skill',

#          'condition.certificate', 'condition.driverLicense', 'condition.other',

#          'welfare.tag', 'welfare.welfare', 'welfare.legalTag',

#          'jobDetail.jobDescription', 'jobDetail.jobCategory', 'jobDetail.salary',

#          'jobDetail.salaryMin', 'jobDetail.salaryMax', 'jobDetail.salaryType',

#          'jobDetail.jobType', 'jobDetail.workType', 'jobDetail.addressNo',

#          'jobDetail.addressRegion', 'jobDetail.addressArea',

#          'jobDetail.addressDetail', 'jobDetail.industryArea',

#          'jobDetail.longitude', 'jobDetail.latitude', 'jobDetail.manageResp',

#          'jobDetail.businessTrip', 'jobDetail.workPeriod',

#          'jobDetail.vacationPolicy', 'jobDetail.startWorkingDay',

#          'jobDetail.hireType', 'jobDetail.delegatedRecruit', 'jobDetail.needEmp',

#          'jobDetail.landmark', 'jobDetail.remoteWork',

#          'interactionRecord.lastProcessedResumeAtTime',

#          'interactionRecord.nowTimestamp', 'jobDetail.remoteWork.type',

#          'jobDetail.remoteWork.description'],

#         dtype='object')

column_names = [
    {"序號": 1, "英文": "switch", "中文": "內部切換/開關"},
    {"序號": 2, "英文": "custLogo", "中文": "公司Logo"},
    {"序號": 3, "英文": "postalCode", "中文": "郵遞區號"},
    {"序號": 4, "英文": "closeDate", "中文": "截止日期"},
    {"序號": 5, "英文": "industry", "中文": "產業類別"},
    {"序號": 6, "英文": "custNo", "中文": "公司代號"},
    {"序號": 7, "英文": "reportUrl", "中文": "檢舉職務網址"},
    {"序號": 8, "英文": "industryNo", "中文": "產業代號"},
    {"序號": 9, "英文": "employees", "中文": "員工人數"},
    {"序號": 10, "英文": "chinaCorp", "中文": "中國大陸關係企業"},
    {
        "序號": 11,
        "英文": "corpImageRight.corpImageRight.imageUrl",
        "中文": "右側公司圖片網址",
    },
    {
        "序號": 12,
        "英文": "corpImageRight.corpImageRight.link",
        "中文": "右側公司圖片連結",
    },
    {"序號": 13, "英文": "header.corpImageTop.imageUrl", "中文": "頂部公司圖片網址"},
    {"序號": 14, "英文": "header.corpImageTop.link", "中文": "頂部公司圖片連結"},
    {"序號": 15, "英文": "header.jobName", "中文": "職務名稱"},
    {"序號": 16, "英文": "header.appearDate", "中文": "更新日期"},
    {"序號": 17, "英文": "header.custName", "中文": "公司名稱"},
    {"序號": 18, "英文": "header.custUrl", "中文": "公司頁面網址"},
    {"序號": 19, "英文": "header.analysisType", "中文": "應徵分析類型"},
    {"序號": 20, "英文": "header.analysisUrl", "中文": "應徵分析網址"},
    {"序號": 21, "英文": "header.isSaved", "中文": "是否已儲存"},
    {"序號": 22, "英文": "header.isApplied", "中文": "是否已應徵"},
    {"序號": 23, "英文": "header.applyDate", "中文": "應徵日期"},
    {"序號": 24, "英文": "header.userApplyCount", "中文": "使用者應徵次數"},
    {"序號": 25, "英文": "header.isActivelyHiring", "中文": "是否為積極徵才"},
    {"序號": 26, "英文": "contact.hrName", "中文": "聯絡人"},
    {"序號": 27, "英文": "contact.email", "中文": "聯絡E-mail"},
    {"序號": 28, "英文": "contact.visit", "中文": "親洽地址"},
    {"序號": 29, "英文": "contact.phone", "中文": "聯絡電話"},
    {"序號": 30, "英文": "contact.other", "中文": "其他聯絡方式"},
    {"序號": 31, "英文": "contact.reply", "中文": "應徵回覆率/時間"},
    {"序號": 32, "英文": "environmentPic.environmentPic", "中文": "公司環境照片"},
    {
        "序號": 33,
        "英文": "environmentPic.corpImageBottom.imageUrl",
        "中文": "底部公司圖片網址",
    },
    {
        "序號": 34,
        "英文": "environmentPic.corpImageBottom.link",
        "中文": "底部公司圖片連結",
    },
    {"序號": 35, "英文": "condition.acceptRole.role", "中文": "接受身份"},
    {
        "序號": 36,
        "英文": "condition.acceptRole.disRole.needHandicapCompendium",
        "中文": "需附身心障礙證明",
    },
    {
        "序號": 37,
        "英文": "condition.acceptRole.disRole.disability",
        "中文": "身心障礙類別",
    },
    {"序號": 38, "英文": "condition.workExp", "中文": "工作經歷"},
    {"序號": 39, "英文": "condition.edu", "中文": "學歷要求"},
    {"序號": 40, "英文": "condition.major", "中文": "科系要求"},
    {"序號": 41, "英文": "condition.language", "中文": "語文條件"},
    {"序號": 42, "英文": "condition.localLanguage", "中文": "本國語言條件"},
    {"序號": 43, "英文": "condition.specialty", "中文": "工作技能"},
    {"序號": 44, "英文": "condition.skill", "中文": "擅長工具"},
    {"序號": 45, "英文": "condition.certificate", "中文": "具備證照"},
    {"序號": 46, "英文": "condition.driverLicense", "中文": "具備駕照"},
    {"序號": 47, "英文": "condition.other", "中文": "其他條件"},
    {"序號": 48, "英文": "welfare.tag", "中文": "福利標籤"},
    {"序號": 49, "英文": "welfare.welfare", "中文": "公司福利(詳細說明)"},
    {"序號": 50, "英文": "welfare.legalTag", "中文": "法定福利標籤"},
    {"序號": 51, "英文": "jobDetail.jobDescription", "中文": "工作內容"},
    {"序號": 52, "英文": "jobDetail.jobCategory", "中文": "職務類別"},
    {"序號": 53, "英文": "jobDetail.salary", "中文": "薪資待遇(文字描述)"},
    {"序號": 54, "英文": "jobDetail.salaryMin", "中文": "最低薪資"},
    {"序號": 55, "英文": "jobDetail.salaryMax", "中文": "最高薪資"},
    {"序號": 56, "英文": "jobDetail.salaryType", "中文": "薪資類型(月薪/面議)"},
    {"序號": 57, "英文": "jobDetail.jobType", "中文": "工作性質(全職/兼職)"},
    {"序號": 58, "英文": "jobDetail.workType", "中文": "工作型態"},
    {"序號": 59, "英文": "jobDetail.addressNo", "中文": "地址郵遞區號"},
    {"序號": 60, "英文": "jobDetail.addressRegion", "中文": "上班地點(縣市)"},
    {"序號": 61, "英文": "jobDetail.addressArea", "中文": "上班地點(鄉鎮市區)"},
    {"序號": 62, "英文": "jobDetail.addressDetail", "中文": "上班地點(詳細地址)"},
    {"序號": 63, "英文": "jobDetail.industryArea", "中文": "工作地點/工業區"},
    {"序號": 64, "英文": "jobDetail.longitude", "中文": "經度"},
    {"序號": 65, "英文": "jobDetail.latitude", "中文": "緯度"},
    {"序號": 66, "英文": "jobDetail.manageResp", "中文": "管理責任"},
    {"序號": 67, "英文": "jobDetail.businessTrip", "中文": "出差外派"},
    {"序號": 68, "英文": "jobDetail.workPeriod", "中文": "上班時段"},
    {"序號": 69, "英文": "jobDetail.vacationPolicy", "中文": "休假制度"},
    {"序號": 70, "英文": "jobDetail.startWorkingDay", "中文": "可上班日"},
    {"序號": 71, "英文": "jobDetail.hireType", "中文": "聘僱類型"},
    {"序號": 72, "英文": "jobDetail.delegatedRecruit", "中文": "是否為派遣工作"},
    {"序號": 73, "英文": "jobDetail.needEmp", "中文": "需求人數"},
    {"序號": 74, "英文": "jobDetail.landmark", "中文": "地標"},
    {"序號": 75, "英文": "jobDetail.remoteWork.type", "中文": "遠端工作類型"},
    {"序號": 76, "英文": "jobDetail.remoteWork.description", "中文": "遠端工作描述"},
    {
        "序號": 77,
        "英文": "interactionRecord.lastProcessedResumeAtTime",
        "中文": "上次處理履歷時間",
    },
    {"序號": 78, "英文": "interactionRecord.nowTimestamp", "中文": "當前時間戳"},
    {"序號": 79, "英文": "jobDetail.remoteWork", "中文": "遠端工作(物件)"},
]

df_new = pd.json_normalize(column_names)
df_new.columns = ["序號", "104_英文", "104_中文"]
df_new
# Output:
#       序號                                       104_英文    104_中文

#   0    1                                       switch   內部切換/開關

#   1    2                                     custLogo    公司Logo

#   2    3                                   postalCode      郵遞區號

#   3    4                                    closeDate      截止日期

#   4    5                                     industry      產業類別

#   ..  ..                                          ...       ...

#   74  75                    jobDetail.remoteWork.type    遠端工作類型

#   75  76             jobDetail.remoteWork.description    遠端工作描述

#   76  77  interactionRecord.lastProcessedResumeAtTime  上次處理履歷時間

#   77  78               interactionRecord.nowTimestamp     當前時間戳

#   78  79                         jobDetail.remoteWork  遠端工作(物件)

#   

#   [79 rows x 3 columns]



================================================
FILE: crawler/project_104/client_104.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import (
    HEADERS_104_JOB_API,
    JOB_API_BASE_URL_104,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_api_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[Dict[str, Any]]:
    """
    通用的 API 請求函式，處理隨機延遲、請求發送、JSON 解析和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before API request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        data = response.json()
        return data
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except json.JSONDecodeError:
        logger.error(
            "Failed to parse JSON response from API.",
            url=url,
            exc_info=True,
            **log_context,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None


def fetch_job_data_from_104_api(job_id: str) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取單一職缺的原始數據。
    """
    api_url = f"{JOB_API_BASE_URL_104}{job_id}"
    return _make_api_request(
        "GET",
        api_url,
        headers=HEADERS_104_JOB_API,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,  # 加上這行
        log_context={"job_id": job_id, "api_type": "job_data"},
    )


def fetch_category_data_from_104_api(
    api_url: str, headers: Dict[str, str]
) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取職務分類的原始數據。
    """
    return _make_api_request(
        "GET",
        api_url,
        headers=headers,
        log_context={"api_type": "category_data"},
    )


def fetch_job_urls_from_104_api(
    base_url: str,
    headers: Dict[str, str],
    params: Dict[str, Any],
    timeout: int,
    verify: bool = True,
) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取職缺 URL 列表的原始數據。
    """
    return _make_api_request(
        "GET",
        base_url,
        headers=headers,
        params=params,
        timeout=timeout,
        verify=verify,
        log_context={"api_type": "job_urls"},
    )



================================================
FILE: crawler/project_104/config_104.py
================================================
# crawler/project_104/config.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# 104 平台相關設定
JOB_CAT_URL_104 = config_section.get(
    "JOB_CAT_URL_104", "https://static.104.com.tw/category-tool/json/JobCat.json"
)
JOB_API_BASE_URL_104 = config_section.get(
    "JOB_API_BASE_URL_104", "https://www.104.com.tw/job/ajax/content/"
)
WEB_NAME_104 = config_section.get("WEB_NAME_104", "104_人力銀行")

HEADERS_104 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/jobs/search",
}

HEADERS_104_JOB_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/job/",
}

HEADERS_104_URL_CRAWLER = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/jobs/search/",
}

URL_CRAWLER_BASE_URL_104 = config_section.get(
    "URL_CRAWLER_BASE_URL_104", "https://www.104.com.tw/jobs/search/api/jobs"
)
URL_CRAWLER_PAGE_SIZE_104 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_104", "20"))
URL_CRAWLER_ORDER_BY_104 = int(
    config_section.get("URL_CRAWLER_ORDER_BY_104", "16")
)  # 16 (最近更新)



================================================
FILE: crawler/project_104/local_fetch_104_url_data.py
================================================
import requests
import sys
from requests.exceptions import HTTPError, JSONDecodeError
import structlog

from crawler.worker import app
from crawler.logging_config import configure_logging  # Import configure_logging
from crawler.config import JOB_API_BASE_URL_104  # Import the base URL from config

configure_logging()  # Call configure_logging at the beginning
logger = structlog.get_logger(__name__)


# 註冊 task, 有註冊的 task 才可以變成任務發送給 rabbitmq
@app.task()
def get_job_api_data(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "referer": "https://www.104.com.tw/",
    }

    job_id = url.split("/")[-1].split("?")[0]
    # Use the configured base URL
    url_api = f"{JOB_API_BASE_URL_104}{job_id}"

    try:
        response = requests.get(url_api, headers=headers)
        response.raise_for_status()
        data = response.json()
    except (HTTPError, JSONDecodeError) as err:
        logger.error(
            "Failed to fetch job API data", url=url_api, error=err
        )  # Improved log message
        return {}

    job_data = data.get("data", {})
    if not job_data or job_data.get("custSwitch", {}) == "off":
        logger.info(
            "Job content does not exist or is closed", job_id=job_id
        )  # Improved log message
        return {}

    extracted_info = {
        "job_id": job_id,
        "update_date": job_data.get("header", {}).get("appearDate"),
        "title": job_data.get("header", {}).get("jobName"),
        "description": job_data.get("jobDetail", {}).get("jobDescription"),
        "salary": job_data.get("jobDetail", {}).get("salary"),
        "work_type": job_data.get("jobDetail", {}).get("workType"),
        "work_time": job_data.get("jobDetail", {}).get("workPeriod"),
        "location": job_data.get("jobDetail", {}).get("addressRegion"),
        "degree": job_data.get("condition", {}).get("edu"),
        "department": job_data.get("jobDetail", {}).get("department"),
        "working_experience": job_data.get("condition", {}).get("workExp"),
        "qualification_required": job_data.get("condition", {}).get("other"),
        "qualification_bonus": job_data.get("welfare", {}).get("welfare"),
        "company_id": job_data.get("header", {}).get("custNo"),
        "company_name": job_data.get("header", {}).get("custName"),
        "company_address": job_data.get("company", {}).get("address"),
        "contact_person": job_data.get("contact", {}).get("hrName"),
        "contact_phone": job_data.get("contact", {}).get("email", "未提供"),
    }

    logger.info(
        "Extracted job information", job_id=job_id, extracted_info=extracted_info
    )  # Improved log message
    return extracted_info  # Return the extracted info


if __name__ == "__main__":
    if len(sys.argv) != 2:
        logger.info(
            "Usage: python local_fetch_104_url_data.py <job_url>"
        )  # Updated usage message
        sys.exit(1)

    job_url = sys.argv[1]
    logger.info("Dispatching job API data task", job_url=job_url)
    get_job_api_data.delay(job_url)



================================================
FILE: crawler/project_104/page_api_data_104.txt
================================================
{
  "data": [
    {
      "appearDate": "20250730",
      "applyCnt": 0,
      "coIndustry": 1011001003,
      "coIndustryDesc": "其他營造業",
      "custName": "統上開發建設股份有限公司",
      "custNo": "130000000194576",
      "description": "工作內容:\n1.市場趨勢研究及同業競品分析。\n2.專案執行(新型態商機專案研究、公司中長期年度計畫)。\n3.專案進度追蹤。\n4.專案分析管控。\n",
      "descSnippet": "工作內容:\n1.市場趨勢研究及同業競品分析。\n2.專案執行(新型態商機專案研究、公司中長期年度計畫)。\n3.專案進度追蹤。\n4.專案分析管控。\n",
      "mrtDist": 0,
      "jobAddress": "永安路165巷155號",
      "jobAddrNo": 6001014007,
      "jobAddrNoDesc": "台南市永康區",
      "jobName": "經營企劃人員(需不動產相關科系)",
      "jobNameSnippet": "經營企劃人員(需不動產相關科系)",
      "jobNo": "14570953",
      "jobRo": 1,
      "jobType": 1,
      "lat": 23.0445358,
      "lon": 120.2425509,
      "link": {
        "job": "https://www.104.com.tw/job/8ob0p",
        "cust": "https://www.104.com.tw/company/1a2x6bm5g0",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8ob0p?channel=104rpt"
      },
      "major": [
        "其他相關科系"
      ],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 35000,
      "salaryLow": 30000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf32": {
          "desc": "",
          "param": "wf32"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf23": {
          "desc": "",
          "param": "wf23"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "8:00~17:00",
      "hrBehaviorPR": 0,
      "jobCat": [
        2001001003,
        2004001005,
        2005003004
      ],
      "labels": [
        "c@wf7",
        "c@wf18",
        "c@wf31",
        "c@wf9",
        "c@wf2",
        "c@wf32",
        "c@wf10",
        "c@wf17",
        "c@wf1",
        "c@wf23",
        "c@wf4",
        "c@wf28",
        "c@wf3"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        },
        {
          "language": 18,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 15,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": null,
        "lastCustReplyTimestamp": null,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250727",
      "applyCnt": 4,
      "coIndustry": 1009004002,
      "coIndustryDesc": "汽機車維修業",
      "custName": "Gogoro_渝城車業有限公司",
      "custNo": "130000000113203",
      "description": "只要你是《積極、樂觀、細膩、善溝通、配合度高 》，歡迎加入我們！\n歡迎尋求長期工作穩定、心思細膩、能跟夥伴互助合作、同甘共苦者，一同加入我們的行列。 \n\n工作內容： \n\n1. Gogoro智慧雙輪 銷售 交車\n2. Gogoro智慧雙輪 品牌價值認同 推廣\n3. 售後服務\n4. 客戶關係維繫 維修預約進廠安排\n\n- 本薪、伙食津貼、業績達成獎金\n- 具銷售經驗1年以上、具備說話藝術者尤佳\n- Gogoro認證培訓課程(帶薪)\n- 需配合門市夥伴排班排假\n- 需配合公司政策及人力需求而調動服務門市\n\n在你的工作中，你願意提供自己的經驗，讓你與你的團隊追求卓越表現。完成公司賦予的目標，包含業績目標、客戶滿意度、行政流程等。此外，在你的工作中，能夠達到甚至超越自己設定的工作目標與表現，讓你與你的團隊追求卓越。",
      "descSnippet": "只要你是《積極、樂觀、細膩、善溝通、配合度高 》，歡迎加入我們！\n歡迎尋求長期工作穩定、心思細膩、能跟夥伴互助合作、同甘共苦者，一同加入我們的行列。 \n\n工作內容： \n\n1. Gogoro智慧雙輪 銷售 交車\n2. Gogoro智慧雙輪 品牌價值認同 推廣\n3. 售後服務\n4. 客戶關係維繫 維修預約進廠安排\n\n- 本薪、伙食津貼、業績達成獎金\n- 具銷售經驗1年以上、具備說話藝術者尤佳\n- Gogoro認證培訓課程(帶薪)\n- 需配合門市夥伴排班排假\n- 需配合公司政策及人力需求而調動服務門市\n\n在你的工作中，你願意提供自己的經驗，讓你與你的團隊追求卓越表現。完成公司賦予的目標，包含業績目標、客戶滿意度、行政流程等。此外，在你的工作中，能夠達到甚至超越自己設定的工作目標與表現，讓你與你的團隊追求卓越。",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6001002021,
      "jobAddrNoDesc": "新北市新莊區",
      "jobName": "GOGORO新莊民安【帶薪培訓】銷售業務&儲備幹部 Sales Specialist [本薪+績效獎金]",
      "jobNameSnippet": "GOGORO新莊民安【帶薪培訓】銷售業務&儲備幹部 Sales Specialist [本薪+績效獎金]",
      "jobNo": "13040563",
      "jobRo": 1,
      "jobType": 1,
      "lat": 25.0265985,
      "lon": 121.4178347,
      "link": {
        "job": "https://www.104.com.tw/job/7ri5v",
        "cust": "https://www.104.com.tw/company/1a2x6bkenn",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7ri5v?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 32400,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "11:30-20:30, 12:30-21:30",
      "hrBehaviorPR": 0.8558028490931103,
      "jobCat": [
        2005002004,
        2001001002,
        2005003004
      ],
      "labels": [
        "college@student_invited",
        "c@wf7"
      ],
      "languageRequirements": [
        {
          "language": 18,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 35,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753613395,
        "lastCustReplyTimestamp": 1753692175,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 28,
      "coIndustry": 1003001015,
      "coIndustryDesc": "綜合商品批發代理業",
      "custName": "京麗國際股份有限公司",
      "custNo": "130000000211566",
      "description": "1.通路主管拜訪、合作評估規劃與開發\n2.熟悉公司產品、規章作業流程\n3.現有客戶之關係維護與溝通\n4.對各類營銷數據具高度敏感分析能力\n5.節慶促銷活動提案與檢討\n6.需具跨部門協商之溝通表達能力\n7.善於安排管理與引領部門所屬工作方向\n8.積極完成上級主管其他交辦事項",
      "descSnippet": "1.通路主管拜訪、合作評估規劃與開發\n2.熟悉公司產品、規章作業流程\n3.現有客戶之關係維護與溝通\n4.對各類營銷數據具高度敏感分析能力\n5.節慶促銷活動提案與檢討\n6.需具跨部門協商之溝通表達能力\n7.善於安排管理與引領部門所屬工作方向\n8.積極完成上級主管其他交辦事項",
      "mrtDist": 0.5,
      "jobAddress": "重陽路477號",
      "jobAddrNo": 6001001011,
      "jobAddrNoDesc": "台北市南港區",
      "jobName": "業務部門主管",
      "jobNameSnippet": "業務部門主管",
      "jobNo": "13636052",
      "jobRo": 3,
      "jobType": 0,
      "lat": 25.06001,
      "lon": 121.61094,
      "link": {
        "job": "https://www.104.com.tw/job/849n8",
        "cust": "https://www.104.com.tw/company/1a2x6bmijy",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/849n8?channel=104rpt"
      },
      "major": [
        "企業管理相關",
        "市場行銷相關",
        "其他商業及管理相關"
      ],
      "mrt": "99001001023",
      "mrtDesc": "捷運南港軟體園區站",
      "optionEdu": [
        4,
        5
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf25": {
          "desc": "",
          "param": "wf25"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "landmark": {
          "desc": "距捷運南港軟體園區站約500公尺"
        }
      },
      "s9": [],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9405876961721676,
      "jobCat": [
        9009002000,
        9009004000,
        9001001000
      ],
      "labels": [
        "foreigners@full_fc_em_jbs",
        "c@wf9",
        "45plus@45invited",
        "c@wf29",
        "c@wf7",
        "c@wf28",
        "senior@senior_job_I",
        "c@wf1",
        "c@wf2",
        "c@wf34",
        "c@wf10",
        "c@wf25",
        "c@wf26",
        "college@student_invited",
        "c@wf3"
      ],
      "languageRequirements": [],
      "acceptRole": [],
      "employeeCount": 30,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861291,
        "lastCustReplyTimestamp": 1753863289,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 12,
      "coIndustry": 1009005002,
      "coIndustryDesc": "美容／美體業",
      "custName": "享淨顏_佳登國際有限公司",
      "custNo": "130000000232919",
      "description": "我們有完整的員工訓練且注重工作氣氛和員工福利對人才極為重視，薪水方面隨著年資增長未來只會更高。\r\n\r\n\r\n【福利】\r\n*績效獎金、每月業績獎金\r\n*享勞健保及6%勞退\r\n*不定期部門聚餐\r\n*每月一次免費潔顏福利\r\n*提供員工工作圍裙\r\n*良好升遷管道(培訓幹部、店長等...)\r\n*三節禮金（春節禮金加碼）\r\n*全額補助考取美容證照\r\n*已有美容證照給予專業加給\r\n丙級500/乙級1000（每月給予）\r\n*久任獎金（滿兩年2000、滿三年5000每年給予）\r\n*每年按考績調升薪資\r\n\r\n【工作內容】\r\n1.基本臉部清潔、美容課程操作(洗卸、粉刺清潔、儀器導入等)、相關知識解說。\r\n2.客戶肌膚檢測分析、建議課程資訊。\r\n3.客戶服務管理、行政庶務及系統操作。\r\n4.店務處理(盤點進貨、環境整潔、日報回覆...等)。\r\n5.無需推銷產品\r\n\r\n【所需特質】\r\n1.無經驗可(乙、丙級美容證照or美容銷售經驗佳)\r\n2.熱愛學習，能保持彈性吸收新鮮事物\r\n3.喜歡與顧客互動，口條流暢善溝通，不害怕主動與人接觸\r\n4.具有高度向心力，具團隊合作精神\r\n\r\n【薪獎制度】\r\n1. 當月激勵：當月作滿105人次以上，每多一人獎金200元(月結)\r\n2. 當月儲值：第一名2000、第二名1500、第三名1000\r\n3. 當月個人營業額獎勵(不含儲值金)： (1)9萬1.5% (2)10萬2% （3）15萬3%\r\n4.全勤：1000元",
      "descSnippet": "我們有完整的員工訓練且注重工作氣氛和員工福利對人才極為重視，薪水方面隨著年資增長未來只會更高。\r\n\r\n\r\n【福利】\r\n*績效獎金、每月業績獎金\r\n*享勞健保及6%勞退\r\n*不定期部門聚餐\r\n*每月一次免費潔顏福利\r\n*提供員工工作圍裙\r\n*良好升遷管道(培訓幹部、店長等...)\r\n*三節禮金（春節禮金加碼）\r\n*全額補助考取美容證照\r\n*已有美容證照給予專業加給\r\n丙級500/乙級1000（每月給予）\r\n*久任獎金（滿兩年2000、滿三年5000每年給予）\r\n*每年按考績調升薪資\r\n\r\n【工作內容】\r\n1.基本臉部清潔、美容課程操作(洗卸、粉刺清潔、儀器導入等)、相關知識解說。\r\n2.客戶肌膚檢測分析、建議課程資訊。\r\n3.客戶服務管理、行政庶務及系統操作。\r\n4.店務處理(盤點進貨、環境整潔、日報回覆...等)。\r\n5.無需推銷產品\r\n\r\n【所需特質】\r\n1.無經驗可(乙、丙級美容證照or美容銷售經驗佳)\r\n2.熱愛學習，能保持彈性吸收新鮮事物\r\n3.喜歡與顧客互動，口條流暢善溝通，不害怕主動與人接觸\r\n4.具有高度向心力，具團隊合作精神\r\n\r\n【薪獎制度】\r\n1. 當月激勵：當月作滿105人次以上，每多一人獎金200元(月結)\r\n2. 當月儲值：第一名2000、第二名1500、第三名1000\r\n3. 當月個人營業額獎勵(不含儲值金)： (1)9萬1.5% (2)10萬2% （3）15萬3%\r\n4.全勤：1000元",
      "mrtDist": 0.09,
      "jobAddress": "和平街19號",
      "jobAddrNo": 6001002015,
      "jobAddrNoDesc": "新北市中和區",
      "jobName": "美容師/潔顏師（享淨顏-新北中和店）",
      "jobNameSnippet": "美容師/潔顏師（享淨顏-新北中和店）",
      "jobNo": "14051744",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.9906111,
      "lon": 121.5085711,
      "link": {
        "job": "https://www.104.com.tw/job/8d6e8",
        "cust": "https://www.104.com.tw/company/1a2x6bmz13",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8d6e8?channel=104rpt"
      },
      "major": [],
      "mrt": "99001004001",
      "mrtDesc": "捷運南勢角站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 32000,
      "tags": {
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運南勢角站約90公尺"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "日班(10:30~19:30)晚班(11:00~20:00)",
      "hrBehaviorPR": 0.8974587323723998,
      "jobCat": [
        2006003001,
        2006003008,
        2001001002
      ],
      "labels": [
        "college@student_invited",
        "c@wf29",
        "c@wf26",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [],
      "employeeCount": 10,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753865431,
        "lastCustReplyTimestamp": 1753865505,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1008003001,
      "coIndustryDesc": "工商顧問服務業",
      "custName": "泰紅心國際有限公司",
      "custNo": "130000000242963",
      "description": "工作內容：\n\n我們正在尋找一位充滿熱情、具有領導能力的門市店長，加入米蘭廚房的大家庭！\n作為店長，您將負責帶領團隊打造高效、溫馨的用餐環境，並確保門市營運順暢，讓每一位顧客都感受到我們的用心與專業。\n\n主要工作職責包含：\n\t•\t門市營運管理：人員排班、進出貨管理、庫存控管\n\t•\t銷售目標設定與達成：分析業績數據、擬定促銷策略\n\t•\t團隊管理與訓練：帶領團隊提升服務品質與工作效率\n\t•\t顧客服務與客訴處理\n\t•\t品牌形象維護與標準SOP執行\n\n工作地點：\n\n台中市西屯區（鄰近逢甲商圈／交通便利）\n\n薪資福利：\n\t•\t月薪 通常 50,000 （績效獎金、營收分紅、年終獎金可能更多）\n\t•\t員工餐飲優惠、完整培訓制度、升遷管道明確\n\n上班時間：\n\n排班制，月休8天（可彈性協調）\n\n我們希望你具備：\n\t•\t具門市管理經驗者優先（如餐飲、零售、美業等）\n\t•\t擅長溝通協調，能激勵團隊士氣\n\t•\t目標導向，具執行力\n\t•\t對顧客服務有熱忱，有責任感與抗壓性",
      "descSnippet": "工作內容：\n\n我們正在尋找一位充滿熱情、具有領導能力的門市店長，加入米蘭廚房的大家庭！\n作為店長，您將負責帶領團隊打造高效、溫馨的用餐環境，並確保門市營運順暢，讓每一位顧客都感受到我們的用心與專業。\n\n主要工作職責包含：\n\t•\t門市營運管理：人員排班、進出貨管理、庫存控管\n\t•\t銷售目標設定與達成：分析業績數據、擬定促銷策略\n\t•\t團隊管理與訓練：帶領團隊提升服務品質與工作效率\n\t•\t顧客服務與客訴處理\n\t•\t品牌形象維護與標準SOP執行\n\n工作地點：\n\n台中市西屯區（鄰近逢甲商圈／交通便利）\n\n薪資福利：\n\t•\t月薪 通常 50,000 （績效獎金、營收分紅、年終獎金可能更多）\n\t•\t員工餐飲優惠、完整培訓制度、升遷管道明確\n\n上班時間：\n\n排班制，月休8天（可彈性協調）\n\n我們希望你具備：\n\t•\t具門市管理經驗者優先（如餐飲、零售、美業等）\n\t•\t擅長溝通協調，能激勵團隊士氣\n\t•\t目標導向，具執行力\n\t•\t對顧客服務有熱忱，有責任感與抗壓性",
      "mrtDist": 0,
      "jobAddress": "寶慶街50巷51號",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "高底薪＋獎金｜門市店長｜西屯區｜米蘭廚房親子餐廳",
      "jobNameSnippet": "高底薪＋獎金｜門市店長｜西屯區｜米蘭廚房親子餐廳",
      "jobNo": "14751928",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1722414,
      "lon": 120.6475223,
      "link": {
        "job": "https://www.104.com.tw/job/8s6ns",
        "cust": "https://www.104.com.tw/company/1a2x6bn6s3",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8s6ns?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 9999999,
      "salaryLow": 50000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf11": {
          "desc": "",
          "param": "wf11"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.64529391953745,
      "jobCat": [
        2005002002,
        2005002001,
        2001001002
      ],
      "labels": [
        "senior@senior_job_I",
        "c@wf2",
        "c@wf4",
        "c@wf11",
        "foreigners@foreigners_tick",
        "foreigners@overseasStudents_tick"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 10,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753849823,
        "lastCustReplyTimestamp": null,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 5,
      "coIndustry": 1004001006,
      "coIndustryDesc": "其他金融及輔助業",
      "custName": "宏旺當舖",
      "custNo": "130000000165552",
      "description": "新型態當鋪徵才中！（無經驗可）\r\n業務專員 / 儲備幹部\r\n\r\n工作內容：\r\n辦公室夏天吹冷氣冬天吹暖氣\r\n只需會操作使用手機及電腦\r\n不需曬太陽淋雨路邊發傳單\r\n與客戶聊天中了解客戶需求\r\n學習金融借款審核模式\r\n與主管學習觀念及管理\r\n升遷管道暢通晉升無阻礙\r\n\r\n工作時間：\r\n0900-1700、1300-2100（輪班制，靈活調整）\r\n依照業績調整工作時數，從8小時可降至6小時，最低上班只要4小時，賺錢輕鬆、時間自由！\r\n\r\n薪資待遇：\r\n保障底薪+業績獎金+每月分紅\r\n半年後月薪：$55,000以上\r\n兩年後月薪：$100,000以上，挑戰無上限！\r\n\r\n工作福利：\r\n分紅獎金、激勵獎金、年終獎金、招募獎金\r\n三節禮品、生日禮金、業績獎金、特殊獎金\r\n不定期員工聚餐，工作氛圍良好，無惡性競爭\r\n\r\n休假制度：\r\n月休共8天每週日固定公休\r\n其餘天數自由排休\r\n\r\n為什麼選我們？\r\n零經驗可，教到會\r\n室內上班，不外跑\r\n獎金無上限，在職越久薪水越多\r\n我們是創新當鋪，主打內勤業務＋高收入。\r\n完整培訓制度，無經驗也能快速上手。\r\n保障基本底薪＋高獎金，輕鬆十萬月薪不是夢！\r\n歡迎銀行、融資、保險、代書、金主、車行、\r\n房仲、保經、電銷、代辦、各行各業業務轉行。\r\n想賺錢，來就對了！",
      "descSnippet": "新型態當鋪徵才中！（無經驗可）\r\n業務專員 / 儲備幹部\r\n\r\n工作內容：\r\n辦公室夏天吹冷氣冬天吹暖氣\r\n只需會操作使用手機及電腦\r\n不需曬太陽淋雨路邊發傳單\r\n與客戶聊天中了解客戶需求\r\n學習金融借款審核模式\r\n與主管學習觀念及管理\r\n升遷管道暢通晉升無阻礙\r\n\r\n工作時間：\r\n0900-1700、1300-2100（輪班制，靈活調整）\r\n依照業績調整工作時數，從8小時可降至6小時，最低上班只要4小時，賺錢輕鬆、時間自由！\r\n\r\n薪資待遇：\r\n保障底薪+業績獎金+每月分紅\r\n半年後月薪：$55,000以上\r\n兩年後月薪：$100,000以上，挑戰無上限！\r\n\r\n工作福利：\r\n分紅獎金、激勵獎金、年終獎金、招募獎金\r\n三節禮品、生日禮金、業績獎金、特殊獎金\r\n不定期員工聚餐，工作氛圍良好，無惡性競爭\r\n\r\n休假制度：\r\n月休共8天每週日固定公休\r\n其餘天數自由排休\r\n\r\n為什麼選我們？\r\n零經驗可，教到會\r\n室內上班，不外跑\r\n獎金無上限，在職越久薪水越多\r\n我們是創新當鋪，主打內勤業務＋高收入。\r\n完整培訓制度，無經驗也能快速上手。\r\n保障基本底薪＋高獎金，輕鬆十萬月薪不是夢！\r\n歡迎銀行、融資、保險、代書、金主、車行、\r\n房仲、保經、電銷、代辦、各行各業業務轉行。\r\n想賺錢，來就對了！",
      "mrtDist": 0.28,
      "jobAddress": "幸福路66號1樓",
      "jobAddrNo": 6001002021,
      "jobAddrNoDesc": "新北市新莊區",
      "jobName": "新手業務第一首選，我們一起成長！",
      "jobNameSnippet": "新手業務第一首選，我們一起成長！",
      "jobNo": "14505866",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0495205,
      "lon": 121.457265,
      "link": {
        "job": "https://www.104.com.tw/job/8mwsq",
        "cust": "https://www.104.com.tw/company/1a2x6blj1s",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8mwsq?channel=104rpt"
      },
      "major": [],
      "mrt": "99001006013",
      "mrtDesc": "捷運幸福站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 100000,
      "salaryLow": 28590,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "landmark": {
          "desc": "距捷運幸福站約280公尺"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "09:00-17:00/13:00-21:00（依業績調整上班時間，最低上班時數四小時）",
      "hrBehaviorPR": 0.9961574713449076,
      "jobCat": [
        2005003016,
        2001001002,
        2005003015
      ],
      "labels": [
        "c@wf1",
        "c@wf4",
        "foreigners@foreigners",
        "c@wf2",
        "college@student_invited",
        "c@wf29",
        "c@wf7",
        "foreigners@chineseDiasporas",
        "c@wf10"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        64,
        2048
      ],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753748167,
        "lastCustReplyTimestamp": 1753893623,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 18,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "1.規劃、執行與協調行政事務作業\n2.協助集團推動相關專案，包含資訊蒐集與分析、數據彙整與報告、進度掌握等\n3.參與跨單位會議與追蹤決議事項 \n4.報表分析與會議簡報製作\n5.廠區人事作業執行(海外幹部到職及入境事宜)\n6.廠區大型活動與會議籌備\n7.執行越南總管理處有關財務、關務、庶務…等工作配合事宜\n8.其他主管交辦事項",
      "descSnippet": "1.規劃、執行與協調行政事務作業\n2.協助集團推動相關專案，包含資訊蒐集與分析、數據彙整與報告、進度掌握等\n3.參與跨單位會議與追蹤決議事項 \n4.報表分析與會議簡報製作\n5.廠區人事作業執行(海外幹部到職及入境事宜)\n6.廠區大型活動與會議籌備\n7.執行越南總管理處有關財務、關務、庶務…等工作配合事宜\n8.其他主管交辦事項",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6003002001,
      "jobAddrNoDesc": "越南",
      "jobName": "【行政/幕僚】廠區主管室主管/人員(越南)",
      "jobNameSnippet": "【行政/幕僚】廠區主管室主管/人員(越南)",
      "jobNo": "14290959",
      "jobRo": 1,
      "jobType": 2,
      "lat": 14.058324,
      "lon": 108.277199,
      "link": {
        "job": "https://www.104.com.tw/job/8iaz3",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8iaz3?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4,
        5
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.8372079340114733,
      "jobCat": [
        2001001003,
        2002001003,
        2001001001
      ],
      "labels": [
        "c@wf16",
        "c@wf9",
        "senior@senior_job_B",
        "c@wf10",
        "foreigners@foreigners",
        "c@wf27",
        "c@wf1",
        "c@wf34",
        "45plus@45invited"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861478,
        "lastCustReplyTimestamp": 1753335411,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 9,
      "coIndustry": 1006003001,
      "coIndustryDesc": "廣告行銷公關業",
      "custName": "時代社群行銷有限公司",
      "custNo": "130000000149039",
      "description": "We Need You Can Do\n①專案控管\n - 專案管理：負責專案規劃、執行與進度追蹤，確保項目目標達成\n - 策略執行：根據營運長決策，協助策略落地並監督執行情況\n - 成本與資源控管：協助優化營運成本，提高工作流程效率\n - 跨部門協作：與全公司營運部門協調合作，確保營運順暢\n - 內外部關係經營：負責與客戶、合作夥伴、供應商的溝通協調\n - 會議與行程管理：協助營運長行程、內外部會議，並製作簡報、會議紀錄與追蹤決議事項\n②行銷策略輔助，協助規劃行銷活動、品牌推廣與執行、執行活動時程控管\n③應變與問題解決，處理突發狀況，提供高效解決方案\n④時間與細節管理，有效安排工作優先順序，確保各項事務順利進行\n⑤稅務作業流程(各類所得及二代健保之扣繳/營業稅)\n⑥配合財務團隊及會計師查帳作業\n⑦其他營運長交辦事項\n\nWe Expect You\n①具備出色的組織和協調能力,能夠有效處理多項任務。\n②擁有優秀的溝通技巧,能夠舆不同層級的人員有效溝通。\n③具備保密性和敏感性,能夠處理機密信息。\n④具有良好的時間管理能力和工作細心的態度。\n⑤學習能力佳，有上進心，願意與公司一同發展成長。\n ＊薪資待遇依照工作能力與經驗而定＊\n\n媒體網站：https://cyclingtime.com/tw.html\n粉絲專頁：https://www.facebook.com/cyclingtime.tw\n公司官網：https://timedia.tw/\n\n了解執行長：www.youtube.com/@heynick2024\n---\n我們是時代社群行銷團隊，公司大多是U35的夥伴\n專門協助傳統產業(ex.自行車)數位轉型、整合行銷...\n我們還擁有一個將近17萬追蹤的粉絲專頁「單車時代CYCLINGTIME」，\n是目前全亞洲最大的自行車媒體，\n更有一個超過4萬訂閱的Youtube頻道，\n讓你的創意可以透過實際數據反饋更加了解市場需求，\n而且我們還舉辦了亞洲最大自行車百K挑戰活動「時代騎輪節」，\n邀請具有創造力，有主見有想法的夥伴一起加入我們的行列\n\n公司文化與福利\n我們創造一個開放式、鼓勵夥伴互相碰撞想法的工作環境\n團隊注重創新與成長，定期安排專家講座激發大家的想像力\n因為我們認為想像是你能力的唯一限制～\n更是非常重視團隊個人的意見與想法，熱於傾聽每個人的聲音\n---\nTIMEDIA\nTendency 趨勢＊Innovation 創新＊Marvel 驚奇＊Enthusiasm 熱情＊Desire 渴望＊Influence 影響力＊Ambition 野心\n\n我們是一個關注「趨勢」擁抱「創新」不斷創造「驚奇」並且充滿「熱情」的團隊，我們是由一群對成長極度「渴望」的Y+Z世代組成，立志於拼搏的過程持續創造對社會正面的「影響力」，「野心」早已刻在心中...",
      "descSnippet": "We Need You Can Do\n①專案控管\n - 專案管理：負責專案規劃、執行與進度追蹤，確保項目目標達成\n - 策略執行：根據營運長決策，協助策略落地並監督執行情況\n - 成本與資源控管：協助優化營運成本，提高工作流程效率\n - 跨部門協作：與全公司營運部門協調合作，確保營運順暢\n - 內外部關係經營：負責與客戶、合作夥伴、供應商的溝通協調\n - 會議與行程管理：協助營運長行程、內外部會議，並製作簡報、會議紀錄與追蹤決議事項\n②行銷策略輔助，協助規劃行銷活動、品牌推廣與執行、執行活動時程控管\n③應變與問題解決，處理突發狀況，提供高效解決方案\n④時間與細節管理，有效安排工作優先順序，確保各項事務順利進行\n⑤稅務作業流程(各類所得及二代健保之扣繳/營業稅)\n⑥配合財務團隊及會計師查帳作業\n⑦其他營運長交辦事項\n\nWe Expect You\n①具備出色的組織和協調能力,能夠有效處理多項任務。\n②擁有優秀的溝通技巧,能夠舆不同層級的人員有效溝通。\n③具備保密性和敏感性,能夠處理機密信息。\n④具有良好的時間管理能力和工作細心的態度。\n⑤學習能力佳，有上進心，願意與公司一同發展成長。\n ＊薪資待遇依照工作能力與經驗而定＊\n\n媒體網站：https://cyclingtime.com/tw.html\n粉絲專頁：https://www.facebook.com/cyclingtime.tw\n公司官網：https://timedia.tw/\n\n了解執行長：www.youtube.com/@heynick2024\n---\n我們是時代社群行銷團隊，公司大多是U35的夥伴\n專門協助傳統產業(ex.自行車)數位轉型、整合行銷...\n我們還擁有一個將近17萬追蹤的粉絲專頁「單車時代CYCLINGTIME」，\n是目前全亞洲最大的自行車媒體，\n更有一個超過4萬訂閱的Youtube頻道，\n讓你的創意可以透過實際數據反饋更加了解市場需求，\n而且我們還舉辦了亞洲最大自行車百K挑戰活動「時代騎輪節」，\n邀請具有創造力，有主見有想法的夥伴一起加入我們的行列\n\n公司文化與福利\n我們創造一個開放式、鼓勵夥伴互相碰撞想法的工作環境\n團隊注重創新與成長，定期安排專家講座激發大家的想像力\n因為我們認為想像是你能力的唯一限制～\n更是非常重視團隊個人的意見與想法，熱於傾聽每個人的聲音\n---\nTIMEDIA\nTendency 趨勢＊Innovation 創新＊Marvel 驚奇＊Enthusiasm 熱情＊Desire 渴望＊Influence 影響力＊Ambition 野心\n\n我們是一個關注「趨勢」擁抱「創新」不斷創造「驚奇」並且充滿「熱情」的團隊，我們是由一群對成長極度「渴望」的Y+Z世代組成，立志於拼搏的過程持續創造對社會正面的「影響力」，「野心」早已刻在心中...",
      "mrtDist": 0.24,
      "jobAddress": "文心路三段241號11樓之五",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "營運長特助",
      "jobNameSnippet": "營運長特助",
      "jobNo": "14545891",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1722076,
      "lon": 120.6629809,
      "link": {
        "job": "https://www.104.com.tw/job/8nroj",
        "cust": "https://www.104.com.tw/company/1a2x6bl6b3",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nroj?channel=104rpt"
      },
      "major": [],
      "mrt": "99003001007",
      "mrtDesc": "捷運文華高中站",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 38000,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "landmark": {
          "desc": "距捷運文華高中站約240公尺"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "09:00-18:00",
      "hrBehaviorPR": 0.7026488192896503,
      "jobCat": [
        2001001003,
        2003001010,
        2001001001
      ],
      "labels": [
        "c@wf1",
        "c@wf2",
        "c@wf29",
        "c@wf3",
        "c@wf26",
        "c@wf7"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2048
      ],
      "employeeCount": 12,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753894774,
        "lastCustReplyTimestamp": 1752222882,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1003001001,
      "coIndustryDesc": "農／畜／水產品批發業",
      "custName": "擎燁有限公司",
      "custNo": "130000000215386",
      "description": "依照固定路線配送蔬果至餐廳、學校、企業等熟客據點\n\n負責每日出車、回場卸貨、整理冷藏區作業（須搬運10～30公斤）\n\n協助車輛保養、載貨安全確認\n\n學習司機排班、新手訓練、路線交接與客訴處理\n\n表現穩定者可晉升儲備幹部，參與跨部門管理會議",
      "descSnippet": "依照固定路線配送蔬果至餐廳、學校、企業等熟客據點\n\n負責每日出車、回場卸貨、整理冷藏區作業（須搬運10～30公斤）\n\n協助車輛保養、載貨安全確認\n\n學習司機排班、新手訓練、路線交接與客訴處理\n\n表現穩定者可晉升儲備幹部，參與跨部門管理會議",
      "mrtDist": 0,
      "jobAddress": "民生路一段202號",
      "jobAddrNo": 6001008020,
      "jobAddrNoDesc": "台中市大雅區",
      "jobName": "【儲備司機幹部｜固定早班】可升主管／熟客路線／穩定長期",
      "jobNameSnippet": "【儲備司機幹部｜固定早班】可升主管／熟客路線／穩定長期",
      "jobNo": "13529865",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.2274529,
      "lon": 120.6509676,
      "link": {
        "job": "https://www.104.com.tw/job/81zpl",
        "cust": "https://www.104.com.tw/company/1a2x6bmli2",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/81zpl?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 60000,
      "salaryLow": 50000,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "0430-1430",
      "hrBehaviorPR": 0.8116430373731316,
      "jobCat": [
        2001001002,
        2011002001,
        2011002003
      ],
      "labels": [
        "c@wf1"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048
      ],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753357347,
        "lastCustReplyTimestamp": 1752072390,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 2,
      "coIndustry": 1002001001,
      "coIndustryDesc": "菸草製造業",
      "custName": "104測試_APP公司(請勿應徵)",
      "custNo": "130000000005797",
      "description": "[img] https://upload.cc/i1/2023/10/20/z7f6QZ.png [/img]\n1.規劃測試計畫與測試案例（test case）。\n2.測試與驗證系統功能、相容性、效能、壓力承載、可靠度等。\n3.建立測試環境，撰寫、維護、改善測試程式。\n4.分析測試資料、釐清問題、提出改善建議，並撰寫測試報告。\n5.維護相關測試設備。",
      "descSnippet": "[img] https://upload.cc/i1/2023/10/20/z7f6QZ.png [/img]\n1.規劃測試計畫與測試案例（test case）。\n2.測試與驗證系統功能、相容性、效能、壓力承載、可靠度等。\n3.建立測試環境，撰寫、維護、改善測試程式。\n4.分析測試資料、釐清問題、提出改善建議，並撰寫測試報告。\n5.維護相關測試設備。",
      "mrtDist": 0,
      "jobAddress": "大安路66號",
      "jobAddrNo": 6007001005,
      "jobAddrNoDesc": "冰島",
      "jobName": "測試職務_請勿應徵_web_qa_e2e_主動應徵",
      "jobNameSnippet": "測試職務_請勿應徵_web_qa_e2e_主動應徵",
      "jobNo": "12201294",
      "jobRo": 1,
      "jobType": 0,
      "lat": 64.963051,
      "lon": -19.020835,
      "link": {
        "job": "https://www.104.com.tw/job/79iku",
        "cust": "https://www.104.com.tw/company/1a2x6bi3s5",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/79iku?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": [],
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9951323278154313,
      "jobCat": [
        2001002005,
        2018002002,
        2001001002
      ],
      "labels": [],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 1234,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753829249,
        "lastCustReplyTimestamp": 1753913252,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "老王炸烤_三匠國際興業股份有限公司",
      "custNo": "130000000206725",
      "description": "老王炸烤/信義旗艦店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：臺北市信義區忠孝東路5段486號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "descSnippet": "老王炸烤/信義旗艦店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：臺北市信義區忠孝東路5段486號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "mrtDist": 0.21,
      "jobAddress": "忠孝東路五段486號",
      "jobAddrNo": 6001001007,
      "jobAddrNoDesc": "台北市信義區",
      "jobName": "門市店長(信義店)",
      "jobNameSnippet": "門市店長(信義店)",
      "jobNo": "13425601",
      "jobRo": 1,
      "jobType": 2,
      "lat": 25.04071,
      "lon": 121.578065,
      "link": {
        "job": "https://www.104.com.tw/job/7zr9d",
        "cust": "https://www.104.com.tw/company/1a2x6bmeth",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7zr9d?channel=104rpt"
      },
      "major": [],
      "mrt": "99001005019",
      "mrtDesc": "捷運永春站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 50000,
      "tags": {
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運永春站約210公尺"
        }
      },
      "s9": [
        1,
        2,
        8
      ],
      "s5": 0,
      "d3": "(依公司規定排班)",
      "hrBehaviorPR": 0.9897622738433338,
      "jobCat": [
        2006002002,
        2001001002,
        2005002001
      ],
      "labels": [
        "c@wf3",
        "c@wf31",
        "c@wf7",
        "foreigners@foreigners_tick",
        "c@wf1",
        "c@wf4",
        "foreigners@overseasStudents_tick",
        "c@wf27",
        "foreigners@foreignStudents",
        "college@student_invited",
        "c@wf28",
        "senior@senior_job_A",
        "c@wf29",
        "c@wf10",
        "c@wf17",
        "c@wf26",
        "foreigners@chineseDiasporas",
        "foreigners@overseasStudents",
        "foreigners@foreigners",
        "45plus@45invited",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 50,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753866167,
        "lastCustReplyTimestamp": 1753852939,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "八和和牛燒肉專門店/黑武士老火鍋/SSAM韓式燒肉專門店_八和餐飲有限公司",
      "custNo": "130000000162856",
      "description": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "descSnippet": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "mrtDist": 0.08,
      "jobAddress": "安和路1段102巷4號",
      "jobAddrNo": 6001001005,
      "jobAddrNoDesc": "台北市大安區",
      "jobName": "【八和和牛燒肉-安和店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNameSnippet": "【八和和牛燒肉-安和店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNo": "14524766",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0329786,
      "lon": 121.5521903,
      "link": {
        "job": "https://www.104.com.tw/job/8nbdq",
        "cust": "https://www.104.com.tw/company/1a2x6blgyw",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nbdq?channel=104rpt"
      },
      "major": [],
      "mrt": "99001002003",
      "mrtDesc": "捷運信義安和站",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf20": {
          "desc": "",
          "param": "wf20"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "landmark": {
          "desc": "距捷運信義安和站約80公尺"
        }
      },
      "s9": [
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.8844506427194253,
      "jobCat": [
        2001001001,
        2001001002,
        2006002002
      ],
      "labels": [
        "c@wf20",
        "c@wf17",
        "foreigners@foreigners_tick",
        "c@wf29",
        "c@wf3",
        "foreigners@foreigners",
        "c@wf1",
        "foreigners@chineseDiasporas",
        "c@wf7",
        "c@wf2",
        "c@wf9",
        "c@wf28",
        "foreigners@overseasStudents_tick",
        "c@wf4",
        "c@wf14"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        65536,
        131072
      ],
      "employeeCount": 130,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753859626,
        "lastCustReplyTimestamp": 1753859315,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 0,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "老王炸烤_三匠國際興業股份有限公司",
      "custNo": "130000000206725",
      "description": "老王炸烤/南京店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：台北市松山區健康路220號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "descSnippet": "老王炸烤/南京店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：台北市松山區健康路220號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "mrtDist": 0.36,
      "jobAddress": "健康路220號",
      "jobAddrNo": 6001001004,
      "jobAddrNoDesc": "台北市松山區",
      "jobName": "門市主管(南京店)",
      "jobNameSnippet": "門市主管(南京店)",
      "jobNo": "13425603",
      "jobRo": 1,
      "jobType": 2,
      "lat": 25.0538007,
      "lon": 121.5614413,
      "link": {
        "job": "https://www.104.com.tw/job/7zr9f",
        "cust": "https://www.104.com.tw/company/1a2x6bmeth",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7zr9f?channel=104rpt"
      },
      "major": [],
      "mrt": "99001003019",
      "mrtDesc": "捷運南京三民站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 50000,
      "tags": {
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "landmark": {
          "desc": "距捷運南京三民站約360公尺"
        }
      },
      "s9": [
        1,
        2,
        8
      ],
      "s5": 0,
      "d3": "(依公司規定排班)",
      "hrBehaviorPR": 0.9179669378472262,
      "jobCat": [
        2006002002,
        2001001002,
        2005002001
      ],
      "labels": [
        "c@wf26",
        "c@wf28",
        "c@wf2",
        "senior@senior_job_A",
        "45plus@45invited",
        "c@wf17",
        "foreigners@overseasStudents_tick",
        "c@wf4",
        "c@wf3",
        "c@wf1",
        "foreigners@chineseDiasporas",
        "c@wf31",
        "c@wf27",
        "foreigners@foreignStudents",
        "c@wf10",
        "foreigners@foreigners_tick",
        "foreigners@foreigners",
        "college@student_invited",
        "c@wf7",
        "c@wf29"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 50,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753851307,
        "lastCustReplyTimestamp": 1753860040,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 4,
      "coIndustry": 1009006001,
      "coIndustryDesc": "保全樓管相關業",
      "custName": "齊家公寓大廈管理維護股份有限公司",
      "custNo": "84760406000",
      "description": "1. 提供管委會全方位的整體性大樓綜合管理服務，及住戶生活服務等業務規劃執行。\n2. 召開會議並執行會議決議事項。\n3. 警衛安全管理、監督廠商設備維護、清潔環保工作。\n4. 處理社區行政事務、規畫財務、安全、環境、設備等管理事項規劃與執行。\n5. 處理住戶客訴，並規勸喧嘩及濫用公物等行為。\n6. 執行公共設施的各項使用管理辦法。\n7. 其他交辦事項。",
      "descSnippet": "1. 提供管委會全方位的整體性大樓綜合管理服務，及住戶生活服務等業務規劃執行。\n2. 召開會議並執行會議決議事項。\n3. 警衛安全管理、監督廠商設備維護、清潔環保工作。\n4. 處理社區行政事務、規畫財務、安全、環境、設備等管理事項規劃與執行。\n5. 處理住戶客訴，並規勸喧嘩及濫用公物等行為。\n6. 執行公共設施的各項使用管理辦法。\n7. 其他交辦事項。",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6001005007,
      "jobAddrNoDesc": "桃園市桃園區",
      "jobName": "社區經理-桃園區-桃園齊家",
      "jobNameSnippet": "社區經理-桃園區-桃園齊家",
      "jobNo": "12262130",
      "jobRo": 1,
      "jobType": 2,
      "lat": 24.9934099,
      "lon": 121.2969674,
      "link": {
        "job": "https://www.104.com.tw/job/7atiq",
        "cust": "https://www.104.com.tw/company/12xs5nu8",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7atiq?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 47000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.7433507960035756,
      "jobCat": [
        2017002005,
        2001001002,
        2002001001
      ],
      "labels": [
        "senior@senior_job_A",
        "c@wf7",
        "45plus@45invited",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048,
        4096,
        32768
      ],
      "employeeCount": 750,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753775815,
        "lastCustReplyTimestamp": 1752584612,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002003,
      "coIndustryDesc": "其他餐飲業",
      "custName": "花磚甜點有限公司",
      "custNo": "130000000237825",
      "description": "1.負責商品介紹銷售與諮詢、收銀結帳包裝等。\r\n2.協助商品訂貨、庫存盤點管理，商品排面整理。\r\n3.協助器具清洗、門市清潔、商品陳列佈置等。\r\n4.協助門市進行活動規劃，並提出營運建議。\r\n5.學習人力調度與人員管理。\r\n\r\n\r\n門市儲備幹部是本公司培養未來銷售管理人才的重要崗位，透過訓練與實務操作，讓您成為專業的銷售幹部，展現您的銷售潛力。\r\n歡迎有興趣的您加入我們的團隊，一同成長與發展！",
      "descSnippet": "1.負責商品介紹銷售與諮詢、收銀結帳包裝等。\r\n2.協助商品訂貨、庫存盤點管理，商品排面整理。\r\n3.協助器具清洗、門市清潔、商品陳列佈置等。\r\n4.協助門市進行活動規劃，並提出營運建議。\r\n5.學習人力調度與人員管理。\r\n\r\n\r\n門市儲備幹部是本公司培養未來銷售管理人才的重要崗位，透過訓練與實務操作，讓您成為專業的銷售幹部，展現您的銷售潛力。\r\n歡迎有興趣的您加入我們的團隊，一同成長與發展！",
      "mrtDist": 0,
      "jobAddress": "忠誠路二段154巷7號1樓",
      "jobAddrNo": 6001001008,
      "jobAddrNoDesc": "台北市士林區",
      "jobName": "店員門市",
      "jobNameSnippet": "店員門市",
      "jobNo": "14198781",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.115113,
      "lon": 121.5323589,
      "link": {
        "job": "https://www.104.com.tw/job/8gbul",
        "cust": "https://www.104.com.tw/company/1a2x6bn2td",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8gbul?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 40000,
      "salaryLow": 34000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "10:00-19:00",
      "hrBehaviorPR": 0.6176480560195472,
      "jobCat": [
        2005002001,
        2005002004,
        2001001002
      ],
      "labels": [
        "c@wf2",
        "c@wf29",
        "c@wf1",
        "c@wf3",
        "c@wf7"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753864415,
        "lastCustReplyTimestamp": 1753890399,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 5,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "八和和牛燒肉專門店/黑武士老火鍋/SSAM韓式燒肉專門店_八和餐飲有限公司",
      "custNo": "130000000162856",
      "description": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "descSnippet": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "mrtDist": 0.38,
      "jobAddress": "敦化北路100號",
      "jobAddrNo": 6001001004,
      "jobAddrNoDesc": "台北市松山區",
      "jobName": "【八和和牛燒肉-敦北店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNameSnippet": "【八和和牛燒肉-敦北店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNo": "14524772",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.053227,
      "lon": 121.5485558,
      "link": {
        "job": "https://www.104.com.tw/job/8nbdw",
        "cust": "https://www.104.com.tw/company/1a2x6blgyw",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nbdw?channel=104rpt"
      },
      "major": [],
      "mrt": "99001003018",
      "mrtDesc": "捷運台北小巨蛋站",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "wf20": {
          "desc": "",
          "param": "wf20"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運台北小巨蛋站約380公尺"
        }
      },
      "s9": [
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.8895634661416718,
      "jobCat": [
        2001001002,
        2001001001,
        2006002002
      ],
      "labels": [
        "college@student_invited",
        "foreigners@overseasStudents_tick",
        "c@wf7",
        "c@wf9",
        "c@wf17",
        "c@wf4",
        "c@wf28",
        "c@wf1",
        "c@wf3",
        "c@wf14",
        "c@wf20",
        "foreigners@foreigners_tick",
        "c@wf29",
        "foreigners@foreigners",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        65536,
        131072
      ],
      "employeeCount": 130,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753677685,
        "lastCustReplyTimestamp": 1753859339,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 6,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "1.負責產線作業及人員管理，達成生產目標\n2.執行生管排程計劃，反饋並解決生產異常\n3.推動精實專案，落實各項專案計劃並提供反饋\n4.負責生產安全，減少工傷發生\n5.配合內外部環安衛稽查及改善\n\n本職位負責負責廠務管理和產線作業等相關工作，為公司製造類的核心職務，具有重要的意義和發揮空間。\n如果您符合以上要求且對此職位有興趣，請提交您的履歷表並聯繫我們，期待您加入我們的專業團隊，謝謝！",
      "descSnippet": "1.負責產線作業及人員管理，達成生產目標\n2.執行生管排程計劃，反饋並解決生產異常\n3.推動精實專案，落實各項專案計劃並提供反饋\n4.負責生產安全，減少工傷發生\n5.配合內外部環安衛稽查及改善\n\n本職位負責負責廠務管理和產線作業等相關工作，為公司製造類的核心職務，具有重要的意義和發揮空間。\n如果您符合以上要求且對此職位有興趣，請提交您的履歷表並聯繫我們，期待您加入我們的專業團隊，謝謝！",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6003002010,
      "jobAddrNoDesc": "印尼",
      "jobName": "【生產製造】產品製造部培訓幹部(印尼)",
      "jobNameSnippet": "【生產製造】產品製造部培訓幹部(印尼)",
      "jobNo": "12435145",
      "jobRo": 1,
      "jobType": 2,
      "lat": -0.789275,
      "lon": 113.921327,
      "link": {
        "job": "https://www.104.com.tw/job/7ej0p",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7ej0p?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4,
        5
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.8037339186910918,
      "jobCat": [
        2001001002,
        2009002001,
        2009001001
      ],
      "labels": [
        "c@wf34",
        "c@wf10",
        "c@wf16",
        "c@wf27",
        "c@wf1",
        "c@wf9",
        "foreigners@foreigners"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753864419,
        "lastCustReplyTimestamp": 1753064041,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "永芯茶檔有限公司",
      "custNo": "130000000229814",
      "description": "\n準備食材並烹調菜餚\n掌控菜餚的擺盤與出菜的順序\n測量食材的容量與重量並協助處理廚房事務完成主廚指派工作\n\n【中壢內壢店】桃園市中壢區成章四街61號\n\n工作時段:(工作時段依現場營運狀況調整)\n10:00-21:30\n\n一天工作八小時+1至2小時休息時間\n月休8-10天(按法定休假日排休)\n\n其他福利項目 員工勞健保並額外替員工承擔「團保職災險」。\n每日供餐、定期聚餐。\n\n年終獎金、分紅獎金、月獎金、季獎金\n讓我們一起朝目標衝刺！\n\n工作環境極佳\n培訓優秀幹部，員工的成長我們看得見\n如果你對我們的職缺有興趣，想在一個公平且友善的環境工作，歡迎你跟我們聯絡。\n\n★起薪依學經歷而定★\n★獎金依個人績效能力而定★\n★依個人工作能力不定期調薪★",
      "descSnippet": "\n準備食材並烹調菜餚\n掌控菜餚的擺盤與出菜的順序\n測量食材的容量與重量並協助處理廚房事務完成主廚指派工作\n\n【中壢內壢店】桃園市中壢區成章四街61號\n\n工作時段:(工作時段依現場營運狀況調整)\n10:00-21:30\n\n一天工作八小時+1至2小時休息時間\n月休8-10天(按法定休假日排休)\n\n其他福利項目 員工勞健保並額外替員工承擔「團保職災險」。\n每日供餐、定期聚餐。\n\n年終獎金、分紅獎金、月獎金、季獎金\n讓我們一起朝目標衝刺！\n\n工作環境極佳\n培訓優秀幹部，員工的成長我們看得見\n如果你對我們的職缺有興趣，想在一個公平且友善的環境工作，歡迎你跟我們聯絡。\n\n★起薪依學經歷而定★\n★獎金依個人績效能力而定★\n★依個人工作能力不定期調薪★",
      "mrtDist": 0,
      "jobAddress": "成章四街61號",
      "jobAddrNo": 6001005001,
      "jobAddrNoDesc": "桃園市中壢區",
      "jobName": "桃園內壢店【內場儲備幹部】熱忱內場廚房【薪資37,000元至45,000元(含全勤)】",
      "jobNameSnippet": "桃園內壢店【內場儲備幹部】熱忱內場廚房【薪資37,000元至45,000元(含全勤)】",
      "jobNo": "14045037",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.974788,
      "lon": 121.2567711,
      "link": {
        "job": "https://www.104.com.tw/job/8d17x",
        "cust": "https://www.104.com.tw/company/1a2x6bmwmu",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8d17x?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 45000,
      "salaryLow": 38000,
      "tags": {
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf30": {
          "desc": "",
          "param": "wf30"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf12": {
          "desc": "",
          "param": "wf12"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9666626734506133,
      "jobCat": [
        2006001002,
        2006001004,
        2001001002
      ],
      "labels": [
        "senior@senior_job_E",
        "c@wf26",
        "c@wf10",
        "foreigners@overseasStudents",
        "45plus@45invited",
        "c@wf18",
        "college@student_invited",
        "c@wf30",
        "c@wf2",
        "c@wf4",
        "c@wf12",
        "c@wf1",
        "foreigners@foreigners",
        "foreigners@chineseDiasporas",
        "foreigners@foreignStudents",
        "c@wf29",
        "c@wf3"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        2048
      ],
      "employeeCount": 20,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753875874,
        "lastCustReplyTimestamp": 1753867603,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "．學習各類鞋材知識及系統開單流程\n．學習工作計畫擬定及任務安排方法\n．供應商管理、交期追蹤\n．協助主管建構當地團隊及與越南區對應\n．協助主管處理異常與索賠",
      "descSnippet": "．學習各類鞋材知識及系統開單流程\n．學習工作計畫擬定及任務安排方法\n．供應商管理、交期追蹤\n．協助主管建構當地團隊及與越南區對應\n．協助主管處理異常與索賠",
      "mrtDist": 0,
      "jobAddress": "Cirebon",
      "jobAddrNo": 6003002010,
      "jobAddrNoDesc": "印尼",
      "jobName": "【行政/幕僚】採購培訓幹部(印尼)",
      "jobNameSnippet": "【行政/幕僚】採購培訓幹部(印尼)",
      "jobNo": "13315358",
      "jobRo": 1,
      "jobType": 2,
      "lat": -6.6898876,
      "lon": 108.4750846,
      "link": {
        "job": "https://www.104.com.tw/job/7xe72",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7xe72?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.6509645956618654,
      "jobCat": [
        2001001002,
        2011001003
      ],
      "labels": [
        "c@wf1",
        "c@wf34",
        "c@wf16",
        "c@wf10",
        "c@wf9",
        "c@wf27"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861718,
        "lastCustReplyTimestamp": 1751964746,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 7,
      "coIndustry": 1003001015,
      "coIndustryDesc": "綜合商品批發代理業",
      "custName": "馨昌股份有限公司",
      "custNo": "22946706000",
      "description": "\r\n1.負責主管交辦專案業務計劃擬定、推動、執行，隨時掌控進度。\r\n2.負責行政事務流程之溝通、整合及規劃。\r\n3.配合公司營運策略，協助進行企劃與專案管理、進行跨技術的統合。\r\n4.協助市場分析，專案成效追蹤及費用相關報告。\r\n5.可配合主管假日專案業務行程。",
      "descSnippet": "\r\n1.負責主管交辦專案業務計劃擬定、推動、執行，隨時掌控進度。\r\n2.負責行政事務流程之溝通、整合及規劃。\r\n3.配合公司營運策略，協助進行企劃與專案管理、進行跨技術的統合。\r\n4.協助市場分析，專案成效追蹤及費用相關報告。\r\n5.可配合主管假日專案業務行程。",
      "mrtDist": 0.49,
      "jobAddress": "基隆路二段51號6樓之6(110)",
      "jobAddrNo": 6001001007,
      "jobAddrNoDesc": "台北市信義區",
      "jobName": "董事長室特助",
      "jobNameSnippet": "董事長室特助",
      "jobNo": "10229416",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0309321,
      "lon": 121.558469,
      "link": {
        "job": "https://www.104.com.tw/job/6392g",
        "cust": "https://www.104.com.tw/company/ajhvjo0",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/6392g?channel=104rpt"
      },
      "major": [
        "商業及管理學科類",
        "企業管理相關",
        "國際貿易相關"
      ],
      "mrt": "99001002002",
      "mrtDesc": "捷運台北101/世貿站",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf32": {
          "desc": "",
          "param": "wf32"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "landmark": {
          "desc": "距捷運台北101/世貿站約490公尺"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "9:00~18:00",
      "hrBehaviorPR": 0.6778261804274771,
      "jobCat": [
        2001001003
      ],
      "labels": [
        "c@wf7",
        "c@wf10",
        "c@wf28",
        "c@wf31",
        "c@wf2",
        "c@wf18",
        "c@wf32",
        "c@wf1"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 45,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753677543,
        "lastCustReplyTimestamp": 1753777038,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 7,
      "coIndustry": 1009006001,
      "coIndustryDesc": "保全樓管相關業",
      "custName": "台灣國際物業_台灣國際公寓大廈管理維護有限公司",
      "custNo": "70654740000",
      "description": "1.本職務為擔任高級社區物業管理工作。\r\n2.具飯店俱樂休閒事業或相關社區實務經驗佳。\r\n3.具有物業經理之相關證照及經驗優先錄取。\r\n4.需具一般電腦文書作業系統及會計基礎概念。\r\n5.本公司有強大之溝通能力、工作支援性與人員教育訓練。\r\n6.免費培訓升為物業經理之專業知識。\r\n7.公司定期參與工會舉辦團體物業經理之報考相關證照與獎勵。\r\n8.公司之薪資福利待遇佳，值得您選擇！\r\n9.若有相關之證照﹝防火管理人及總幹事證照﹞請影印一份攜帶至本公司。\r\n（一）深入物業組織學習能力、專業教育訓練，有效累積物業管理的專業知識及經驗。\r\n（二）不與同業惡性競爭，培養具有產業差異化且互補的核心專長業務。\r\n（三）研訂長期發展策略，以勞力密集的建築物管理維護業務為基礎，朝資訊化、人文涵養方面多角化發展，拓展高附加價值的商業支援、資產管理及生活服務的業務。\r\n（四）提升業界形象，改善工作條件，以吸引更多優秀人才投入服務、提升企業優質文化與同仁工作尊嚴。\r\n（五）培育專業物業經理人以儲備未來業務發展的需要，與國際品牌競爭。",
      "descSnippet": "1.本職務為擔任高級社區物業管理工作。\r\n2.具飯店俱樂休閒事業或相關社區實務經驗佳。\r\n3.具有物業經理之相關證照及經驗優先錄取。\r\n4.需具一般電腦文書作業系統及會計基礎概念。\r\n5.本公司有強大之溝通能力、工作支援性與人員教育訓練。\r\n6.免費培訓升為物業經理之專業知識。\r\n7.公司定期參與工會舉辦團體物業經理之報考相關證照與獎勵。\r\n8.公司之薪資福利待遇佳，值得您選擇！\r\n9.若有相關之證照﹝防火管理人及總幹事證照﹞請影印一份攜帶至本公司。\r\n（一）深入物業組織學習能力、專業教育訓練，有效累積物業管理的專業知識及經驗。\r\n（二）不與同業惡性競爭，培養具有產業差異化且互補的核心專長業務。\r\n（三）研訂長期發展策略，以勞力密集的建築物管理維護業務為基礎，朝資訊化、人文涵養方面多角化發展，拓展高附加價值的商業支援、資產管理及生活服務的業務。\r\n（四）提升業界形象，改善工作條件，以吸引更多優秀人才投入服務、提升企業優質文化與同仁工作尊嚴。\r\n（五）培育專業物業經理人以儲備未來業務發展的需要，與國際品牌競爭。",
      "mrtDist": 0,
      "jobAddress": "（依照公司規定分派）",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "台中市豪宅物業經理",
      "jobNameSnippet": "台中市豪宅物業經理",
      "jobNo": "4855867",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.162568013470807,
      "lon": 120.64123167036131,
      "link": {
        "job": "https://www.104.com.tw/job/2w2t7",
        "cust": "https://www.104.com.tw/company/wgi0aao",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/2w2t7?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4,
        5
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 38000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "09:00~18:00",
      "hrBehaviorPR": 0.6610461761278131,
      "jobCat": [
        2001001001,
        2005001001,
        2017002005
      ],
      "labels": [
        "senior@senior_job_G",
        "c@wf7",
        "45plus@45invited",
        "c@wf31"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048,
        4096,
        32768
      ],
      "employeeCount": 800,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": null,
        "lastCustReplyTimestamp": 1752044986,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 8,
      "coIndustry": 1003002003,
      "coIndustryDesc": "鞋類／布類／服飾品零售業",
      "custName": "Nab_Shop女裝_自然堂國際時尚有限公司",
      "custNo": "24350667000",
      "description": "工作內容： \n1. 門市管理及推動業績目標達成。 \n2. 商品介紹及銷售服務。 \n3. 排班及人員填補。 \n4. 獨立性高、具備帶人及管理經驗。 \n5.負責員工的招募、訓練、稽核與管理與調度。\n6. 可支援台中區門市巡店及人員調度。\n\n\n公司好康: \n1. 月休8-9天、 滿半年特休3天、滿一年特休7天。\n2. 享有勞、健保與6%勞退新制 。\n3. 全勤獎金、伙食津貼、業績獎金、全區月達標獎金。\n4. 員購優惠 。\n5.生日、中秋、端午禮金(品)。\n6. 補助津貼(結婚、生育) 。\n7. 公司聚餐，培養姊妹滔同事們的情誼。\n\n公司悄悄話: \n只要是人才，有自信心，業績表現優異，公司待遇優於其他同業。 \nNab_Shop在台中、新竹已設有9家門市，陸續在拓展中，如果屬於你的感覺跟 \n風格，趕快加入我們行列吧 。\n有耐心,善於溝通,並喜歡流行服飾者,我們歡迎你。\n具備一年以上相關門市管理經驗者優先面試。\n\n相關資料請參考: \n官網: www.nab.com.tw\nIG: nabshop.ig\n",
      "descSnippet": "工作內容： \n1. 門市管理及推動業績目標達成。 \n2. 商品介紹及銷售服務。 \n3. 排班及人員填補。 \n4. 獨立性高、具備帶人及管理經驗。 \n5.負責員工的招募、訓練、稽核與管理與調度。\n6. 可支援台中區門市巡店及人員調度。\n\n\n公司好康: \n1. 月休8-9天、 滿半年特休3天、滿一年特休7天。\n2. 享有勞、健保與6%勞退新制 。\n3. 全勤獎金、伙食津貼、業績獎金、全區月達標獎金。\n4. 員購優惠 。\n5.生日、中秋、端午禮金(品)。\n6. 補助津貼(結婚、生育) 。\n7. 公司聚餐，培養姊妹滔同事們的情誼。\n\n公司悄悄話: \n只要是人才，有自信心，業績表現優異，公司待遇優於其他同業。 \nNab_Shop在台中、新竹已設有9家門市，陸續在拓展中，如果屬於你的感覺跟 \n風格，趕快加入我們行列吧 。\n有耐心,善於溝通,並喜歡流行服飾者,我們歡迎你。\n具備一年以上相關門市管理經驗者優先面試。\n\n相關資料請參考: \n官網: www.nab.com.tw\nIG: nabshop.ig\n",
      "mrtDist": 0,
      "jobAddress": "文華路45號",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "Nab 女裝-【台中區】儲備幹部",
      "jobNameSnippet": "Nab 女裝-【台中區】儲備幹部",
      "jobNo": "12364218",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1772192,
      "lon": 120.6464504,
      "link": {
        "job": "https://www.104.com.tw/job/7d0ai",
        "cust": "https://www.104.com.tw/company/b6prawo",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7d0ai?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        2,
        3,
        4,
        5
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.925921316149764,
      "jobCat": [
        2005002002,
        2005002001,
        2001001002
      ],
      "labels": [
        "senior@senior_job_E",
        "c@wf2",
        "c@wf14",
        "45plus@45invited",
        "c@wf28",
        "c@wf29",
        "c@wf4",
        "c@wf7"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048
      ],
      "employeeCount": 20,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753867143,
        "lastCustReplyTimestamp": 1753694861,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    }
  ],
  "metadata": {
    "pagination": {
      "count": 22,
      "currentPage": 2,
      "lastPage": 150,
      "total": 21314
    },
    "isPreciseHotJob": false,
    "filterQuery": {
      "order": 11,
      "asc": 0,
      "jobcat": [
        "2001001000"
      ],
      "ro": [
        0
      ],
      "scstrict": 0,
      "scneg": 0,
      "page": 2,
      "pagesize": 20
    },
    "personalBoost": 0
  }
}


================================================
FILE: crawler/project_104/parser_apidata_104.py
================================================
import re
from datetime import datetime
from typing import Optional
import structlog

from crawler.database.schemas import (
    JobPydantic,
    SourcePlatform,
    JobStatus,
    JobType,
    SalaryType,
)

logger = structlog.get_logger(__name__)

# 104 API 的 jobType 到我們內部 JobType Enum 的映射
JOB_TYPE_MAPPING = {
    0: JobType.FULL_TIME, # 0 也代表全職
    1: JobType.FULL_TIME,
    2: JobType.PART_TIME,
    3: JobType.INTERNSHIP,
    4: JobType.CONTRACT,  # 派遣
    5: JobType.TEMPORARY,  # 兼職/計時
}

# 104 API 的教育程度 (optionEdu) 映射
EDUCATION_MAPPING_104 = {
    1: "不拘",
    2: "國中",
    3: "高中",
    4: "專科",
    5: "大學",
    6: "碩士",
    7: "博士",
}

# 104 API 的工作經驗 (period) 映射
EXPERIENCE_MAPPING_104 = {
    0: "不拘",
    1: "1年以下",
    2: "1-3年",
    3: "3-5年",
    4: "5-10年",
    5: "10年以上",
}


def parse_salary(
    salary_text: str,
) -> (Optional[int], Optional[int], Optional[SalaryType]):
    salary_min, salary_max, salary_type = None, None, None
    text = salary_text.replace(",", "").lower()

    # 月薪 (範圍)
    match_monthly_range = re.search(r"月薪([0-9]+)(?:[至~])([0-9]+)元", text)
    if match_monthly_range:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_range.group(1))
        salary_max = int(match_monthly_range.group(2))
        return salary_min, salary_max, salary_type

    # 月薪 (單一數值)
    match_monthly_single = re.search(r"月薪([0-9]+)元", text)
    if match_monthly_single:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_single.group(1))
        salary_max = int(match_monthly_single.group(1))
        return salary_min, salary_max, salary_type

    # 月薪 (以上)
    match_monthly_above = re.search(r"月薪([0-9]+)元以上", text)
    if match_monthly_above:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_above.group(1))
        salary_max = 9999999 # 設定一個足夠大的上限值
        return salary_min, salary_max, salary_type

    # 年薪
    match_yearly = re.search(r"年薪([0-9]+)萬(?:[至~])([0-9]+)萬", text) or re.search(
        r"年薪([0-9]+)萬以上", text
    )
    if match_yearly:
        salary_type = SalaryType.YEARLY
        salary_min = int(match_yearly.group(1)) * 10000
        if len(match_yearly.groups()) > 1 and match_yearly.group(2):
            salary_max = int(match_yearly.group(2)) * 10000
        return salary_min, salary_max, salary_type

    # 時薪
    match_hourly = re.search(r"時薪([0-9]+)元", text)
    if match_hourly:
        salary_type = SalaryType.HOURLY
        salary_min = int(match_hourly.group(1))
        salary_max = int(match_hourly.group(1))
        return salary_min, salary_max, salary_type

    # 日薪
    match_daily = re.search(r"日薪([0-9]+)元", text)
    if match_daily:
        salary_type = SalaryType.DAILY
        salary_min = int(match_daily.group(1))
        salary_max = int(match_daily.group(1))
        return salary_min, salary_max, salary_type

    # 論件計酬
    if "論件計酬" in text:
        salary_type = SalaryType.BY_CASE
        return None, None, salary_type

    # 面議
    if "面議" in text:
        salary_type = SalaryType.NEGOTIABLE
        return None, None, salary_type

    return salary_min, salary_max, salary_type


def parse_job_item_to_pydantic(job_item: dict) -> Optional[JobPydantic]:
    """
    從 104 API 的單一職缺項目(dict)解析並轉換為 JobPydantic 物件。
    此函式可處理來自「列表頁 API」和「單一職缺 API」的回應。
    """
    try:
        # 判斷資料來源是列表頁 API 還是單一職缺 API
        # 列表頁 API 的特徵是 jobName, jobNo, link, description, appearDate, custName 直接在 job_item 頂層
        # 單一職缺 API 的特徵是包含 'header' 和 'jobDetail' 鍵
        is_single_job_api = "header" in job_item and "jobDetail" in job_item

        if is_single_job_api:  # 來自單一職缺 API (data.data)
            header = job_item.get("header", {})
            job_detail = job_item.get("jobDetail", {})
            condition = job_item.get("condition", {})

            job_id_match = re.search(r'/analysis/([a-zA-Z0-9]+)', header.get("analysisUrl", ""))
            job_id = job_id_match.group(1) if job_id_match else None
            url = header.get("analysisUrl", "")
            cust_no = header.get("custNo")
            cust_name = header.get("custName")
            cust_url = header.get("custUrl", "")
            title = header.get("jobName")
            description = job_detail.get("jobDescription")
            raw_job_type = job_detail.get("jobType")
            job_addr_region = job_detail.get("addressRegion", "")
            job_address_detail = job_detail.get("addressDetail", "")
            appear_date_str = header.get("appearDate")
            salary_text_raw = job_detail.get("salary", "")
            experience_required_text = condition.get("workExp")
            education_required_text = condition.get("edu")

        else:  # 來自列表頁 API
            job_id = job_item.get("jobNo")
            url = job_item.get("link", {}).get("job", "")
            cust_no = job_item.get("custNo")
            cust_name = job_item.get("custName")
            cust_url = job_item.get("link", {}).get("cust", "")
            title = job_item.get("jobName")
            description = job_item.get("description")
            raw_job_type = job_item.get("jobType")
            job_addr_region = job_item.get("jobAddrNoDesc", "")
            job_address_detail = job_item.get("jobAddress", "")
            appear_date_str = job_item.get("appearDate")

            # 列表頁 API 的薪資處理
            salary_text_raw = ""
            salary_low = job_item.get("salaryLow")
            salary_high = job_item.get("salaryHigh")

            if salary_low is not None and salary_high is not None:
                if salary_low == 0 and salary_high == 0:
                    salary_text_raw = "面議"  # 假設 0,0 表示面議
                elif salary_low == salary_high:
                    salary_text_raw = f"月薪{salary_low}元"
                else:
                    salary_text_raw = f"月薪{salary_low}至{salary_high}元"
            elif salary_low is not None:
                salary_text_raw = f"月薪{salary_low}元以上"

            # 從列表頁 API 獲取教育程度和工作經驗，並進行映射
            raw_education = job_item.get("optionEdu")
            if raw_education and isinstance(raw_education, list):
                if 1 in raw_education: # If "不拘" is an option
                    education_required_text = "不拘"
                else:
                    # Find the lowest education level required
                    min_edu_code = min(raw_education)
                    education_required_text = EDUCATION_MAPPING_104.get(min_edu_code, "不拘")
                    # Check if there are higher education levels also accepted
                    if any(code > min_edu_code for code in raw_education):
                        education_required_text += "以上"
            else:
                education_required_text = "不拘"

            raw_experience = job_item.get("period")
            experience_required_text = EXPERIENCE_MAPPING_104.get(raw_experience, "不拘")

        if not job_id:
            logger.warning("Missing job_id in job_item.", job_item_keys=job_item.keys(), full_job_item=job_item)
            return None

        # 組合地址
        location_text = (job_addr_region + job_address_detail).strip()
        if not location_text:
            location_text = None

        # 解析發布日期
        posted_at = None
        if appear_date_str:
            try:
                # 嘗試 YYYY/MM/DD 格式 (單一職缺 API)
                posted_at = datetime.strptime(appear_date_str, "%Y/%m/%d")
            except ValueError:
                try:
                    # 嘗試 YYYYMMDD 格式 (列表頁 API)
                    posted_at = datetime.strptime(appear_date_str, "%Y%m%d")
                except ValueError:
                    logger.warning(
                        "Could not parse posted_at date format.",
                        appear_date=appear_date_str,
                        job_id=job_id,
                    )

        # 解析薪資
        salary_min, salary_max, salary_type = parse_salary(salary_text_raw)

        # 處理 job_type 轉換
        job_type = JOB_TYPE_MAPPING.get(raw_job_type)
        if job_type is None: # 如果映射後為 None，則設置為 OTHER 或保持 None
            job_type = JobType.OTHER # 假設 JobType 中有 OTHER，如果沒有，請自行定義或保持 None

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_104,
            source_job_id=str(job_id),
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text_raw,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=str(cust_no),
            company_name=cust_name,
            company_url=cust_url,
        )
        return job_pydantic_data

    except (AttributeError, KeyError) as e:
        logger.error(
            "Missing key fields when parsing job_item.",
            error=e,
            job_id=job_item.get("jobNo", "N/A"),
            exc_info=True,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error when parsing job_item.",
            error=e,
            job_id=job_item.get("jobNo", "N/A"),
            exc_info=True,
        )
        return None



================================================
FILE: crawler/project_104/producer_category_104.py
================================================
from .task_category_104 import fetch_url_data_104
import structlog

from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import JOB_CAT_URL_104  # Changed import path

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_url_data_104.s(JOB_CAT_URL_104).apply_async(queue='producer_category_104')
logger.info("send task_category_104 url", url=JOB_CAT_URL_104, queue='producer_category_104')





================================================
FILE: crawler/project_104/producer_jobs_104.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_104.task_jobs_104 import fetch_url_data_104
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_job_urls():
    """
    從資料庫讀取待處理或失敗的職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_104,
            statuses=statuses_to_fetch,  # 傳入狀態列表
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_104.s(url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_104")

        logger.info(
            "已成功分發一批職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_104"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))




================================================
FILE: crawler/project_104/producer_urls_104.py
================================================
from crawler.database.repository import get_all_categories_for_platform, get_all_crawled_category_ids_pandas, get_stale_crawled_category_ids_pandas
from crawler.project_104.task_urls_104 import crawl_and_store_category_urls
from crawler.database.models import SourcePlatform, CategorySourcePydantic
import structlog
from typing import Optional, Set, List


from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

def dispatch_urls_for_all_categories(sort_key: Optional[str] = None, limit: int = 0, url_limit: int = 0, n_days: int = 7) -> None:
    """
    分發所有 104 職務類別的 URL 抓取任務。

    從資料庫中獲取所有 104 平台的類別，並為每個類別分發一個 Celery 任務，
    由 `crawl_and_store_category_urls` 任務負責實際的 URL 抓取。

    :param sort_key: 用於排序類別的鍵 (例如 'source_category_id', 'source_category_name')。
    :param limit: 限制分發的類別數量。0 表示無限制。
    :param url_limit: 限制每個分類任務抓取的 URL 數量。0 表示無限制。
    :param n_days: 判斷類別是否需要重新爬取的時間間隔（天）。
    :return: 無。
    :rtype: None
    """
    logger.info("Starting URL task distribution for all 104 categories.",  sort_key=sort_key, limit=limit, url_limit=url_limit, n_days=n_days)

    # 1. 取出所有類別 A
    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_104)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}

    # 2. 取出 tb_url_categories.source_category_id B
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104)

    # 3. B 超過7天 視為 C
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104, n_days)

    # 4. D = (A - B) | C
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids

    # 5. 分發任務
    # Filter the full pydantic objects for dispatch
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    if categories_to_dispatch:
        logger.info("Found categories to dispatch.", count=len(categories_to_dispatch))
        
        if limit > 0:
            categories_to_dispatch = categories_to_dispatch[:limit]
            logger.info(
                "Applying category limit for dispatch.",
                limit=limit,
                actual_count=len(categories_to_dispatch),
            )

        for category_info in categories_to_dispatch:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id, url_limit=url_limit)
            # 在直接執行模式下，我們直接調用函數
            crawl_and_store_category_urls(category_info.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for URL crawling.")



================================================
FILE: crawler/project_104/single_url_api_data_104.py
================================================
import requests
import sys
import structlog
import json

from requests.exceptions import (
    HTTPError,
    ConnectionError,
    Timeout,
    RequestException,
    JSONDecodeError,
)

from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import (
    HEADERS_104_JOB_API,
    JOB_API_BASE_URL_104,
)  # Changed import path

configure_logging()
logger = structlog.get_logger(__name__)


def fetch_url_data_104(url: str) -> dict:
    """
    從 104 職缺 API 抓取單一 URL 的資料。
    """
    job_id = url.split("/")[-1].split("?")[0]
    url_api = f"{JOB_API_BASE_URL_104}{job_id}"

    logger.info(
        "Fetching data for single URL.", url=url, job_id=job_id, api_url=url_api
    )

    try:
        response = requests.get(url_api, headers=HEADERS_104_JOB_API, timeout=10)
        response.raise_for_status()
        data = response.json()
        logger.info(
            "Successfully fetched data.", job_id=job_id, data_keys=list(data.keys())
        )
        return data
    except (HTTPError, ConnectionError, Timeout, RequestException) as e:
        logger.error(
            "Network or request error occurred.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}
    except JSONDecodeError as e:
        logger.error(
            "Failed to decode JSON response.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}
    except Exception as e:
        logger.error(
            "An unexpected error occurred.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}


if __name__ == "__main__":
    if len(sys.argv) != 2:
        logger.info(
            "Usage: python -m crawler.project_104.single_url_api_data_104 <job_url>"
        )
        sys.exit(1)

    job_url = sys.argv[1]
    data = fetch_url_data_104(job_url)
    if data:
        logger.info(
            "Fetched data content (sample).",
            job_url=job_url,
            data_sample=json.dumps(data, indent=2, ensure_ascii=False)[:500],
        )
    else:
        logger.warning("No data fetched for the given URL.", job_url=job_url)



================================================
FILE: crawler/project_104/single_url_api_data_104.txt
================================================
{'data': {'corpImageRight': {'corpImageRight': {'imageUrl': '', 'link': ''}}, 'header': {'corpImageTop': {'imageUrl': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/custintroduce/image2.jpg?v=20241107162243', 'link': ''}, 'jobName': '軟體自動化測試工程師(新竹)', 'appearDate': '2025/07/29', 'custName': '全景軟體股份有限公司', 'custUrl': 'https://www.104.com.tw/company/7hzjbag', 'analysisType': 1, 'analysisUrl': '//www.104.com.tw/jobs/apply/analysis/2ews9', 'isSaved': False, 'isFollowed': False, 'isApplied': False, 'applyDate': '', 'userApplyCount': 0, 'hrBehaviorPR': 0.016831555955287163}, 'contact': {'hrName': '温先生', 'email': '', 'visit': '', 'phone': [], 'other': '', 'reply': ''}, 'environmentPic': {'environmentPic': [{'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303088.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303088.jpg?v=20241107162243', 'description': '25周年運動會'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_964488071933812501.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_964488071933812501.jpg?v=20241107162243', 'description': 'Lobby-1'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_964488071933812502.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_964488071933812502.jpg?v=20241107162243', 'description': 'Lobby-2'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303089.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303089.jpg?v=20241107162243', 'description': '員工交誼廳'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303090.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303090.jpg?v=20241107162243', 'description': '尾牙活動'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303091.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303091.jpg?v=20241107162243', 'description': '社團活動-羽球社'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_893506157025877465.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_893506157025877465.jpg?v=20241107162243', 'description': '春聯DIY'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_893506157025877466.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_893506157025877466.jpg?v=20241107162243', 'description': '員工旅遊'}], 'corpImageBottom': {'imageUrl': '', 'link': ''}}, 'condition': {'acceptRole': {'role': [{'code': 2, 'description': '應屆畢業生'}, {'code': 64, 'description': '原住民'}], 'disRole': {'needHandicapCompendium': False, 'disability': []}}, 'workExp': '不拘', 'edu': '大學以上', 'major': ['資訊工程相關', '資訊管理相關'], 'language': [{'code': 1, 'language': '英文', 'ability': {'listening': '中等', 'speaking': '中等', 'reading': '中等', 'writing': '中等'}}], 'localLanguage': [], 'specialty': [], 'skill': [], 'certificate': [], 'driverLicense': [], 'other': '1.熟悉 Windows及Linux系統。\n2.具備自動化測試程式開發相關經驗者佳。(JUnit/Selenium/Sikulix/Jmeter/Postman等)\n3.具備RESTful API測試經驗、基本SQL指令操作者佳。\n4.具備DevOps相關經驗者佳。\n5.規劃產品測試相關流程，評估測試結果。\n6.有密碼學或網路安全相關經驗者佳。\n\n人格特質：\n1.發想靈活，具邏輯分析能力。\n2.具掌握時程的能力，有效控管、細心、負責任。\n3.具解決問題及應變能力，有耐心及抗壓性。\n4.具團隊合作精神，能接受指導及良好的溝通能力。\n5.熱情正面，積極追求新知與接受考驗。'}, 'welfare': {'tag': [], 'welfare': '【努力工作也要充電休息】\n· 新人入職當天，即享有特休假配套福利\n· 全年彈性放假不用補班，別人要上班，我們悠閒吃早午餐\n· 給薪活力假，充電完畢，活力滿滿回崗位\n· 彈性上下班時間，讓你更能兼顧家庭生活\n· 年度休假日數優於勞基法\n\n【優於同業的各項獎勵】\n· 年終獎金、績效獎金、員工分紅\n· 介紹獎金、專利獎金\n· 生日禮金、佳節禮金\n· 完善的調薪制度\n每位員工皆是全景軟體的重要資產，我們鼓勵員工努力的付出，並期許能與全景互相成長。\n\n【員工照顧】\n· 年度健檢及團保，我們在乎每一位同仁的健康\n· 員工婚喪喜慶補助、生育津貼等，你的家人就是我們的家人\n\n【在全景的生活多采多姿】\n· 小當家社、燃脂社、壘球社、桌遊社等多元社團活動\n· 定期下午茶、零食點心，在上班時刻撫慰你的身心靈\n· 電影包場欣賞，不用人擠人排隊買票\n· 國內外旅遊補助，想去哪就去哪\n· 聖誕節派對、年度尾牙等活動，讓我們一起共度重要節慶\n\n【各項技能的訓練及提升】\n· 新進同仁訓：職前訓練、制訂個別培訓計畫\n· 在職同仁訓：e-learning線上學習平台、專業技術分享、多樣化課程\n· 各階主管訓：激發領導統御力，提升跨部門合作效率\n', 'legalTag': []}, 'jobDetail': {'jobDescription': '1.建立測試環境，各項產品運作架構熟悉。\n2.負責系統自動化測試系統及相關系統操作(Jenkins, Git, VM, Docker)。\n3.自動化程式撰寫Selenium, Python, Postman, Script等。\n4.產品相關測試(壓力、負載、效能)，測試規範建立、改善流程。', 'jobCategory': [{'code': '2007001004', 'description': '軟體工程師'}, {'code': '2007001006', 'description': 'Internet程式設計師'}], 'salary': '月薪35,000~50,000元', 'salaryMin': 35000, 'salaryMax': 50000, 'salaryType': 50, 'jobType': 1, 'workType': [], 'addressNo': '6001006001', 'addressRegion': '新竹市', 'addressArea': '新竹市', 'addressDetail': '新竹科學園區園區二路48號2樓', 'industryArea': '新竹科學園區', 'longitude': '121.0067597', 'latitude': '24.7739304', 'manageResp': '不需負擔管理責任', 'businessTrip': '無需出差外派', 'workPeriod': '日班，09:00~18:00', 'vacationPolicy': '週休二日', 'startWorkingDay': '不限', 'hireType': 0, 'delegatedRecruit': '', 'needEmp': '不限', 'landmark': '', 'remoteWork': None}, 'switch': 'on', 'custLogo': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/logo.png?v=20241107162243', 'postalCode': '300', 'closeDate': '2021-04-08', 'industry': '電腦軟體服務業', 'custNo': '16325089000', 'reportUrl': 'https://www.104.com.tw/feedback?category=2&custName=%E5%85%A8%E6%99%AF%E8%BB%9F%E9%AB%94%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&jobName=%E8%BB%9F%E9%AB%94%E8%87%AA%E5%8B%95%E5%8C%96%E6%B8%AC%E8%A9%A6%E5%B7%A5%E7%A8%8B%E5%B8%AB%28%E6%96%B0%E7%AB%B9%29', 'industryNo': '1001001002', 'employees': '170人', 'chinaCorp': False, 'interactionRecord': {'lastProcessedResumeAtTime': None, 'lastCustReplyTimestamp': 1751353490, 'nowTimestamp': 1753789855}}, 'metadata': {'enableHTML': False, 'hiddenBanner': False, 'seo': {'noindex': False}}}


================================================
FILE: crawler/project_104/task_category_104.py
================================================
# import os
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from crawler.worker import app
from crawler.database.schemas import SourcePlatform
from crawler.project_104.config_104 import HEADERS_104, JOB_CAT_URL_104
from crawler.project_104.client_104 import (
    fetch_category_data_from_104_api,
)

logger = structlog.get_logger(__name__)


def flatten_jobcat_recursive(node_list, parent_no=None):
    """
    Recursively flattens the category tree using a generator.
    """
    for node in node_list:
        yield {
            "parent_source_id": parent_no,
            "source_category_id": node.get("no"),
            "source_category_name": node.get("des"),
            "source_platform": SourcePlatform.PLATFORM_104.value,
        }
        if "n" in node and node.get("n"):
            yield from flatten_jobcat_recursive(
                node_list=node["n"],
                parent_no=node.get("no"),
            )


@app.task()
def fetch_url_data_104(url_JobCat):
    import crawler.database.repository as repository
    from crawler.database import connection as db_connection
    
    logger.info("Current database connection", db_url=str(db_connection.get_engine().url))
    logger.info("Starting category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_104)

        jobcat_data = fetch_category_data_from_104_api(url_JobCat, HEADERS_104)
        if jobcat_data is None:
            logger.error("Failed to fetch category data from 104 API.", url=url_JobCat)
            return

        flattened_data = list(flatten_jobcat_recursive(jobcat_data))

        if not existing_categories:
            logger.info("Database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            repository.sync_source_categories(SourcePlatform.PLATFORM_104, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
            if d.get("parent_source_id")
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_104.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            categories_to_sync.sort(key=lambda x: x['source_category_id'])
            logger.info(
                "Found new or updated categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_104, categories_to_sync)
        else:
            logger.info("No new or updated categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    # python -m crawler.project_104.task_category_104
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    logger.info("Dispatching fetch_url_data_104 task for local testing.", url=JOB_CAT_URL_104)
    fetch_url_data_104(JOB_CAT_URL_104)


================================================
FILE: crawler/project_104/task_jobs_104.py
================================================
# import os
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from typing import Optional
from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_104.client_104 import fetch_job_data_from_104_api
from crawler.project_104.parser_apidata_104 import parse_job_item_to_pydantic
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)


@app.task()
def fetch_url_data_104(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed information for a single job vacancy from a given URL,
    parses it, stores it in the database, and marks the URL's crawl status.
    """
    job_id = None
    try:
        job_id = url.split("/")[-1].split("?")[0]
        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        data = fetch_job_data_from_104_api(job_id)
        if data is None:
            logger.error("Failed to fetch job data from 104 API.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

    except Exception as e:
        logger.error(
            "Unexpected error during API call or job ID extraction.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_api_data = data.get("data")
    if not job_api_data or job_api_data.get("switch") == "off":
        logger.warning("Job content does not exist or is closed.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_pydantic_data = parse_job_item_to_pydantic(job_api_data)

    if not job_pydantic_data:
        logger.error("Failed to parse job data.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    try:
        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when upserting job data.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    # python -m crawler.project_104.task_jobs_104

    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    PRODUCER_BATCH_SIZE = 20 # Changed from 10 to 20
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]
    
    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_104,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info(f"Found {len(urls_to_process)} URLs to process.")
        for url in urls_to_process:
            logger.info(f"Processing URL: {url}")
            fetch_url_data_104(url)
    else:
        logger.info("No URLs found to process for testing.")


================================================
FILE: crawler/project_104/task_urls_104.py
================================================
# import os
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from collections import deque
from typing import Set, List
from crawler.worker import app
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    upsert_jobs,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.project_104.client_104 import fetch_job_urls_from_104_api
from crawler.project_104.parser_apidata_104 import parse_job_item_to_pydantic
from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_104.config_104 import (
    URL_CRAWLER_BASE_URL_104,
    URL_CRAWLER_PAGE_SIZE_104,
    URL_CRAWLER_ORDER_BY_104,
    HEADERS_104_URL_CRAWLER,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_category_urls(job_category: dict, url_limit: int = 0) -> None:
    """
    Celery task: Iterates through all pages of a specified job category, fetches job URLs
    and preliminary data, and stores them in the database in batches.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    
    global_job_url_set = set()
    current_batch_jobs = []
    current_batch_urls = []
    current_batch_url_categories = []
    recent_counts = deque(maxlen=4)

    current_page = 1
    logger.info(
        "Task started: crawling job category URLs and data.",
        job_category_code=job_category_code,
        url_limit=url_limit,
    )

    while True:
        if url_limit > 0 and len(global_job_url_set) >= url_limit:
            logger.info(
                "URL limit reached. Ending task early.",
                job_category_code=job_category_code,
                url_limit=url_limit,
                collected_urls=len(global_job_url_set),
            )
            break

        if current_page % 5 == 1:
            logger.info(
                "Current page being processed.",
                page=current_page,
                job_category_code=job_category_code,
            )

        params = {
            "jobsource": "index_s",
            "page": current_page,
            "pagesize": URL_CRAWLER_PAGE_SIZE_104,
            "order": URL_CRAWLER_ORDER_BY_104,
            "jobcat": job_category_code,
            "mode": "s",
            "searchJobs": "1",
        }

        api_response = fetch_job_urls_from_104_api(
            URL_CRAWLER_BASE_URL_104,
            HEADERS_104_URL_CRAWLER,
            params,
            URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
            verify=False,
        )

        if api_response is None:
            logger.error(
                "Failed to retrieve data from 104 API.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        job_items = api_response.get("data", [])
        if not isinstance(job_items, list):
            logger.error(
                "API response 'data.list' format is incorrect or missing.",
                page=current_page,
                job_category_code=job_category_code,
                api_data_type=type(job_items),
            )
            break

        if not job_items:
            logger.info(
                "No more job items found. Ending task.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        for job_item in job_items:
            job_pydantic = parse_job_item_to_pydantic(job_item)
            if job_pydantic and job_pydantic.url:
                if job_pydantic.url not in global_job_url_set:
                    global_job_url_set.add(job_pydantic.url)
                    current_batch_jobs.append(job_pydantic)
                    current_batch_urls.append(job_pydantic.url)
                
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=job_pydantic.url,
                        source_category_id=job_category_code,
                    ).model_dump()
                )

        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "Batch upload size reached. Starting data upload.",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_jobs(current_batch_jobs)
            upsert_urls(SourcePlatform.PLATFORM_104, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            
            current_batch_jobs.clear()
            current_batch_urls.clear()
            current_batch_url_categories.clear()

        total_jobs = len(global_job_url_set)
        recent_counts.append(total_jobs)
        if len(recent_counts) == recent_counts.maxlen and len(set(recent_counts)) == 1:
            logger.info(
                "No new data found consecutively. Ending task early.",
                job_category_code=job_category_code,
            )
            break

        current_page += 1

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining data to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_jobs(current_batch_jobs)
        upsert_urls(SourcePlatform.PLATFORM_104, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No new data collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code)


if __name__ == "__main__":
    # python -m crawler.project_104.task_urls_104
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    n_days = 7  # Define n_days for local testing
    url_limit = 1000000

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_104)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    # Only process the first category for local testing
    if categories_to_dispatch:
        # categories_to_process_single = [categories_to_dispatch[0]]
        
        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")


================================================
FILE: crawler/project_1111/1111_人力銀行_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import urllib.parse
import urllib3

# 忽略不安全請求的警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

WEB_NAME = "1111_人力銀行"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.1111.com.tw",
}

print(f"開始執行 {WEB_NAME} ")
# Output:
#   開始執行 1111_人力銀行 


# ## 取得網站所有職業總覽
# # 1. 取得 JSON 資料
# # jobcat 檔案名稱

# file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
# url_JobCat = "https://www.1111.com.tw/api/v1/codeCategories/"

# response_jobcat = requests.get(url_JobCat, headers=HEADERS, verify=False, timeout=10)
# response_jobcat.raise_for_status()
# jobcat_data = response_jobcat.json()
# with open(file_jobcat_json, "w", encoding="utf-8") as f:
#     json.dump(jobcat_data, f, ensure_ascii=False, indent=4)

# print("JSON資料，分類總覽", pd.json_normalize(jobcat_data).columns)
# # 'nation', 'city', 'region', 'mrt', 'jobPosition', 'industry',
# # 'certification', 'major', 'workAbility', 'computerSkill',
# # 'companyBenefit', 'jobBenefit',
# # pd.json_normalize(jobcat_data)
# # pd.json_normalize(jobcat_data["region"][0]["categories"])
# # pd.json_normalize(jobcat_data["certification"])
# # pd.json_normalize(jobcat_data["major"])
# # pd.json_normalize(jobcat_data["workAbility"])
# # pd.json_normalize(jobcat_data["computerSkill"])
# print(f"職業總覽資料已儲存為 {file_jobcat_json}")

# df_jobcat = pd.json_normalize(jobcat_data["industry"])
# df_jobcat.to_excel(f"{WEB_NAME}_category.xlsx", index=False)
# print(f"職業總覽資料已轉換為 '{WEB_NAME}_category.xlsx'")

# mask = df_jobcat["parentCode"].astype(str).str.startswith("100")
# df_it_jobs = df_jobcat[mask]
# df_it_jobs.head(5)



def catch_1111_url(KEYWORDS, CATEGORY, ORDER="date", PAGE_NUM=1, USE_API=False):
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 1111 求職網的完整職缺網址或 API 網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。若無則傳入空字串 ""。
    CATEGORY (str or list): 職缺的類別代碼，例如 "140100" 或者類別代碼的列表。
                            若無則傳入空字串 ""。
    ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。
                           預設為 "date"。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。
    USE_API (bool, optional): 是否使用 API 網址。預設為 False。

    返回:
    str: 生成的 1111 求職網址或 API 網址。
    """
    BASE_URL = "https://www.1111.com.tw/search/job"
    API_URL = "https://www.1111.com.tw/api/v1/search/jobs/"

    # 確保頁碼至少為 1，避免負數或 0 造成計算錯誤
    safe_page_num = max(1, PAGE_NUM)

    if USE_API:
        params = {
            "page": safe_page_num,
            "fromOffset": 0,
            "sortBy": "ab" if ORDER == "relevance" else "da",
            "sortOrder": "desc",
            "conditionsText": KEYWORDS,
            "searchUrl": f"/search/job?page={safe_page_num}&col={'ab' if ORDER == 'relevance' else 'da'}&sort=desc&ks={urllib.parse.quote(KEYWORDS)}&d0={','.join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY}",
            "keyword": KEYWORDS,
        }

        # 如果有提供職務類別，加入到參數中
        if CATEGORY:
            if isinstance(CATEGORY, list):
                for code in CATEGORY:
                    params["jobPositions"] = code

        query_string = urllib.parse.urlencode(params)
        return f"{API_URL}?{query_string}"
    else:
        params = {
            "page": safe_page_num,
            "col": "ab" if ORDER == "relevance" else "da",
            "sort": "desc",
            "ks": KEYWORDS,
            "d0": ",".join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY,
        }

        query_string = urllib.parse.urlencode(params)
        return f"{BASE_URL}?{query_string}"


# # --- 測試範例 ---

# KEYWORDS_STR = "雲端工程師"
# CATEGORY_CODE1 = "140100"  # 單一類別
# CATEGORY_CODE2 = ["140100", "140200"]  # 多個類別

# # 1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁
# print("1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁:")
# url_1 = catch_1111_url(
#     KEYWORDS_STR, CATEGORY_CODE1, ORDER="relevance"
# )  # PAGE_NUM 省略，預設為 1
# print(url_1, "\n")

# # 2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁
# print("2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁:")
# url_2 = catch_1111_url("", CATEGORY_CODE2, ORDER="date", PAGE_NUM=2, USE_API=False)
# print(url_2, "\n")

# # 3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁
# print("3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁:")
# url_3 = catch_1111_url(KEYWORDS_STR, "", ORDER="date", PAGE_NUM=3, USE_API=False)
# print(url_3, "\n")

# # 4. 使用 API 生成網址
# print("4. 有關鍵字, 單一類別, 使用 API, 第 1 頁:")
# api_url_1 = catch_1111_url(
#     KEYWORDS_STR, CATEGORY_CODE1, ORDER="relevance", USE_API=True
# )
# print(api_url_1, "\n")

# # 5. 無關鍵字, 多個類別, 使用 API, 第 2 頁
# print("5. 無關鍵字, 多個類別, 使用 API, 第 2 頁:")
# api_url_2 = catch_1111_url("", CATEGORY_CODE2, ORDER="date", PAGE_NUM=2, USE_API=True)
# print(api_url_2, "\n")

# # 6. 有關鍵字, 無類別, 使用 API, 第 3 頁
# print("6. 有關鍵字, 無類別, 使用 API, 第 3 頁:")
# api_url_3 = catch_1111_url(KEYWORDS_STR, "", ORDER="date", PAGE_NUM=3, USE_API=True)
# print(api_url_3, "\n")

from tqdm.auto import tqdm
import urllib3
from typing import Optional, List, Union


def get_1111_api_data(
    KEYWORDS: str,
    CATEGORY: Union[str, List[str]],
    ORDER: str = "date",
    max_page: Optional[int] = None,
) -> pd.DataFrame:
    """
    從 1111 求職網高效擷取指定搜尋條件下的所有職缺資料。
    1.  將首次請求與總頁數獲取合併，減少 API 呼叫。
    2.  提供更穩健的錯誤處理機制，能跳過請求失敗的頁面。
    3.  程式碼結構簡潔，使用進度條顯示抓取進度。

    Args:
        KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。
        CATEGORY (Union[str, List[str]]): 職缺的類別代碼，例如 "140100" 或 ["140100", "140200"]。
        ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。預設為 "date"。
        max_page (Optional[int], optional): 欲抓取的最大頁數。若為 None，則自動抓取所有頁面。預設為 None。

    Returns:
        pd.DataFrame: 包含所有職缺資料的 DataFrame。如果抓取失敗或無資料，則返回空的 DataFrame。
    """

    all_data = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    # 請求第一頁，同時獲取總頁數
    try:
        url = catch_1111_url(KEYWORDS, CATEGORY, ORDER, PAGE_NUM=1, USE_API=True)
        response = requests.get(url, headers=headers, verify=False, timeout=15)
        response.raise_for_status()
        data = response.json()

        hits = data.get("result", {}).get("hits", [])
        if not hits:
            print("找不到任何相關職缺資料。")
            return pd.DataFrame()

        all_data.extend(hits)
        totalPage = data.get("result", {}).get("pagination", {}).get("totalPage", 1)

    except (
        requests.exceptions.RequestException,
        requests.exceptions.JSONDecodeError,
    ) as e:
        print(f"抓取第一頁時發生錯誤，無法繼續: {e}")
        return pd.DataFrame()

    # 決定最終要抓取的頁數
    limit_page = totalPage
    if max_page is not None and max_page > 0:
        limit_page = min(totalPage, max_page)

    if limit_page <= 1:
        return pd.json_normalize(all_data)

    # 迭代抓取剩餘頁面 (從第 2 頁開始)
    for page_num in tqdm(range(2, limit_page + 1), desc="正在擷取資料"):
        try:
            url = catch_1111_url(
                KEYWORDS, CATEGORY, ORDER, PAGE_NUM=page_num, USE_API=True
            )
            response = requests.get(url, headers=headers, verify=False, timeout=15)
            response.raise_for_status()

            page_data = response.json().get("result", {}).get("hits", [])
            if not page_data:
                print(f"\n在第 {page_num} 頁後已無更多資料，提前結束。")
                break

            all_data.extend(page_data)

        except (
            requests.exceptions.RequestException,
            requests.exceptions.JSONDecodeError,
        ) as e:
            print(f"\n抓取第 {page_num} 頁時發生錯誤，已跳過: {e}")
            continue

    if not all_data:
        return pd.DataFrame()

    return pd.json_normalize(all_data)


# --- 使用範例 ---
KEYWORDS = "雲端工程師"
CATEGORY = ["140100", "140200"]

df_data = get_1111_api_data(KEYWORDS, CATEGORY, max_page=2)
# df_data = get_1111_data(KEYWORDS, CATEGORY)
df_data.head(1)
# Output:
#   正在擷取資料:   0%|          | 0/1 [00:00<?, ?it/s]
#                 updateAt      jobId  companyId           companyName  \

#   0  2025/06/17 11:24:00  113019436   48931667  富邦媒體科技股份有限公司(富邦momo)   

#   

#                                            description                 title  \

#   0  1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資...  D4000 JAVA資料工程師 【台中】   

#   

#     role remind  replyInDays            salary  ...  require.certificates  \

#   0  [1]    [0]            0  面議（經常性薪資達4萬元或以上）  ...                    []   

#   

#      require.experience  require.grades       require.majors industry.id  \

#   0                   3    [16, 32, 64]  [130100, 130600, 0]      250214   

#   

#     industry.name  workCity.id workCity.name                highlight.title  \

#   0          百貨相關       100906        台中市北屯區  D4000 JAVA資料<em>工程師</em> 【台中】   

#   

#                                  highlight.description  

#   0  1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資...  

#   

#   [1 rows x 34 columns]

# 從指定的職缺網址獲取職缺的相關數據

def get_1111_page_data(job_url):
    """
    解析 1111 新版職缺頁面的 HTML 內容，並回傳結構化資料。

    參數:
    job_url (str): 職缺頁面的 URL。

    回傳:
    dict: 包含職缺資訊的結構化資料，包括職務名稱、公司名稱、工作性質、薪資待遇、學歷要求、上班地點、福利資訊及聯絡資訊等。
    """

    response = requests.get(job_url, verify=False)
    soup = BeautifulSoup(response.content, "html.parser")

    data = {}

    data["job_url"] = job_url

    # --- 1. 頁首區塊 (Top Section) ---
    header_section = soup.select_one(
        "section[data-v-e57f1019] > div.container > div.text-gray-600"
    )
    if header_section:
        data["職務名稱"] = header_section.select_one("h1").get_text(strip=True) or "N/A"
        data["公司名稱"] = (
            header_section.select_one("h2.inline").get_text(strip=True) or "N/A"
        )

        pills = header_section.select("div.flex.flex-wrap.mt-4.gap-3 > div")
        top_info = [p.get_text(strip=True, separator=" ") for p in pills]
        if len(top_info) >= 4:
            data["工作性質"], data["薪資待遇"], data["學歷要求"], data["上班地點"] = (
                top_info[:4]
            )

        info_items = header_section.select("ul.info-item > li")
        for item in info_items:
            key_tag = item.select_one("h3")
            val_tag = item.select_one("span") or item.select_one("time")
            if key_tag and val_tag:
                data[key_tag.get_text(strip=True)] = val_tag.get_text(strip=True)

    # --- 2. 主要內容區塊 (Main Content Sections) ---
    sections = soup.select("section[id]")
    for section in sections:
        section_title_tag = section.select_one("h2.text-lg.text-main")
        if not section_title_tag:
            continue

        section_title = section_title_tag.get_text(strip=True)
        section_data = {}

        # --- 標準內容區塊的通用解析 ---
        for div in section.select("div.content"):
            key_tag = div.find("h3")
            if key_tag:
                key = key_tag.get_text(strip=True)
                value_container = key_tag.find_next_sibling()
                if value_container:
                    list_items = value_container.select("li")
                    section_data[key] = (
                        [
                            li.get_text(strip=True, separator=" ")
                            .replace("、", "")
                            .strip()
                            for li in list_items
                        ]
                        if list_items
                        else value_container.get_text(separator="\n", strip=True)
                    )

        # --- 福利資訊的特殊結構處理 ---
        if section_title == "福利資訊":
            legal_welfare_h3 = section.find(
                "h3", string=lambda t: t and "法定福利" in t
            )
            if legal_welfare_h3:
                container = legal_welfare_h3.find_next_sibling("div")
                section_data["法定福利"] = [
                    p.get_text(strip=True) for p in container.select("p")
                ]

            company_welfare_h3 = section.find(
                "h3", string=lambda t: t and "公司福利" in t
            )
            if company_welfare_h3:
                welfare_map = {}
                categories = (
                    company_welfare_h3.find_parent("div")
                    .find_next_sibling("div")
                    .find_all("div", recursive=False, class_="flex")
                )
                for cat_div in categories:
                    cat_title = (
                        cat_div.select_one("h4").get_text(strip=True)
                        if cat_div.select_one("h4")
                        else "未分類"
                    )
                    items = [
                        p.get_text(strip=True)
                        for p in cat_div.select("div.flex-wrap p")
                    ]
                    welfare_map[cat_title] = items
                section_data["公司福利"] = welfare_map

            more_info_h3 = section.find("h3", string=lambda t: t and "更多說明" in t)
            if more_info_h3:
                container = more_info_h3.find_next_sibling("div")
                section_data["更多說明"] = (
                    container.get_text(separator="", strip=True)
                    .split("\n展開全部")[0]
                    .strip()
                )

        if section_data:
            data[section_title] = section_data

    # --- 單獨處理聯絡資訊 ---
    contact_section = soup.select_one("section#CONTACT_INFO")
    if contact_section:
        contact_data = {}
        contact_person_tag = contact_section.find(
            "h3", string=lambda t: t and "聯絡人員" in t
        )
        if contact_person_tag:
            contact_data["聯絡人員"] = contact_person_tag.find_next_sibling(
                "p"
            ).get_text(strip=True)
        if contact_data:
            data["聯絡資訊"] = contact_data
    # df = pd.json_normalize(data)
    return data


# 測試範例
job_url = "https://www.1111.com.tw/job/113019436"
get_1111_page_data(job_url)

# Output:
#   {'job_url': 'https://www.1111.com.tw/job/113019436',

#    '職務名稱': 'D4000 JAVA資料工程師 【台中】',

#    '公司名稱': '富邦媒體科技股份有限公司(富邦momo)',

#    '需求人數': '1  ~ 2 人',

#    '到職日期': '一個月內',

#    '更新日期': '2025 / 06 / 17',

#    '工作內容': {'職缺描述': '1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資料處理時所衍生之效能與維護問題\n3.雲端平台開發及維運(GCP/AWS)\n4.Data Pipeline規劃、設計、開發與監控維護\n5.研發大數據架構相關技術，優化現有架構並導入新技術\n收合內容',

#     '工作待遇': '面議 (經常性薪資達4萬元或以上)\n查看薪資水平',

#     '職務類別': ['演算法開發工程師', '軟體工程師', '網站程式設計師'],

#     '工作性質': ['全職'],

#     '工作時間': ['日班'],

#     '工作地點': '台中市\n                    北屯區\n                    文心路四段955號9樓'},

#    '要求條件': {'學歷要求': ['大學以上'],

#     '科系要求': ['數學統計學門', '電算機學門'],

#     '工作經驗': '1 年以上經驗',

#     '外語能力': ['英文 聽｜ 中等 説｜ 中等 讀｜ 中等 寫｜ 中等'],

#     '工作技能': '不拘',

#     '附加條件': '1.一年以上Java的軟體開發經驗\n2.具有 NoSQL (Redis、elasticsearch)的實務經驗\n3.具有SQL語法且具備Oracle、Postgre實務經驗\n4.具有Message Queue (例 Kafka、RabbitMQ)的實務經驗\n5.具有Git、Jenkis、Airflow 等Data Pipeline 開發實務經驗\n6.熟悉Linux作業系統操作\n\n* 如經面試錄取後，報到時須繳交「體格檢查表(一般)」，請於報到前自行至醫療機構辦理體檢 *\n展開全部',

#     '歡迎身份': ['原住民']},

#    '公司資訊': {'公司名稱': '富邦媒體科技股份有限公司(富邦momo)', '產業類別': ['百貨相關'], '公司人數': '3,100 人'},

#    '聯絡資訊': {'聯絡人員': '張小姐'}}

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime("%Y-%m-%d", time.localtime(time.time()))
JOBCAT_CODE = "140100"
KEYWORDS = "雲端工程師"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{JOBCAT_CODE}"
MAX_WORKERS = 5  # 同時運行的執行緒數量

print(f"開始執行 {FILE_NAME}")
df_api_data = get_1111_api_data(KEYWORDS, JOBCAT_CODE)
df_api_data["company_url"] = "https://www.1111.com.tw/corp/" + df_api_data["companyId"].astype(str)
df_api_data["job_url"] = "https://www.1111.com.tw/job/" + df_api_data["jobId"].astype(str)


# 使用 ThreadPoolExecutor 進行並行處理
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:  # 可以根據需要調整 max_workers 的數量
    list_page_data = list(tqdm(executor.map(get_1111_page_data, df_api_data['job_url']), total=len(df_api_data['job_url']), desc="Parsing Job URLs"))

df_page_data = pd.json_normalize(list_page_data)
all_jobs_df = pd.merge(df_api_data, df_page_data, on='job_url', how='left')

print(all_jobs_df.shape)

all_jobs_df.head(1)
# Output:
#   開始執行 (2025-06-17)_1111_人力銀行_雲端工程師_140100

#   正在擷取資料:   0%|          | 0/14 [00:00<?, ?it/s]
#   Parsing Job URLs:   0%|          | 0/437 [00:00<?, ?it/s]
#   (437, 77)

#                 updateAt     jobId  companyId companyName  \

#   0  2025/06/17 12:46:00  99025652       2169  經濟實業股份有限公司   

#   

#                                            description         title role  \

#   0  1.泵浦發電機大數據輔助設計與運維之智慧雲端平台開發相關工作\n2.泵浦、發電機運轉中數據收...  軟體分析工程師(總公司)  [1]   

#   

#     remind  replyInDays              salary  ...  工作內容.實習時段  要求條件.打字速度  \

#   0    [0]            0  月薪 45,000元~55,000元  ...        NaN        NaN   

#   

#      工作內容.發展遠景  工作內容.成就樂趣 公司資訊.品牌名稱 工作內容.遠距工作  工作性質 薪資待遇  學歷要求  上班地點  

#   0        NaN        NaN       NaN       NaN   NaN  NaN   NaN   NaN  

#   

#   [1 rows x 77 columns]

# all_jobs_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
# Output:
#   已將所有職缺資料儲存到 (2025-06-17)_1111_人力銀行_雲端工程師_140100.xlsx


all_jobs_df.columns
# Output:
#   Index(['updateAt', 'jobId', 'companyId', 'companyName', 'description', 'title',

#          'role', 'remind', 'replyInDays', 'salary', 'recruitCount', 'mrtId',

#          'mrtTime', 'mrtNear', 'benefits', 'companyTags', 'isHappiness',

#          'internship', 'jobType', 'bookmarked', 'hasMedias', 'isTop',

#          'hasCompanyLogo', 'require.drivingLicense', 'require.certificates',

#          'require.experience', 'require.grades', 'require.majors', 'industry.id',

#          'industry.name', 'workCity.id', 'workCity.name', 'highlight.title',

#          'highlight.description', 'company_url', 'job_url', '職務名稱', '公司名稱',

#          '應徵人數', '需求人數', '到職日期', '更新日期', '工作內容.職缺描述', '工作內容.工作待遇', '工作內容.職務類別',

#          '工作內容.工作性質', '工作內容.工作時間', '工作內容.休假制度', '工作內容.工作地點', '要求條件.學歷要求',

#          '要求條件.科系要求', '要求條件.工作經驗', '要求條件.外語能力', '要求條件.方言能力', '要求條件.工作技能',

#          '要求條件.歡迎身份', '公司資訊.公司名稱', '公司資訊.產業類別', '公司資訊.公司人數', '公司資訊.資本額',

#          '聯絡資訊.聯絡人員', '要求條件.附加條件', '要求條件.電腦專長', '要求條件.具備駕照', '要求條件.自備車輛',

#          '要求條件.專業證照', '要求條件.其他說明', '工作內容.實習時段', '要求條件.打字速度', '工作內容.發展遠景',

#          '工作內容.成就樂趣', '公司資訊.品牌名稱', '工作內容.遠距工作', '工作性質', '薪資待遇', '學歷要求', '上班地點'],

#         dtype='object')



================================================
FILE: crawler/project_1111/client_1111.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional, Union, List

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import urllib.parse

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_1111.config_1111 import (
    HEADERS_1111_JOB_API,
    JOB_API_BASE_URL_1111,
    JOB_CAT_URL_1111,
    HEADERS_1111,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_api_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = False,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[Dict[str, Any]]:
    """
    通用的 API 請求函式，處理隨機延遲、請求發送、JSON 解析和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before API request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        data = response.json()
        return data
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except json.JSONDecodeError:
        logger.error(
            "Failed to parse JSON response from API.",
            url=url,
            exc_info=True,
            **log_context,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None

def fetch_category_data_from_1111_api(
    api_url: str = JOB_CAT_URL_1111, headers: Dict[str, str] = HEADERS_1111
) -> Optional[Dict[str, Any]]:
    """
    從 1111 API 獲取職務分類的原始數據。
    """
    return _make_api_request(
        "GET",
        api_url,
        headers=headers,
        log_context={"api_type": "1111_category_data"},
    )

def catch_1111_url(KEYWORDS: str, CATEGORY: Union[str, List[str]], ORDER: str = "date", PAGE_NUM: int = 1, USE_API: bool = True):
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 1111 求職網的完整職缺網址或 API 網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。若無則傳入空字串 ""。
    CATEGORY (str or list): 職缺的類別代碼，例如 "140100" 或者類別代碼的列表。
                            若無則傳入空字串 ""。
    ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。
                           預設為 "date"。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。
    USE_API (bool, optional): 是否使用 API 網址。預設為 False。

    返回:
    str: 生成的 1111 求職網址或 API 網址。
    """
    BASE_URL = "https://www.1111.com.tw/search/job"
    API_URL = JOB_API_BASE_URL_1111

    # 確保頁碼至少為 1，避免負數或 0 造成計算錯誤
    safe_page_num = max(1, PAGE_NUM)

    if USE_API:
        params = {
            "page": safe_page_num,
            "fromOffset": 0,
            "sortBy": "ab" if ORDER == "relevance" else "da",
            "sortOrder": "desc",
        }

        # 如果有提供職務類別，加入到參數中
        if CATEGORY:
            if isinstance(CATEGORY, list):
                params["d0"] = ",".join(CATEGORY)
            else:
                params["d0"] = CATEGORY
        
        if KEYWORDS and KEYWORDS != "":
            params["keyword"] = KEYWORDS

        query_string = urllib.parse.urlencode(params)
        return f"{API_URL}?{query_string}"
    else:
        params = {
            "page": safe_page_num,
            "col": "ab" if ORDER == "relevance" else "da",
            "sort": "desc",
            "ks": KEYWORDS,
            "d0": ",".join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY,
        }

        query_string = urllib.parse.urlencode(params)
        return f"{BASE_URL}?{query_string}"

def fetch_job_urls_from_1111_api(
    KEYWORDS: str,
    CATEGORY: Union[str, List[str]],
    ORDER: str = "date",
    PAGE_NUM: int = 1,
) -> Optional[Dict[str, Any]]:
    """
    從 1111 API 獲取職缺 URL 列表的原始數據。
    """
    api_url = catch_1111_url(KEYWORDS, CATEGORY, ORDER, PAGE_NUM, USE_API=True)
    return _make_api_request(
        "GET",
        api_url,
        headers=HEADERS_1111_JOB_API,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "1111_job_urls",
            "keywords": KEYWORDS,
            "category": CATEGORY,
            "page": PAGE_NUM,
        },
    )

def fetch_job_data_from_1111_web(job_url: str) -> Optional[Dict[str, Any]]:
    """
    從 1111 職缺頁面抓取單一 URL 的資料。
    """
    try:
        response = requests.get(job_url, verify=False, timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS)
        response.raise_for_status()
        return {"content": response.text} # Return content for BeautifulSoup parsing
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during 1111 job detail request.",
            url=job_url,
            error=e,
            exc_info=True,
        )
        raise # Re-raise to trigger tenacity retry if applied to this function
    except Exception as e:
        logger.error(
            "Unexpected error during 1111 job detail request.",
            url=job_url,
            error=e,
            exc_info=True,
        )
        return None



================================================
FILE: crawler/project_1111/config_1111.py
================================================
# crawler/project_1111/config_1111.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# 1111 平台相關設定
WEB_NAME_1111 = config_section.get("WEB_NAME_1111", "1111_人力銀行")
JOB_CAT_URL_1111 = config_section.get("JOB_CAT_URL_1111", "https://www.1111.com.tw/api/v1/codeCategories/")
JOB_API_BASE_URL_1111 = config_section.get("JOB_API_BASE_URL_1111", "https://www.1111.com.tw/api/v1/search/jobs/")
JOB_DETAIL_BASE_URL_1111 = config_section.get("JOB_DETAIL_BASE_URL_1111", "https://www.1111.com.tw/job/")

HEADERS_1111 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.1111.com.tw",
}

HEADERS_1111_JOB_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.1111.com.tw/search/job",
}

URL_CRAWLER_PAGE_SIZE_1111 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_1111", "20"))
URL_CRAWLER_ORDER_BY_1111 = config_section.get("URL_CRAWLER_ORDER_BY_1111", "date") # "date" or "relevance"



================================================
FILE: crawler/project_1111/page_api_data_1111.txt
================================================
{
  "result": {
    "pagination": {
      "page": 2,
      "limit": 30,
      "totalCount": 41528,
      "totalPage": 1385
    },
    "hits": [
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 98706131,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-台中福雅店】門市正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~32,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100907,
          "name": "台中市西屯區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-台中福雅店】門市正職人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/30 08:58:00",
        "jobId": 85120237,
        "companyId": 69588457,
        "companyName": "全家便利商店(全國門市聯合招募)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔",
        "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 45,000元~50,000元",
        "industry": {
          "id": 250215,
          "name": "量販流通相關"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1010"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
          "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 130198445,
        "companyId": 51471366,
        "companyName": "花間集養生事業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~",
        "title": "銷售員假日班(需輪蘆洲及重新家樂福)",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 205元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100220,
          "name": "新北市三重區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "銷售員假日班(需輪蘆洲及重新家樂福)",
          "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~"
        }
      },
      {
        "updateAt": "2025/07/29 07:23:10",
        "jobId": 113015398,
        "companyId": 117091183,
        "companyName": "金好運投注站",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.台灣彩券相關產品銷售，如大樂透,威力彩、今彩539以及刮刮樂之相關產品。\n2.無需負擔業績\n3.薪資,福利優渥,包刮交通與餐費經貼\n4.具三節獎金\n5.旅遊經貼補助",
        "title": "門市人員(海湖店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 35,000元~40,000元",
        "industry": {
          "id": 210101,
          "name": "運動服務"
        },
        "workCity": {
          "id": 100513,
          "name": "桃園市蘆竹區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1003",
          "1004",
          "1005",
          "1010",
          "1017",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員(海湖店)",
          "description": "1.台灣彩券相關產品銷售，如大樂透,威力彩、今彩539以及刮刮樂之相關產品。\n2.無需負擔業績\n3.薪資,福利優渥,包刮交通與餐費經貼\n4.具三節獎金\n5.旅遊經貼補助"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998332,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "陳列人員(縣聯店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102101,
          "name": "花蓮縣花蓮市"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "陳列人員(縣聯店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998325,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "商品陳列人員(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102104,
          "name": "花蓮縣吉安鄉"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "商品陳列人員(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 113108005,
        "companyId": 69429432,
        "companyName": "寶御食品有限公司(宝泉)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務（如：電話諮詢、調貨、修改、包裝及退換貨處理）。\n3.負責商品進貨入庫、銷售管理及庫存管理。\n4.負責商品包裝、陳列及促銷品換檔工作。\n5.維持店櫃週遭之整潔。\n6.具高度服務熱忱，有學習意願。\n7.騎機車可到，有停車場。",
        "title": "門市人員(湖口服務區)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 29,000元以上",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100702,
          "name": "新竹縣湖口鄉"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市人員(湖口服務區)",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務（如：電話諮詢、調貨、修改、包裝及退換貨處理）。\n3.負責商品進貨入庫、銷售管理及庫存管理。\n4.負責商品包裝、陳列及促銷品換檔工作。\n5.維持店櫃週遭之整潔。\n6.具高度服務熱忱，有學習意願。\n7.騎機車可到，有停車場。"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 103843529,
        "companyId": 51471366,
        "companyName": "花間集養生事業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n應徵方式加入LINE ID:flower22683268",
        "title": "銷售員假日班(林口家樂福)",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 205元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100223,
          "name": "新北市林口區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "銷售員假日班(林口家樂福)",
          "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n應徵方式加入LINE ID:flower22683268"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 78600194,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作 \n4.進貨庫存、補貨上架、陳列擺設 \n5.執行庫存盤點、電腦行政作業等工作 \n6.維持店鋪環境整潔 \n7.可配合店鋪輪調尤佳\n\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修] \n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-內湖瑞光店】門市正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-內湖瑞光店】門市正職人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作 \n4.進貨庫存、補貨上架、陳列擺設 \n5.執行庫存盤點、電腦行政作業等工作 \n6.維持店鋪環境整潔 \n7.可配合店鋪輪調尤佳\n\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修] \n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/29 10:28:21",
        "jobId": 113134032,
        "companyId": 71959281,
        "companyName": "京旺煙酒有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 收銀、銷貨單、網購商品取貨作業\n2. 庫存作業管理。\n3. 商品展示及陳列。\n4. 消費者需求動態掌握。\n5. 門市常態工作執行。\n6. 配合總公司規則及決策。\n7. 配合總公司促銷活動執行。\n8. 上級交辦事項。\n9. 具升遷或升等機會。",
        "title": "門市人員-東明、大明(儲備幹部)-連鎖煙酒專賣 (無經驗可/獎金另計)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 5,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 250202,
          "name": "食品什貨零售"
        },
        "workCity": {
          "id": 100910,
          "name": "台中市大里區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1005",
          "1010",
          "1017"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市人員-東明、大明(儲備幹部)-連鎖煙酒專賣 (無經驗可/獎金另計)",
          "description": "1. 收銀、銷貨單、網購商品取貨作業\n2. 庫存作業管理。\n3. 商品展示及陳列。\n4. 消費者需求動態掌握。\n5. 門市常態工作執行。\n6. 配合總公司規則及決策。\n7. 配合總公司促銷活動執行。\n8. 上級交辦事項。\n9. 具升遷或升等機會。"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998329,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "陳列人員(全民店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102101,
          "name": "花蓮縣花蓮市"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "陳列人員(全民店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/28 00:10:00",
        "jobId": 132040158,
        "companyId": 117198099,
        "companyName": "四寶貝美食團購有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "160100",
            "100100",
            "100200"
          ]
        },
        "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月",
        "title": "門市人員 (土城店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 250116,
          "name": "綜合商品批發代理"
        },
        "workCity": {
          "id": 100216,
          "name": "新北市土城區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1010",
          "1017",
          "1023",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員 (土城店)",
          "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月"
        }
      },
      {
        "updateAt": "2025/07/28 00:10:00",
        "jobId": 79956370,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-板橋大遠百店】正職門市人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100203,
          "name": "新北市板橋區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1006"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-板橋大遠百店】正職門市人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/28 14:12:33",
        "jobId": 98744798,
        "companyId": 2915040,
        "companyName": "旺來昌實業股份有限公司",
        "require": {
          "drivingLicense": [
            "69"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品管理-陳列補貨/商品效期檢視/貨架清潔/維持賣場環境整潔 \n\n2.互相協助、代理同事工作，完成主管交辦事項\n\n3.倉庫區商品整理定位，維持倉庫整齊與清潔 \n\n4.接受顧客詢問或主動提供諮商建議給顧客\n\n5.向顧客說明商品性質、特徵、品質與價格\n\n6.可配合公司輪班(排班制)\n\n7.須能獨立搬貨物25-30公斤\n\n日班:08:30~17:30 休息一小時\n晚班:12:45-21:15 休息半小時",
        "title": "正職營業人員（鳳山店）",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 29,000元以上",
        "industry": {
          "id": 250102,
          "name": "食品什貨批發"
        },
        "workCity": {
          "id": 101824,
          "name": "高雄市鳳山區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1003",
          "1005"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "正職營業人員（鳳山店）",
          "description": "1.商品管理-陳列補貨/商品效期檢視/貨架清潔/維持賣場環境整潔 \n\n2.互相協助、代理同事工作，完成主管交辦事項\n\n3.倉庫區商品整理定位，維持倉庫整齊與清潔 \n\n4.接受顧客詢問或主動提供諮商建議給顧客\n\n5.向顧客說明商品性質、特徵、品質與價格\n\n6.可配合公司輪班(排班制)\n\n7.須能獨立搬貨物25-30公斤\n\n日班:08:30~17:30 休息一小時\n晚班:12:45-21:15 休息半小時"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 130293013,
        "companyId": 595118,
        "companyName": "英達資訊有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "2",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "GARMIN 品牌銷售　GPS衛星導航／行車紀錄器／運動休閒手錶／配件／週邊商品\n＊產品介紹 \n＊顧客服務 \n＊庫存管理\n＊整潔維護",
        "title": "Garmin 科技生活體驗館－(八德)門市人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 36,000元~60,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 100101,
          "name": "台北市中正區"
        },
        "recruitCount": 1,
        "mrtId": 110514,
        "mrtTime": 303,
        "mrtNear": 360,
        "benefits": [
          "1005",
          "1010",
          "1014",
          "1017",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "Garmin 科技生活體驗館－(八德)門市人員",
          "description": "GARMIN 品牌銷售　GPS衛星導航／行車紀錄器／運動休閒手錶／配件／週邊商品\n＊產品介紹 \n＊顧客服務 \n＊庫存管理\n＊整潔維護"
        }
      },
      {
        "updateAt": "2025/07/27 00:00:00",
        "jobId": 132086538,
        "companyId": 72429252,
        "companyName": "卷卷有限公司",
        "require": {
          "drivingLicense": [
            "1"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表",
        "title": "台中清水門市銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100926,
          "name": "台中市清水區"
        },
        "recruitCount": 11,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "台中清水門市銷售人員",
          "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 132040162,
        "companyId": 117198099,
        "companyName": "四寶貝美食團購有限公司",
        "require": {
          "drivingLicense": [
            "64"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "160100",
            "100100",
            "100200"
          ]
        },
        "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月",
        "title": "門市人員 (江翠店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 250116,
          "name": "綜合商品批發代理"
        },
        "workCity": {
          "id": 100203,
          "name": "新北市板橋區"
        },
        "recruitCount": 1,
        "mrtId": 110509,
        "mrtTime": 0,
        "mrtNear": 433,
        "benefits": [
          "1001",
          "1005",
          "1010",
          "1017",
          "1023",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員 (江翠店)",
          "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月"
        }
      },
      {
        "updateAt": "2025/07/27 00:00:00",
        "jobId": 132086539,
        "companyId": 72429252,
        "companyName": "卷卷有限公司",
        "require": {
          "drivingLicense": [
            "1"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表",
        "title": "台中北屯門市銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100906,
          "name": "台中市北屯區"
        },
        "recruitCount": 10,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "台中北屯門市銷售人員",
          "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 103770365,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "喜歡甜點、享受與人互動的你，歡迎加入糖村團隊！\n我們正在尋找對銷售服務有熱情、願意學習、細心負責的你，擔任現場銷售夥伴！\n\n~工作內容~\n1.接待顧客，介紹糖村經典商品，提供親切服務。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.協助商品進貨、入庫與銷售及庫存管理。\n4.商品包裝、陳列與促銷品更換。\n5.執行庫存盤點、電腦行政作業等工作。\n6.協助維持店鋪整潔、營造良好購物環境。\n7.節慶期間支援專案活動（如：電話開發、報價、送樣等）。\n\n\n• 具備服務熱誠與主動積極的態度\n• 樂於學習，能配合假日與彈性排班\n• 喜歡與人互動、有銷售或門市經驗更佳\n• 能細心完成包裝、陳列等工作內容\n\n※完整教育訓練計畫與穩定升遷管道\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-台北瑞光店】門市兼職人員",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 190元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-台北瑞光店】門市兼職人員",
          "description": "喜歡甜點、享受與人互動的你，歡迎加入糖村團隊！\n我們正在尋找對銷售服務有熱情、願意學習、細心負責的你，擔任現場銷售夥伴！\n\n~工作內容~\n1.接待顧客，介紹糖村經典商品，提供親切服務。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.協助商品進貨、入庫與銷售及庫存管理。\n4.商品包裝、陳列與促銷品更換。\n5.執行庫存盤點、電腦行政作業等工作。\n6.協助維持店鋪整潔、營造良好購物環境。\n7.節慶期間支援專案活動（如：電話開發、報價、送樣等）。\n\n\n• 具備服務熱誠與主動積極的態度\n• 樂於學習，能配合假日與彈性排班\n• 喜歡與人互動、有銷售或門市經驗更佳\n• 能細心完成包裝、陳列等工作內容\n\n※完整教育訓練計畫與穩定升遷管道\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 132009129,
        "companyId": 69601638,
        "companyName": "金車生物科技股份有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。",
        "title": "金車生物科技★礁溪門市人員★日班8:00-17:00★礁溪場",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 190101,
          "name": "農藝╱園藝相關"
        },
        "workCity": {
          "id": 100403,
          "name": "宜蘭縣礁溪鄉"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "金車生物科技★礁溪門市人員★日班8:00-17:00★礁溪場",
          "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。"
        }
      },
      {
        "updateAt": "2025/07/14 13:40:47",
        "jobId": 130157790,
        "companyId": 50921958,
        "companyName": "億進寢具企業有限公司(總公司)",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "2024年2月11日開始『億進寢具』在不景氣環境下及考慮員工工作與生活平衡，全面調整『縮短營業時間』及『提升薪資待遇』!!!\n\n✨全台直營連鎖店正持續擴展中~目前已有44家直營門市!!!積極招募熱情、願意學習、願意自我挑戰、具有企圖心，想為自己人生創造高薪收入，對銷售有興趣的夥伴們，加入我們『億進寢具』成為幸福億家人。✨\n\n✨本公司重視員工教育訓練與客戶服務，提供完善的教育訓練制度及專業的學習培訓環境，經勞動部發展署TTQS人才發展品質管理系統(企業機構版)評核，榮獲銅牌肯定!✨\n\n✨我們非常重視員工健康生活，提供多元運動項目、減重比賽等活動，連續2年榮獲教育部體育署辦理職工運動活動及聘用運動人才之表揚企業!✨\n\n【工作內容】\n1.商品陳列及換檔等工作。\n2.商品進出貨、銷售管理、庫存管理。\n3.介紹及銷售公司之商品與服務。\n4.門市環境清潔與維護。\n5.配合公司人力調度與配置(工作排班)、人員工作指導與教育訓練。\n6.從事門市營運必要之工作其他公司交辦之事項。\n\n【履歷投遞說明】\n1.履歷請填寫完整。\n2.初步須審查履歷，若未合格者恕無另行通知。",
        "title": "門市銷售專員／台南安南店",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 35,033元~60,000元",
        "industry": {
          "id": 110305,
          "name": "其他紡織製品製造"
        },
        "workCity": {
          "id": 101606,
          "name": "台南市安南區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1005",
          "1006",
          "1010",
          "1014"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市銷售專員／台南安南店",
          "description": "2024年2月11日開始『億進寢具』在不景氣環境下及考慮員工工作與生活平衡，全面調整『縮短營業時間』及『提升薪資待遇』!!!\n\n✨全台直營連鎖店正持續擴展中~目前已有44家直營門市!!!積極招募熱情、願意學習、願意自我挑戰、具有企圖心，想為自己人生創造高薪收入，對銷售有興趣的夥伴們，加入我們『億進寢具』成為幸福億家人。✨\n\n✨本公司重視員工教育訓練與客戶服務，提供完善的教育訓練制度及專業的學習培訓環境，經勞動部發展署TTQS人才發展品質管理系統(企業機構版)評核，榮獲銅牌肯定!✨\n\n✨我們非常重視員工健康生活，提供多元運動項目、減重比賽等活動，連續2年榮獲教育部體育署辦理職工運動活動及聘用運動人才之表揚企業!✨\n\n【工作內容】\n1.商品陳列及換檔等工作。\n2.商品進出貨、銷售管理、庫存管理。\n3.介紹及銷售公司之商品與服務。\n4.門市環境清潔與維護。\n5.配合公司人力調度與配置(工作排班)、人員工作指導與教育訓練。\n6.從事門市營運必要之工作其他公司交辦之事項。\n\n【履歷投遞說明】\n1.履歷請填寫完整。\n2.初步須審查履歷，若未合格者恕無另行通知。"
        }
      },
      {
        "updateAt": "2025/07/31 00:00:00",
        "jobId": 92234280,
        "companyId": 73463474,
        "companyName": "崑山玩具精品",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "我們是一家專注於玩具銷售的專業門市。我們的主要服務對象涵蓋各年齡層的顧客，並提供各類具創意與教育意義的玩具。致力打造一個讓顧客感受到快樂與興趣的購物環境。\n我們誠摯邀請熱愛玩具、擁有服務熱忱的伙伴加入我們，一起為顧客帶來美好的購物體驗。\n\n\n工作內容：\n1. 商品的陳列與擺放，確保商品依照陳列標準分類，可吸引顧客目光。\n2. 維護營業場地的清潔與舒適，包括定時巡檢與環境整理。\n3. 提供專業的產品與價格諮詢，向顧客解釋商品特性及用途。\n4. 操作櫃檯收銀機進行收銀作業，包括結帳、發票及現金管理。\n5. 主動與顧客互動，熱情解答問題，增強顧客購物的滿意度。\n6. 快速學習玩具產品知識，能準確地介紹新品及促銷資訊。\n7. 協助庫存管理，包括商品上架、補貨與庫存數據維護。\n8. 支援促銷活動的佈置與執行，包含活動區域設置、商品介紹。\n\n【注意事項】\n有時間觀念，不遲到\n注重禮儀\n\n【親洽面試】\n預約好時間，到店洽詢店長\n敬請重視求職禮儀!並請記得攜帶履歷!\n\n\n【薪資面議】\n依照工作能力經驗而定及入職後調整\n\n\n我們正在尋找注重細節、善於溝通且充滿熱誠的您加入我們，與我們一起成為業界中的佼佼者，共同為顧客提供愉快的購物體驗！立即投遞履歷，我們期待您的加入！",
        "title": "★左營店★ 門市人員/有經驗者佳/親洽優先",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元以上",
        "industry": {
          "id": 111602,
          "name": "玩具相關"
        },
        "workCity": {
          "id": 101811,
          "name": "高雄市左營區"
        },
        "recruitCount": 3,
        "mrtId": 280114,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "★左營店★ 門市人員/有經驗者佳/親洽優先",
          "description": "我們是一家專注於玩具銷售的專業門市。我們的主要服務對象涵蓋各年齡層的顧客，並提供各類具創意與教育意義的玩具。致力打造一個讓顧客感受到快樂與興趣的購物環境。\n我們誠摯邀請熱愛玩具、擁有服務熱忱的伙伴加入我們，一起為顧客帶來美好的購物體驗。\n\n\n工作內容：\n1. 商品的陳列與擺放，確保商品依照陳列標準分類，可吸引顧客目光。\n2. 維護營業場地的清潔與舒適，包括定時巡檢與環境整理。\n3. 提供專業的產品與價格諮詢，向顧客解釋商品特性及用途。\n4. 操作櫃檯收銀機進行收銀作業，包括結帳、發票及現金管理。\n5. 主動與顧客互動，熱情解答問題，增強顧客購物的滿意度。\n6. 快速學習玩具產品知識，能準確地介紹新品及促銷資訊。\n7. 協助庫存管理，包括商品上架、補貨與庫存數據維護。\n8. 支援促銷活動的佈置與執行，包含活動區域設置、商品介紹。\n\n【注意事項】\n有時間觀念，不遲到\n注重禮儀\n\n【親洽面試】\n預約好時間，到店洽詢店長\n敬請重視求職禮儀!並請記得攜帶履歷!\n\n\n【薪資面議】\n依照工作能力經驗而定及入職後調整\n\n\n我們正在尋找注重細節、善於溝通且充滿熱誠的您加入我們，與我們一起成為業界中的佼佼者，共同為顧客提供愉快的購物體驗！立即投遞履歷，我們期待您的加入！"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 130431473,
        "companyId": 490216,
        "companyName": "三井資訊股份有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "本公司擴大營業，廣邀人才，提供絕佳的『升遷』機會！提供保障薪資+優渥的銷售獎金+完善的員工褔利 + 完整的訓練發展 + 絕佳的『升遷』機會。『時勢造英雄』，歡迎您加入，一起共創未來。\n【工作內容】: \n1.羅技商品銷售與陳列管理。\n2.客戶服務與管理 。\n3.門市營運作業 。\n4.營運幹部培訓。\n5.偶爾須配合支援資訊展等活動。\n6.業績獎金另計。\n7.月休9~10天。\n8.本門市預計三月開幕，故報到後會先至鄰近門市受訓。",
        "title": "門市服務銷售專員【羅技南港lalaport旗艦館】",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 31,590元~55,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 100111,
          "name": "台北市南港區"
        },
        "recruitCount": 1,
        "mrtId": 110123,
        "mrtTime": 377,
        "mrtNear": 464,
        "benefits": [
          "1005",
          "1006",
          "1014",
          "1019",
          "1024"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市服務銷售專員【羅技南港lalaport旗艦館】",
          "description": "本公司擴大營業，廣邀人才，提供絕佳的『升遷』機會！提供保障薪資+優渥的銷售獎金+完善的員工褔利 + 完整的訓練發展 + 絕佳的『升遷』機會。『時勢造英雄』，歡迎您加入，一起共創未來。\n【工作內容】: \n1.羅技商品銷售與陳列管理。\n2.客戶服務與管理 。\n3.門市營運作業 。\n4.營運幹部培訓。\n5.偶爾須配合支援資訊展等活動。\n6.業績獎金另計。\n7.月休9~10天。\n8.本門市預計三月開幕，故報到後會先至鄰近門市受訓。"
        }
      },
      {
        "updateAt": "2025/07/31 01:00:33",
        "jobId": 78431188,
        "companyId": 70672475,
        "companyName": "嘉鑫科技有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "1",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 負責智慧型手機及相關通訊商品的銷售，清楚介紹功能與規格特性。  \n2. 提供門號資費方案規劃，依客戶需求設計最適合的通訊方案。  \n3. 接待顧客，協助解答電話諮詢，處理調貨與產品設定服務。  \n4. 負責商品進貨入庫檢查，定期執行銷售與庫存管理。  \n5. 負責通訊商品的包裝作業、商品陳列及促銷品的更新更換。  \n6. 維護門市內外環境整潔，提升顧客購物體驗。  \n7. 執行銷售數據分析，制定改善策略以達成門市業績目標。  \n8. 參與定期培訓與會議，深化專業知識並提升服務品質。",
        "title": "【 嘉新電腦通訊埔里門市】通訊門市人員/儲備幹部 (薪資30000~65000)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~65,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 101105,
          "name": "南投縣埔里鎮"
        },
        "recruitCount": 4,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1014",
          "1019",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【 嘉新電腦通訊埔里門市】通訊門市人員/儲備幹部 (薪資30000~65000)",
          "description": "1. 負責智慧型手機及相關通訊商品的銷售，清楚介紹功能與規格特性。  \n2. 提供門號資費方案規劃，依客戶需求設計最適合的通訊方案。  \n3. 接待顧客，協助解答電話諮詢，處理調貨與產品設定服務。  \n4. 負責商品進貨入庫檢查，定期執行銷售與庫存管理。  \n5. 負責通訊商品的包裝作業、商品陳列及促銷品的更新更換。  \n6. 維護門市內外環境整潔，提升顧客購物體驗。  \n7. 執行銷售數據分析，制定改善策略以達成門市業績目標。  \n8. 參與定期培訓與會議，深化專業知識並提升服務品質。"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 113056625,
        "companyId": 73207520,
        "companyName": "統一超商(玉泓企業社)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 負責連鎖系列直營店或分店內督導及協調所屬人員之日常工作及營運。\n2. 於專櫃、賣場或門市裡提供商品介紹及銷售服務。\n3. 本類人員處理客戶訂單、查詢及處理一般性客戶投訴事宜、門市經營等人員。",
        "title": "(代天府門市)正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~30,000元",
        "industry": {
          "id": 250218,
          "name": "其他零售業"
        },
        "workCity": {
          "id": 101624,
          "name": "台南市北門區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "(代天府門市)正職人員",
          "description": "1. 負責連鎖系列直營店或分店內督導及協調所屬人員之日常工作及營運。\n2. 於專櫃、賣場或門市裡提供商品介紹及銷售服務。\n3. 本類人員處理客戶訂單、查詢及處理一般性客戶投訴事宜、門市經營等人員。"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 98638344,
        "companyId": 73217699,
        "companyName": "百吉玩具有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "🎮誠徵夜市遊戲攤位儲備幹部🎮\n               （男女不拘)\n🔴地點：高雄市區（瑞豐夜市）\n\n🟡上班時間：\n星期二、四、五  17:00-01:00\n星期六、日          16:00-01:00\n每日8小時、加班費另計\n\n🔴工作內容：擺攤、招呼客人、收攤、等等...\n\n❮薪資❯\n•底薪  薪資:$30000~$40000\n\n❮獎金❯  (其他獎金另外加給‼️)\n•全勤獎金:$2000\n•職位津貼:依職位進行加給\n•團體業績獎金:依團體整體業績,每月進行分紅。\n•個人業績獎金:依個人負責攤位業績,每月進行分紅。\n•勞保、健保補貼\n\n\n🟡意者請洽👉林先生 0921063975 \n\n🔴意者請洽👉陳先生 0981442561\n\n盡量用Line聯繫、以免沒回覆到各位\n（電話聯絡或加LINE的、請用電話搜尋👆同上方電話）\n\n本公司正在擴大招募人才、對於挑戰高薪這邊是個好地方、只要您肯學習、只要您夠積極、只要您夠努力💪、還是您肯吃苦、又或著是您另有其他過人之處、歡迎你們來加入我們的團隊。\n\n公司有提供非常好的學習環境、訓練流程、及地表最強團隊、歡迎👏想轉換跑道看看的新鮮人！！！",
        "title": "【儲備幹部】遊戲攤位人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 210102,
          "name": "休閒服務"
        },
        "workCity": {
          "id": 101811,
          "name": "高雄市左營區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "【儲備幹部】遊戲攤位人員",
          "description": "🎮誠徵夜市遊戲攤位儲備幹部🎮\n               （男女不拘)\n🔴地點：高雄市區（瑞豐夜市）\n\n🟡上班時間：\n星期二、四、五  17:00-01:00\n星期六、日          16:00-01:00\n每日8小時、加班費另計\n\n🔴工作內容：擺攤、招呼客人、收攤、等等...\n\n❮薪資❯\n•底薪  薪資:$30000~$40000\n\n❮獎金❯  (其他獎金另外加給‼️)\n•全勤獎金:$2000\n•職位津貼:依職位進行加給\n•團體業績獎金:依團體整體業績,每月進行分紅。\n•個人業績獎金:依個人負責攤位業績,每月進行分紅。\n•勞保、健保補貼\n\n\n🟡意者請洽👉林先生 0921063975 \n\n🔴意者請洽👉陳先生 0981442561\n\n盡量用Line聯繫、以免沒回覆到各位\n（電話聯絡或加LINE的、請用電話搜尋👆同上方電話）\n\n本公司正在擴大招募人才、對於挑戰高薪這邊是個好地方、只要您肯學習、只要您夠積極、只要您夠努力💪、還是您肯吃苦、又或著是您另有其他過人之處、歡迎你們來加入我們的團隊。\n\n公司有提供非常好的學習環境、訓練流程、及地表最強團隊、歡迎👏想轉換跑道看看的新鮮人！！！"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 132027113,
        "companyId": 9780802,
        "companyName": "耶里食品有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.接待顧客、結帳收銀。\n2.補貨、簡易商品擺設整理。\n3.協助店內日常清潔與運作。\n\n【待遇符勞基法規定】\n勞、健保  等\n\n【員工福利】\n 免費 晚餐天天有，每月可省好幾千。",
        "title": "[供晚餐]晚班門市 (仁愛本店) 請電洽2705-2578",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100105,
          "name": "台北市大安區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "[供晚餐]晚班門市 (仁愛本店) 請電洽2705-2578",
          "description": "1.接待顧客、結帳收銀。\n2.補貨、簡易商品擺設整理。\n3.協助店內日常清潔與運作。\n\n【待遇符勞基法規定】\n勞、健保  等\n\n【員工福利】\n 免費 晚餐天天有，每月可省好幾千。"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 78421167,
        "companyId": 69601638,
        "companyName": "金車生物科技股份有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。",
        "title": "金車生物科技★水產門市人員★日班8:00-17:00★礁溪場",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 190101,
          "name": "農藝╱園藝相關"
        },
        "workCity": {
          "id": 100403,
          "name": "宜蘭縣礁溪鄉"
        },
        "recruitCount": 4,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "金車生物科技★水產門市人員★日班8:00-17:00★礁溪場",
          "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。"
        }
      },
      {
        "updateAt": "2025/07/30 00:00:00",
        "jobId": 132087785,
        "companyId": 73541456,
        "companyName": "鑫富航企業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.介紹品牌及銷售商品\n2.顧客接待及提供需求服務(如：調貨、包裝及退換貨處理、諮詢等)\n3.商品收銀、進貨、庫存管理\n4.商品包裝、陳列及促銷品換檔\n5.維持店櫃周遭整潔",
        "title": "百貨專櫃人員 (天母大葉高島屋)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~80,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100108,
          "name": "台北市士林區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1010"
        ],
        "companyTags": [],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "百貨專櫃人員 (天母大葉高島屋)",
          "description": "1.介紹品牌及銷售商品\n2.顧客接待及提供需求服務(如：調貨、包裝及退換貨處理、諮詢等)\n3.商品收銀、進貨、庫存管理\n4.商品包裝、陳列及促銷品換檔\n5.維持店櫃周遭整潔"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 78471984,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.門市服務: 對台灣顧客貼心、外國旅客熱情! \n2.介紹銷售: 常客、熟客、外國客，哪個是他們最喜歡的?\n用專業知識與經驗match客人需要的! \n我們不是銷售員，我們是客人專業的消費顧問!\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-信義A8】專櫃正職人員",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100107,
          "name": "台北市信義區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1006"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-信義A8】專櫃正職人員",
          "description": "1.門市服務: 對台灣顧客貼心、外國旅客熱情! \n2.介紹銷售: 常客、熟客、外國客，哪個是他們最喜歡的?\n用專業知識與經驗match客人需要的! \n我們不是銷售員，我們是客人專業的消費顧問!\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      }
    ],
    "fromOffset": 0
  },
  "recommendJobs": {
    "hits": [
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 75808167,
        "companyId": 68754471,
        "companyName": "高北牛乳大王",
        "require": {
          "drivingLicense": [
            "2"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 接受顧客詢問或主動提供諮商建議給顧客\n2. 陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n3. 向顧客說明貨品的性質、特徵、品質與價格\n4. 在成交後，包裝商品、收取款項、交付商品，完成交易手續\n5. 在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n6. POS系統收銀機操作、結帳\n7. 每日填寫工作報表。\n8. 需不定期回總公司接受教育訓練課程。\n9. 視公司需求支援面銷活動。",
        "title": "行動餐車部晚班客服正職 高北牛乳大王",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~36,000元",
        "industry": {
          "id": 220202,
          "name": "飲料店"
        },
        "workCity": {
          "id": 101208,
          "name": "彰化縣和美鎮"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1003",
          "1004",
          "1006",
          "1019",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "行動餐車部晚班客服正職 高北牛乳大王",
          "description": "1. 接受顧客詢問或主動提供諮商建議給顧客\n2. 陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n3. 向顧客說明貨品的性質、特徵、品質與價格\n4. 在成交後，包裝商品、收取款項、交付商品，完成交易手續\n5. 在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n6. POS系統收銀機操作、結帳\n7. 每日填寫工作報表。\n8. 需不定期回總公司接受教育訓練課程。\n9. 視公司需求支援面銷活動。"
        }
      },
      {
        "updateAt": "2025/07/29 09:55:13",
        "jobId": 132050507,
        "companyId": 73519128,
        "companyName": "沛思彩虹行銷有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16,
            32
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.3C配件、微家電、耳機，各式手機貼膜銷售,無經驗可教導培訓\n2.店面環境整潔\n3.協助店長交辦事宜。\n4.銷售獎金另計\n\n*表現良好或對此產業有高度興趣者可培訓為儲備店長*\n*我們有多家分店,需可配合支援北部分店*",
        "title": "＜桃園新光影城＞手機貼膜/3C產品門市人員(無經驗可培訓)",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 34,000元以上",
        "industry": {
          "id": 250114,
          "name": "電子通訊╱電腦週邊批發"
        },
        "workCity": {
          "id": 100501,
          "name": "桃園市中壢區"
        },
        "recruitCount": 5,
        "mrtId": 150118,
        "mrtTime": 0,
        "mrtNear": 331,
        "benefits": [
          "0"
        ],
        "companyTags": [],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "＜桃園新光影城＞手機貼膜/3C產品門市人員(無經驗可培訓)",
          "description": "1.3C配件、微家電、耳機，各式手機貼膜銷售,無經驗可教導培訓\n2.店面環境整潔\n3.協助店長交辦事宜。\n4.銷售獎金另計\n\n*表現良好或對此產業有高度興趣者可培訓為儲備店長*\n*我們有多家分店,需可配合支援北部分店*"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 92137968,
        "companyId": 73075810,
        "companyName": "冠宏通信行",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "【門市服務、門市銷售及電信業務推廣】 \n\n1. 電信門市、遠傳電信門號申辦、通訊商品(配件)、門市銷售服務 \n2. 提供顧客之接待與需求服務 \n3. 月薪35000元 . 另有個人及團隊獎金 . 平均月收入5萬 . 不倒扣底薪  \n4. 月排休9天 . 團隊內自己討論排休 . 可排連休 . 可排假日 \n5. 明確晉升制度依照個人能力及成績取得晉升 \n6. 無經驗可 \n*有一年以上電信系統店或通訊行門市銷售經驗者薪資另議公司來進行面談！〝",
        "title": "遠傳電信 北港大同 銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 35,000元~50,000元",
        "industry": {
          "id": 100201,
          "name": "電信相關"
        },
        "workCity": {
          "id": 101316,
          "name": "雲林縣北港鎮"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "遠傳電信 北港大同 銷售人員",
          "description": "【門市服務、門市銷售及電信業務推廣】 \n\n1. 電信門市、遠傳電信門號申辦、通訊商品(配件)、門市銷售服務 \n2. 提供顧客之接待與需求服務 \n3. 月薪35000元 . 另有個人及團隊獎金 . 平均月收入5萬 . 不倒扣底薪  \n4. 月排休9天 . 團隊內自己討論排休 . 可排連休 . 可排假日 \n5. 明確晉升制度依照個人能力及成績取得晉升 \n6. 無經驗可 \n*有一年以上電信系統店或通訊行門市銷售經驗者薪資另議公司來進行面談！〝"
        }
      }
    ]
  },
  "seo": {
    "canonicalLink": "https://www.1111.com.tw/search/job?page=2&d0=120400",
    "nextLink": "https://www.1111.com.tw/search/job?page=3&d0=120400",
    "title": "「門市銷售」職缺 - 2025年7月熱門工作機會｜1111人力銀行",
    "keyword": "門市銷售,找工作,工作,求職,搜尋工作,查詢工作,職缺,工作機會",
    "description": "(2025/7/31)－(41528) 個工作職缺機會｜(門市人員【金好運投注站】、門市人員【寶御食品有限公司(宝泉)】、銷售員假日班【花間集養生事業有限公司】、門市人員【四寶貝美食團購有限公司】、正職營業人員（鳳山店）【旺來昌實業股份有限公司】)....等。1111提供全台最齊全的工作職缺及求職網路平台服務，更多找工作職缺請上1111。",
    "imageUrl": "https://www.1111.com.tw/talents/MicroBlogging/images/facebook_banner01.jpg",
    "ldJson": {
      "@context": "https://schema.org/",
      "@type": "CollectionPage",
      "name": "1111人力銀行 - 職位搜尋結果",
      "description": "搜尋結果頁面，顯示與關鍵字相關的職位列表。",
      "url": "https://www.1111.com.tw/search/job",
      "mainEntity": {
        "@type": "ItemList",
        "itemListElement": [
          {
            "item": {
              "title": "【糖村-台中福雅店】門市正職人員",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "糖村"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "台中市"
              },
              "datePosted": "2022/3/21",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/98706131",
              "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
            },
            "@type": "ListItem",
            "position": 1
          },
          {
            "item": {
              "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "全家便利商店"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "台北市"
              },
              "datePosted": "2018/9/26",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/85120237",
              "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔"
            },
            "@type": "ListItem",
            "position": 2
          },
          {
            "item": {
              "title": "銷售員假日班",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "花間集養生事業有限公司"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "新北市"
              },
              "datePosted": "2025/5/21",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/130198445",
              "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~"
            },
            "@type": "ListItem",
            "position": 3
          }
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://www.1111.com.tw/search/job?ks={search_term_string}",
        "query-input": "required name=search_term_string"
      },
      "@keywords": [
        "門市銷售",
        "找工作",
        "工作",
        "求職",
        "搜尋工作",
        "查詢工作",
        "職缺",
        "工作機會"
      ]
    }
  },
  "query": null
}


================================================
FILE: crawler/project_1111/parser_apidata_1111.py
================================================
import re
from datetime import datetime
from typing import Optional
import structlog
from bs4 import BeautifulSoup

from crawler.database.schemas import (
    JobPydantic,
    SourcePlatform,
    JobStatus,
    JobType,
    SalaryType,
)
from crawler.project_1111.config_1111 import JOB_DETAIL_BASE_URL_1111
from crawler.utils.salary_parser import parse_salary_text # Import the new parser

logger = structlog.get_logger(__name__)

# 1111 API 的 jobType 到我們內部 JobType Enum 的映射
JOB_TYPE_MAPPING_1111 = {
    "全職": JobType.FULL_TIME,
    "兼職": JobType.PART_TIME,
    "實習": JobType.INTERNSHIP,
    "派遣": JobType.CONTRACT,
    "約聘": JobType.TEMPORARY,
    "其他": JobType.OTHER,
}

# 1111 API 的 jobType (整數) 到字串的映射
JOB_TYPE_INT_TO_STR_MAPPING_1111 = {
    1: "全職",
    2: "兼職",
    3: "實習",
    4: "派遣",
    5: "約聘",
}

# 1111 API 的教育程度 (grades) 映射
EDUCATION_MAPPING_1111 = {
    2: "高中",
    8: "專科",
    16: "大學",
    32: "碩士",
    64: "博士",
}

# Removed the old parse_salary function

def derive_salary_type(salary_text: str, salary_min: Optional[int], job_type: Optional[JobType]) -> Optional[SalaryType]:
    logger.debug("Deriving salary type", salary_text=salary_text, salary_min=salary_min, job_type=job_type)
    text = salary_text.replace(",", "").replace(" ", "").lower()

    # Priority 1: Explicit keywords
    if "月薪" in text:
        logger.debug("Derived: MONTHLY (keyword)")
        return SalaryType.MONTHLY
    elif "時薪" in text:
        logger.debug("Derived: HOURLY (keyword)")
        return SalaryType.HOURLY
    elif "年薪" in text:
        logger.debug("Derived: YEARLY (keyword)")
        return SalaryType.YEARLY
    elif "日薪" in text:
        logger.debug("Derived: DAILY (keyword)")
        return SalaryType.DAILY
    elif "論件計酬" in text:
        logger.debug("Derived: BY_CASE (keyword)")
        return SalaryType.BY_CASE
    
    # Priority 2: "面議" combined with "萬" (implying monthly)
    # This handles "面議（經常性薪資達4萬元或以上）" -> 月薪
    if "面議" in text and "萬" in text:
        logger.debug("Derived: MONTHLY (negotiable + wan)")
        return SalaryType.MONTHLY # Assuming "萬" in "面議" context implies monthly

    # Priority 3: If job_type is FULL_TIME, assume MONTHLY
    if job_type == JobType.FULL_TIME:
        logger.debug("Derived: MONTHLY (full-time job type)")
        return SalaryType.MONTHLY

    # Priority 4: General "面議"
    if "面議" in text:
        logger.debug("Derived: NEGOTIABLE (general negotiable)")
        return SalaryType.NEGOTIABLE
    
    # Priority 5: Numerical inference based on salary_min (if no keyword found)
    if salary_min is not None:
        if salary_min < 2000: # Adjusted threshold for hourly
            logger.debug("Derived: HOURLY (numerical)")
            return SalaryType.HOURLY
        elif salary_min > 200000:
            logger.debug("Derived: YEARLY (numerical)")
            return SalaryType.YEARLY
        else:
            logger.debug("Derived: MONTHLY (numerical)")
            return SalaryType.MONTHLY # Default to monthly for typical ranges

    logger.debug("Derived: None (no match)")
    return None

def parse_job_list_json_to_pydantic(job_item: dict) -> Optional[JobPydantic]:
    """
    從 1111 列表頁 API 的 JSON 數據解析並轉換為 JobPydantic 物件。
    """
    try:
        job_id = str(job_item.get("jobId"))
        company_source_id = str(job_item.get("companyId"))

        url = f"{JOB_DETAIL_BASE_URL_1111}{job_id}"
        company_url = f"https://www.1111.com.tw/corp/{company_source_id}"

        title = job_item.get("title")
        company_name = job_item.get("companyName")
        description = job_item.get("description")
        location_text = job_item.get("workCity", {}).get("name", "").strip() or None

        posted_at = None
        update_at_str = job_item.get("updateAt")
        if update_at_str:
            try:
                posted_at = datetime.strptime(update_at_str, "%Y/%m/%d %H:%M:%S")
            except ValueError:
                logger.warning(
                    "Could not parse posted_at date format from list API.",
                    update_at=update_at_str,
                    job_id=job_id,
                )

        salary_text = job_item.get("salary", "")
        # Derive job_type first, as it's needed for derive_salary_type
        job_type_int = job_item.get("jobType")
        job_type_str = JOB_TYPE_INT_TO_STR_MAPPING_1111.get(job_type_int)
        job_type = JOB_TYPE_MAPPING_1111.get(job_type_str) if job_type_str else None

        # Use the new parse_salary_text from salary_parser.py
        salary_min, salary_max = parse_salary_text(salary_text)
        # Derive salary_type using the new function, passing job_type
        salary_type = derive_salary_type(salary_text, salary_min, job_type)

        experience_required_text = job_item.get("require", {}).get("experience")
        if experience_required_text == "0":
            experience_required_text = "不拘"
        elif experience_required_text is None:
            experience_required_text = "不拘"

        education_required_text = "不拘"
        education_grades = job_item.get("require", {}).get("grades")
        if education_grades and isinstance(education_grades, list):
            valid_grades = sorted(
                [g for g in education_grades if g in EDUCATION_MAPPING_1111]
            )
            if valid_grades:
                min_edu_code = min(valid_grades)
                education_required_text = EDUCATION_MAPPING_1111.get(
                    min_edu_code, "不拘"
                )
                if any(g > min_edu_code for g in valid_grades):
                    education_required_text += "以上"

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_1111,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=str(experience_required_text),
            education_required_text=education_required_text,
            company_source_id=company_source_id,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing 1111 job list JSON.",
            error=e,
            job_item=job_item,
            exc_info=True,
        )
        return None


def parse_job_detail_html_to_pydantic(
    html_content: str, url: str
) -> Optional[JobPydantic]:
    """
    從 1111 職缺頁面的 HTML 內容解析並轉換為 JobPydantic 物件。
    """
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        job_id = url.split("/")[-1].split("?")[0]

        # --- 1. 頁首區塊 (Top Section) ---
        header_section = soup.select_one(
            "section[data-v-e57f1019] > div.container > div.text-gray-600"
        )

        title = None
        company_name = None
        job_type_str = None
        salary_text = None
        education_required_text = None
        location_text = None
        posted_at = None
        experience_required_text = None
        description = None
        company_url = None
        company_source_id = None

        if header_section:
            title = header_section.select_one("h1").get_text(strip=True) or None
            company_name = (
                header_section.select_one("h2.inline").get_text(strip=True) or None
            )
            # Extract company_url from company_name's parent a tag
            company_link_tag = header_section.select_one("h2.inline a")
            if company_link_tag and "href" in company_link_tag.attrs:
                company_url = company_link_tag["href"]
                # Extract company_source_id from company_url if possible
                match = re.search(r"/corp/(\d+)", company_url)
                if match:
                    company_source_id = match.group(1)

            pills = header_section.select("div.flex.flex-wrap.mt-4.gap-3 > div")
            top_info = [p.get_text(strip=True, separator=" ") for p in pills]
            if len(top_info) >= 4:
                job_type_str = top_info[0]  # e.g., "全職"
                salary_text = top_info[1]
                education_required_text = top_info[2]
                location_text = top_info[3]

            info_items = header_section.select("ul.info-item > li")
            for item in info_items:
                key_tag = item.select_one("h3")
                val_tag = item.select_one("span") or item.select_one("time")
                if key_tag and val_tag:
                    key = key_tag.get_text(strip=True)
                    value = val_tag.get_text(strip=True)
                    if "更新日期" in key:
                        try:
                            posted_at = datetime.strptime(
                                value.replace(" ", ""), "%Y/%m/%d"
                            )
                        except ValueError:
                            logger.warning(
                                "Could not parse posted_at date format.",
                                value=value,
                                job_id=job_id,
                            )
                    elif "工作經驗" in key:
                        experience_required_text = value

        # --- 2. 主要內容區塊 (Main Content Sections) ---
        sections = soup.select("section[id]")
        for section in sections:
            section_title_tag = section.select_one("h2.text-lg.text-main")
            if not section_title_tag:
                continue

            section_title = section_title_tag.get_text(strip=True)

            if section_title == "工作內容":
                # Find the job description within the '工作內容' section
                job_description_h3 = section.find(
                    "h3", string=lambda t: t and "職缺描述" in t
                )
                if job_description_h3:
                    description_container = job_description_h3.find_next_sibling()
                    if description_container:
                        description = description_container.get_text(
                            separator="\n", strip=True
                        )

        # Derive job_type first, as it's needed for derive_salary_type
        job_type = JOB_TYPE_MAPPING_1111.get(job_type_str)
        if job_type is None:
            job_type = JobType.OTHER

        # Use the new parse_salary_text from salary_parser.py
        salary_min, salary_max = parse_salary_text(salary_text or "")
        # Derive salary_type using the new function, passing job_type
        salary_type = derive_salary_type(salary_text or "", salary_min, job_type)

        if experience_required_text is None:
            experience_required_text = "不拘"
        if education_required_text is None:
            education_required_text = "不拘"

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_1111,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=company_source_id,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing 1111 job HTML.",
            error=e,
            url=url,
            exc_info=True,
        )
        return None


================================================
FILE: crawler/project_1111/producer_category_1111.py
================================================
from crawler.project_1111.task_category_1111 import fetch_and_sync_1111_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_1111.config_1111 import JOB_CAT_URL_1111

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_1111_categories.s(JOB_CAT_URL_1111).apply_async(queue='producer_category_1111')
logger.info("send task_category_1111 url", url=JOB_CAT_URL_1111, queue='producer_category_1111')


================================================
FILE: crawler/project_1111/producer_jobs_1111.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_1111.task_jobs_1111 import fetch_url_data_1111
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_1111_job_urls():
    """
    從資料庫讀取待處理或失敗的 1111 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 1111 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_1111,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 1111 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 1111 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 1111 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_1111.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_1111")

        logger.info(
            "已成功分發一批 1111 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_1111"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_1111/producer_urls_1111.py
================================================
from crawler.database.repository import get_all_categories_for_platform
from crawler.project_1111.task_urls_1111 import crawl_and_store_1111_category_urls
from crawler.database.models import SourcePlatform
import structlog

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Starting URL task distribution for all 1111 categories.")

all_1111_categories = get_all_categories_for_platform(SourcePlatform.PLATFORM_1111)

if all_1111_categories:
    logger.info("Found categories for PLATFORM_1111.", count=len(all_1111_categories))
    root_categories = [
        cat for cat in all_1111_categories if cat.parent_source_id is None
    ]

    if root_categories:
        logger.info(
            "Found root categories for PLATFORM_1111.", count=len(root_categories)
        )

        for category_info in root_categories:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id)
            crawl_and_store_1111_category_urls.delay(category_info.model_dump())
    else:
        logger.info("No root categories found for PLATFORM_1111.")
else:
    logger.info("No categories found for PLATFORM_1111.")


================================================
FILE: crawler/project_1111/task_category_1111.py
================================================
# import os
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

import structlog
from crawler.worker import app
from crawler.database.schemas import SourcePlatform
from crawler.project_1111.config_1111 import HEADERS_1111, JOB_CAT_URL_1111
from crawler.project_1111.client_1111 import fetch_category_data_from_1111_api

logger = structlog.get_logger(__name__)


def flatten_jobcat_recursive(node_list):
    """
    Flattens the 1111 job categories list, extracting main/sub categories.
    """
    for node in node_list:
        yield {
            "parent_source_id": str(node.get("parentCode")) if node.get("parentCode") else None,
            "source_category_id": str(node.get("code")),
            "source_category_name": node.get("name"),
            "source_platform": SourcePlatform.PLATFORM_1111.value,
        }


@app.task()
def fetch_and_sync_1111_categories(url_JobCat: str = JOB_CAT_URL_1111):
    import crawler.database.repository as repository
    logger.info("Starting 1111 category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_1111)

        jobcat_data = fetch_category_data_from_1111_api(url_JobCat, HEADERS_1111)
        if jobcat_data is None:
            logger.error("Failed to fetch 1111 category data from API.", url=url_JobCat)
            return

        job_position_data = jobcat_data.get("jobPosition", [])
        if not job_position_data:
            logger.warning("No 'jobPosition' data found in 1111 category API response.", url=url_JobCat)
            return

        flattened_data = list(flatten_jobcat_recursive(job_position_data))

        if not existing_categories:
            logger.info("1111 category database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            repository.sync_source_categories(SourcePlatform.PLATFORM_1111, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_1111.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            logger.info(
                "Found new or updated 1111 categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_1111, categories_to_sync)
        else:
            logger.info("No new or updated 1111 categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during 1111 category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    # python -m crawler.project_1111.task_category_1111
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    logger.info("Dispatching fetch_and_sync_1111_categories task for local testing.", url=JOB_CAT_URL_1111)
    fetch_and_sync_1111_categories(JOB_CAT_URL_1111)



================================================
FILE: crawler/project_1111/task_jobs_1111.py
================================================
import os
# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

import structlog
from typing import Optional
from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_1111.client_1111 import fetch_job_data_from_1111_web
from crawler.project_1111.parser_apidata_1111 import parse_job_detail_html_to_pydantic
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)


@app.task()
def fetch_url_data_1111(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed information for a single job vacancy from a given URL,
    parses it, stores it in the database, and marks the URL's crawl status.
    """
    job_id = None
    try:
        job_id = url.split("/")[-1].split("?")[0]
        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        response_data = fetch_job_data_from_1111_web(url)
        if response_data is None or "content" not in response_data:
            logger.error("Failed to fetch job data from 1111 web.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        html_content = response_data["content"]

    except Exception as e:
        logger.error(
            "Unexpected error during web fetch or job ID extraction.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_pydantic_data = parse_job_detail_html_to_pydantic(html_content, url)

    if not job_pydantic_data:
        logger.error("Failed to parse job data.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    try:
        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when upserting job data.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    # To run this script for local testing, execute:
    # python -m crawler.project_1111.task_jobs_1111
    # This will automatically use the 'test_db' as configured at the top of the script.

    from crawler.database.connection import initialize_database
    initialize_database()

    PRODUCER_BATCH_SIZE = 20 # Changed from 10 to 20
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]

    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_1111,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info(f"Found {len(urls_to_process)} URLs to process.")
        for url in urls_to_process:
            logger.info(f"Processing URL: {url}")
            fetch_url_data_1111(url)
    else:
        logger.info("No URLs found to process for testing.")


================================================
FILE: crawler/project_1111/task_urls_1111.py
================================================
# import os
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from collections import deque
from typing import Set, List
from crawler.worker import app
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    upsert_jobs,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.project_1111.client_1111 import fetch_job_urls_from_1111_api
from crawler.project_1111.parser_apidata_1111 import parse_job_list_json_to_pydantic
from crawler.config import (
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_1111.config_1111 import (
    URL_CRAWLER_ORDER_BY_1111,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_1111_category_urls(job_category: dict, url_limit: int = 0) -> int:
    """
    Celery task: Iterates through all pages of a specified 1111 job category, fetches job URLs
    and preliminary data, and stores them in the database in batches.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    
    global_job_url_set = set()
    current_batch_jobs = []
    current_batch_urls = []
    current_batch_url_categories = []
    recent_counts = deque(maxlen=4)

    current_page = 1
    logger.info(
        "Task started: crawling 1111 job category URLs and data.",
        job_category_code=job_category_code,
        url_limit=url_limit,
    )

    while True:
        if url_limit > 0 and len(global_job_url_set) >= url_limit:
            logger.info(
                "URL limit reached. Ending task early.",
                job_category_code=job_category_code,
                url_limit=url_limit,
                collected_urls=len(global_job_url_set),
            )
            break

        if current_page % 5 == 1:
            logger.info(
                "Current page being processed.",
                page=current_page,
                job_category_code=job_category_code,
            )

        api_response = fetch_job_urls_from_1111_api(
            KEYWORDS="",
            CATEGORY=job_category_code,
            ORDER=URL_CRAWLER_ORDER_BY_1111,
            PAGE_NUM=current_page,
        )

        if api_response is None:
            logger.error(
                "Failed to retrieve data from 1111 API.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        job_items = api_response.get("result", {}).get("hits", [])
        if not isinstance(job_items, list):
            logger.error(
                "API response 'result.hits' format is incorrect or missing.",
                page=current_page,
                job_category_code=job_category_code,
                api_data_type=type(job_items),
            )
            break

        if not job_items:
            logger.info(
                "No more job items found. Ending task.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        for job_item in job_items:
            job_pydantic = parse_job_list_json_to_pydantic(job_item)
            if job_pydantic and job_pydantic.url:
                if job_pydantic.url not in global_job_url_set:
                    global_job_url_set.add(job_pydantic.url)
                    current_batch_jobs.append(job_pydantic)
                    current_batch_urls.append(job_pydantic.url)
                
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=job_pydantic.url,
                        source_category_id=job_category_code,
                    ).model_dump()
                )

        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "Batch upload size reached. Starting data upload.",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_jobs(current_batch_jobs)
            upsert_urls(SourcePlatform.PLATFORM_1111, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            
            current_batch_jobs.clear()
            current_batch_urls.clear()
            current_batch_url_categories.clear()

        total_jobs = len(global_job_url_set)
        recent_counts.append(total_jobs)
        if len(recent_counts) == recent_counts.maxlen and len(set(recent_counts)) == 1:
            logger.info(
                "No new data found consecutively. Ending task early.",
                job_category_code=job_category_code,
            )
            break

        current_page += 1

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining data to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_jobs(current_batch_jobs)
        upsert_urls(SourcePlatform.PLATFORM_1111, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No new data collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code)
    return len(global_job_url_set)


if __name__ == "__main__":
    # python -m crawler.project_1111.task_urls_1111
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    n_days = 7  # Define n_days for local testing
    url_limit = 100000

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_1111)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_1111)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_1111, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]


    if categories_to_dispatch:
        # categories_to_process_single = [categories_to_dispatch[0]]

        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_1111_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_1111_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")



================================================
FILE: crawler/project_cakeresume/cake_me_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import random
import json
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import pandas as pd
from collections import deque
from concurrent.futures import ThreadPoolExecutor

WEB_NAME = 'Cake_me'
WEB_URL = "https://www.cake.me/job"

# 取得職業類別


# 1. 取得 JSON 資料
# jobcat 檔案名稱
file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
jobtt = requests.get(WEB_URL)
jobtt_soup = BeautifulSoup(jobtt.text, 'html.parser')
scripts = jobtt_soup.find_all('script')
jobcat_data = json.loads(scripts[-1].string)['props']['pageProps']['_nextI18Next']

# 2. 儲存 JSON 資料
with open(file_jobcat_json, "w", encoding="utf-8") as f:
    json.dump(jobcat_data, f, ensure_ascii=False, indent=4)
print(f"職業總覽資料已儲存為 {file_jobcat_json}")

df = pd.json_normalize(jobcat_data)

sector_prefix = 'initialI18nStore.zh-TW.sector.sectors'
sector_groups_prefix = 'initialI18nStore.zh-TW.sector.sector_groups.'
prefix = sector_prefix

filtered_df = (
            df.filter(like=prefix)
              .rename(columns=lambda col: col.replace(prefix, ''))
        )

filtered_df
# Output:
#   職業總覽資料已儲存為 Cake_me_jobcat_json.txt

#     .advertising-marketing-agency_adtech-martech  \

#   0                                  廣告技術 / 行銷技術   

#   

#     .advertising-marketing-agency_advertising  \

#   0                                        廣告   

#   

#     .advertising-marketing-agency_design .advertising-marketing-agency_digital  \

#   0                                   設計                                    數位   

#   

#     .advertising-marketing-agency_event-management  \

#   0                                           事件管理   

#   

#     .advertising-marketing-agency_marketing-communications  \

#   0                                            行銷 / 溝通       

#   

#     .advertising-marketing-agency_public-relations  \

#   0                                           公共關係   

#   

#     .agriculture_agricultural-technology .agriculture_dairy  \

#   0                                 農業科技           乳製品 / 酪農   

#   

#     .agriculture_farming  ... .tech_gambling-casinos .tech_games .tech_hardware  \

#   0                   農業  ...                博弈 / 賭場          遊戲             硬體   

#   

#     .tech_information-services .tech_internet .tech_mobile-apps .tech_robotics  \

#   0                       資訊服務           網際網路            手機應用程式          機器人科學   

#   

#     .tech_saas-cloud-services .tech_semiconductor .tech_software  

#   0               軟體即服務 / 雲服務                 半導體             軟體  

#   

#   [1 rows x 140 columns]

# 產生cake.me網址 https://www.cake.me 根據提供的(關鍵字和職缺類別) 轉換為職缺網址

def cake_me_url(KEYWORDS, CATEGORY, ORDER=None):
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (str, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。

    # 測試範例
    url_1 = cake_me_url("雲端工程師", "it", "latest")    
    # https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page=

    url_2 = cake_me_url("雲端工程師", "")      
    # https://www.cake.me/jobs/雲端工程師?page=

    url_3 = cake_me_url("", "it", "latest")             
    # https://www.cake.me/jobs/categories/it?order=latest&page=

    url_4 = cake_me_url("", "")               
    # https://www.cake.me/jobs?page=
    
    """

    BASE_URL = "https://www.cake.me/jobs"

    if KEYWORDS and CATEGORY:
        url = f"{BASE_URL}/{KEYWORDS}?profession[0]={CATEGORY}&page="
    elif KEYWORDS:
        url = f"{BASE_URL}/{KEYWORDS}?page="
    elif CATEGORY:
        url = f"{BASE_URL}/categories/{CATEGORY}?page="
    else:
        url = f"{BASE_URL}?page="

    if ORDER:  # 只在 ORDER 不為 None 時添加
        url = url.replace("?page=", f"?order={ORDER}&page=")

    return url


# # # 測試範例  類別: {軟體, it}
# url_1 = cake_me_url("雲端工程師", "it", "latest")    
# print(url_1)  # https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page=

# url_2 = cake_me_url("雲端工程師", "")      
# print(url_2)  # https://www.cake.me/jobs/雲端工程師?page=

# url_3 = cake_me_url("", "it", "latest")             
# print(url_3)  # https://www.cake.me/jobs/categories/it?order=latest&page=

# url_4 = cake_me_url("", "")               
# print(url_4)  # https://www.cake.me/jobs?page=

#  從指定的職缺網址獲取工作職缺的網址

def fetch_job_url(joburl):
    """
    這個函數會遍歷多個頁面，並從每個頁面中提取工作職缺的網址，將其存儲在一個集合中以避免重複。
    使用 tqdm 顯示進度條，並在每次請求之間隨機延遲以避免過於頻繁的請求。

    參數:
    joburl (str): 職缺列表的基本網址。
    PAGE (int): 起始頁碼。

    返回:
    list: 包含所有獲取到的工作職缺網址的列表。
    """

    PAGE = 0
    MAX_PAGE = 1

    MAX_LENGTH = 4
    recent_counts = deque(maxlen=MAX_LENGTH)

    job_url_set = set()  # 使用 set() 來存儲網址
    with tqdm(total=MAX_PAGE, desc="cake.me職缺列表 ", unit="PAGE", leave=True) as pbar:
        while True:
            # 獲取當前頁面內容的工作網址
            response = requests.get(f"{joburl}{PAGE}")
            response_soup = BeautifulSoup(response.text, 'html.parser')
            job_urls = response_soup.find_all('a', class_='JobSearchItem_jobTitle__bu6yO')
            for job_url in job_urls:
                job_url_set.add("https://www.cake.me" + job_url['href'])  # 添加到 set 中
            
            # 檢查是否有新資料
            total_jobs = len(job_url_set) 
            recent_counts.append(total_jobs)
            if len(recent_counts) == MAX_LENGTH and len(set(recent_counts)) == 1:
                print(f"連續{MAX_LENGTH}次沒有新資料，提前結束。")
                print(f"Total unique job URLs fetched: {len(job_url_set)}")
                break
               
            # 獲取總頁數
            time.sleep(random.uniform(0.5, 1.5))
            pagination_items = response_soup.find_all('a', class_='Pagination_itemNumber___enNq')
            if pagination_items:
                MAX_PAGE = int(pagination_items[-1].text) + 1
                pbar.total = MAX_PAGE  # 更新進度條的總頁數
            pbar.set_postfix_str(f"目前頁面 {PAGE}, 最大頁數: {MAX_PAGE}")
            pbar.update(1)

            if PAGE <= MAX_PAGE:  
                PAGE = PAGE + 1 
            else:
                break

        return list(job_url_set)  # 將 set 轉換為 list
    
    
# # 測試範例
# joburl = "https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page="
# job_urls = fetch_job_url(joburl)
# job_urls[0]

# 從指定的職缺網址獲取職缺的相關數據

import requests
import pandas as pd
import json
from bs4 import BeautifulSoup

def clean_html_if_string(value):
    """
    輔助函數：只在輸入值為字串時，才清除 HTML 標籤。
    對於其他類型（數字、列表、None 等），直接返回原值。
    """
    if isinstance(value, str):
        # 使用 .get_text() 是從 HTML 中提取純文字的標準方法
        # separator=' ' 在標籤間插入空格，讓文字更自然
        # strip=True 移除開頭和結尾的空白
        return BeautifulSoup(value, "html.parser").get_text(separator=' ', strip=True)
    return value
    

def fetch_job_data(job_url: str) -> pd.DataFrame:
    """
    這個函數會發送 GET 請求到提供的職缺網址，並使用 BeautifulSoup 解析返回的 HTML 文檔。
    它會從頁面中的 JavaScript 代碼中提取職缺的元數據，將其轉換為 Pandas DataFrame，
    並清除特定欄位中的 HTML 標籤。

    參數:
    job_url (str): 職缺的網址。

    返回:
    pd.DataFrame: 包含職缺詳細信息的 DataFrame。如果發生錯誤，則返回一個空的 DataFrame。
    """
    try:
        time.sleep(random.uniform(0.3, 0.8)) 
        
        # 加上 User-Agent，讓請求看起來更像來自瀏覽器
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(job_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        response_soup = BeautifulSoup(response.text, 'html.parser')
        
        data_script = response_soup.find('script', id='__NEXT_DATA__')
        
        if not data_script:
            print("錯誤：在頁面中找不到職缺資料 (script#__NEXT_DATA__) 。")
            return pd.DataFrame()

        page_props = json.loads(data_script.string)['props']['pageProps']

        # 前一版本
        # jobMetaData = json.loads(scripts[-1].string)['props']['pageProps']['ssr']['jobMetaData']['job']
        # df = pd.json_normalize(jobMetaData)
        # # 公司名稱、網址、其他職缺、目前職缺網址
        # office_name = json.loads(scripts[1].string)['itemListElement'][0]['item']['name']
        # office_url = json.loads(scripts[1].string)['itemListElement'][0]['item']['@id']
        # job_other = json.loads(scripts[1].string)['itemListElement'][1]['item']['@id']
        # job_url = json.loads(scripts[1].string)['itemListElement'][2]['item']['@id']
        # df['job_url'] = job_url
        # df['office_name'] = office_name
        # df['office_url'] = office_url
        # df['job_other'] = job_other


        # 在這裡，我們直接從 pageProps 提取 'job' 的資料，而不是 'company'
        # 因為職缺頁面的核心是職缺本身，它內部已經包含了公司資訊
        job_details = page_props.get('job')
        
        if not job_details:
            print("錯誤：無法從 JSON 中解析出職缺資料 ('job' key not found)。")
            return pd.DataFrame()
        
        df = pd.json_normalize([job_details])

        columns_to_clean = [
            'description',
            'job_responsibilities',
            'job_requirements',
            'requirements'
            'job_preferred_requirements',
            'company.description'
        ]

        # 2. 只對存在於 DataFrame 且需要清洗的欄位進行操作
        for col in columns_to_clean:
            if col in df.columns:
        # for col in df.columns:
        #     if col in df.columns and df[col].dtype == 'object':
                df[col] = df[col].apply(clean_html_if_string)
        
        return df

    except requests.exceptions.RequestException as e:
        print(f"網路請求失敗: {e}")
        return pd.DataFrame()
    except (KeyError, TypeError, json.JSONDecodeError) as e:
        print(f"解析頁面資料失敗，頁面結構可能已變更: {e}")
        return pd.DataFrame()


# # --- 測試範例 ---
# # 使用您的範例網址
# job_url = 'https://www.cakeresume.com/companies/qdm/jobs/2f3db0'
# job_data = fetch_job_data(job_url)
# job_data

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime('%Y-%m-%d', time.localtime(time.time()))
KEYWORDS = "雲端工程師"
CATAGORY = "it"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{CATAGORY}"

print( f"開始執行 {FILE_NAME}" )
job_data_list = []
SEARCH_PAGE_URL = cake_me_url(KEYWORDS, CATAGORY)       # 產生cake.me網址
job_urls = fetch_job_url(SEARCH_PAGE_URL)               # 列出職缺列表
with ThreadPoolExecutor(max_workers=3) as executor:    # 列出職缺細項
    futures = [executor.submit(fetch_job_data, url) for url in job_urls]
    for future in tqdm(futures):
        result = future.result()
        if result is not None and not result.empty:  # 確保結果不為空
            job_data_list.append(result)

all_jobs_df = pd.concat(job_data_list, axis=0)  
all_jobs_df.reset_index(drop=True, inplace=True)
all_jobs_df['job_url'] = 'https://www.cake.me/companies/geovision/jobs/' + all_jobs_df['path']

all_jobs_df.shape

all_jobs_df['job_url'] = 'https://www.cake.me/companies/geovision/jobs/' + all_jobs_df['path']
all_jobs_df

# merged_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
all_jobs_df.head(1)

all_jobs_df.columns

column_names = [
    {"序號": 1, "英文": "path", "中文": "職缺頁面路徑"},
    {"序號": 2, "英文": "title", "中文": "職缺標題"},
    {"序號": 3, "英文": "profession_v2", "中文": "職業類別"},
    {"序號": 4, "英文": "job_type", "中文": "工作類型"}, # 例如: 全職, 兼職
    {"序號": 5, "英文": "seniority_level", "中文": "資歷級別"}, # 例如: Entry, Mid, Senior
    {"序號": 6, "英文": "min_work_exp_year", "中文": "最少工作經驗年資"},
    {"序號": 7, "英文": "locations", "中文": "工作地點"},
    {"序號": 8, "英文": "remote", "中文": "是否可遠端工作"},
    {"序號": 9, "英文": "number_of_management", "中文": "管理職責人數"},
    {"序號": 10, "英文": "number_of_openings", "中文": "招聘人數"},
    {"序號": 11, "英文": "salary_min", "中文": "最低薪資"},
    {"序號": 12, "英文": "salary_max", "中文": "最高薪資"},
    {"序號": 13, "英文": "salary_currency", "中文": "薪資貨幣"}, # 例如: TWD, USD
    {"序號": 14, "英文": "salary_type", "中文": "薪資類型"}, # 例如: 月薪, 年薪
    {"序號": 15, "英文": "inclusivity_traits", "中文": "多元共融特質"},
    {"序號": 16, "英文": "tags", "中文": "技能標籤"},
    {"序號": 17, "英文": "description", "中文": "職缺描述"},
    {"序號": 18, "英文": "requirements", "中文": "應徵條件"},
    {"序號": 19, "英文": "category", "中文": "職缺類別(總)"},
    {"序號": 20, "英文": "hide_salary_completely", "中文": "完全隱藏薪資"},
    {"序號": 21, "英文": "hide_salary_max", "中文": "隱藏最高薪資"},
    {"序號": 22, "英文": "year_of_seniority", "中文": "資歷年限(另一種)"},
    {"序號": 23, "英文": "lang", "中文": "刊登語言"},
    {"序號": 24, "英文": "signing_bonus", "中文": "簽約獎金"},
    {"序號": 25, "英文": "unique_impression_count", "中文": "不重複瀏覽次數"},
    {"序號": 26, "英文": "cached_votes_up", "中文": "讚數(快取)"},
    {"序號": 27, "英文": "content_updated_at", "中文": "內容更新時間"},
    {"序號": 28, "英文": "sponsored", "中文": "是否為贊助職缺"},
    {"序號": 29, "英文": "external_url", "中文": "外部應徵連結"},
    {"序號": 30, "英文": "impression_count", "中文": "總瀏覽次數"},
    {"序號": 31, "英文": "interview_process", "中文": "面試流程"},
    {"序號": 32, "英文": "created_at", "中文": "建立時間"},
    {"序號": 33, "英文": "updated_at", "中文": "更新時間"},
    {"序號": 34, "英文": "liked", "中文": "是否已收藏"}, # 使用者個人狀態
    {"序號": 35, "英文": "job_recruiters", "中文": "負責招募者"},
    {"序號": 36, "英文": "job_questions", "中文": "應徵問題"},
    {"序號": 37, "英文": "noindex", "中文": "禁止搜尋引擎索引"},
    {"序號": 38, "英文": "processed_description", "中文": "處理過的職缺描述"},
    {"序號": 39, "英文": "impression_token", "中文": "瀏覽令牌"}, # 技術性欄位
    {"序號": 40, "英文": "common_applied_jobs", "中文": "常見相關應徵職缺"},
    {"序號": 41, "英文": "aasm_state", "中文": "職缺狀態"}, # 技術性欄位 (例如: published, archived)
    {"序號": 42, "英文": "metadata.images", "中文": "相關圖片"},
]

df_new = pd.json_normalize(column_names)
df_new.columns = ["序號", "cake.me_英文", "cake.me_中文"]
df_new

#  前一版本
# column_names = [
#     {"序號": 1, "英文": "name", "中文": "公司名稱"},
#     {"序號": 2, "英文": "description", "中文": "公司簡介"},
#     {"序號": 3, "英文": "path", "中文": "公司頁面路徑"},
#     {"序號": 4, "英文": "unique_impression_count", "中文": "不重複曝光次數"},
#     {"序號": 5, "英文": "address", "中文": "公司地址"},
#     {"序號": 6, "英文": "country", "中文": "國家"},
#     {"序號": 7, "英文": "contact_name", "中文": "聯絡人姓名"},
#     {"序號": 8, "英文": "contact_phone", "中文": "聯絡電話"},
#     {"序號": 9, "英文": "aasm_state", "中文": "工作流程狀態"},
#     {"序號": 10, "英文": "content_updated_at", "中文": "內容更新時間"},
#     {"序號": 11, "英文": "twitter_handle", "中文": "Twitter 帳號"},
#     {"序號": 12, "英文": "email", "中文": "電子郵件"},
#     {"序號": 13, "英文": "phone", "中文": "電話"},
#     {"序號": 14, "英文": "work_environment", "中文": "工作環境"},
#     {"序號": 15, "英文": "employee_benefits", "中文": "員工福利"},
#     {"序號": 16, "英文": "products_or_services", "中文": "產品或服務"},
#     {"序號": 17, "英文": "mission", "中文": "公司使命"},
#     {"序號": 18, "英文": "media_coverage", "中文": "媒體報導"},
#     {"序號": 19, "英文": "cover_image", "中文": "封面圖片"},
#     {"序號": 20, "英文": "youtube_video_url", "中文": "YouTube 影片網址"},
#     {"序號": 21, "英文": "recruiter_contact_info", "中文": "招募人員聯絡資訊"},
#     {"序號": 22, "英文": "recruiting_website_url", "中文": "招募網站網址"},
#     {"序號": 23, "英文": "founded_year", "中文": "創立年份"},
#     {"序號": 24, "英文": "created_at", "中文": "建立時間"},
#     {"序號": 25, "英文": "updated_at", "中文": "更新時間"},
#     {"序號": 26, "英文": "company", "中文": "公司"},
#     {"序號": 27, "英文": "website_url", "中文": "公司網站網址"},
#     {"序號": 28, "英文": "geo_city", "中文": "地理位置-城市"},
#     {"序號": 29, "英文": "geo_formatted_address", "中文": "地理位置-格式化地址"},
#     {"序號": 30, "英文": "geo_state_code", "中文": "地理位置-州/省代碼"},
#     {"序號": 31, "英文": "geo_country_code", "中文": "地理位置-國家代碼"},
#     {"序號": 32, "英文": "geo_zip", "中文": "地理位置-郵遞區號"},
#     {"序號": 33, "英文": "featured", "中文": "精選狀態"},
#     {"序號": 34, "英文": "geo_state_name", "中文": "地理位置-州/省名稱"},
#     {"序號": 35, "英文": "geo_city_l", "中文": "地理位置-城市 (本地化)"},
#     {"序號": 36, "英文": "geo_formatted_address_l", "中文": "地理位置-格式化地址 (本地化)"},
#     {"序號": 37, "英文": "geo_state_name_l", "中文": "地理位置-州/省名稱 (本地化)"},
#     {"序號": 38, "英文": "geo_street_address_l", "中文": "地理位置-街道地址 (本地化)"},
#     {"序號": 39, "英文": "geo_l_locale", "中文": "地理位置-本地化語系"},
#     {"序號": 40, "英文": "sector", "中文": "產業類別"},
#     {"序號": 41, "英文": "facebook_url", "中文": "Facebook 網址"},
#     {"序號": 42, "英文": "linkedin_url", "中文": "LinkedIn 網址"},
#     {"序號": 43, "英文": "instagram_url", "中文": "Instagram 網址"},
#     {"序號": 44, "英文": "medium_url", "中文": "Medium 網址"},
#     {"序號": 45, "英文": "whatsapp_id", "中文": "WhatsApp ID"},
#     {"序號": 46, "英文": "geo_country_l", "中文": "地理位置-國家 (本地化)"},
#     {"序號": 47, "英文": "application_read_rate", "中文": "應徵讀取率"},
#     {"序號": 48, "英文": "application_read_time", "中文": "應徵讀取時間"},
#     {"序號": 49, "英文": "candidate_read_rate", "中文": "應徵者資料讀取率"},
#     {"序號": 50, "英文": "candidate_read_time", "中文": "應徵者資料讀取時間"},
#     {"序號": 51, "英文": "published_at", "中文": "發布時間"},
#     {"序號": 52, "英文": "logo", "中文": "公司 Logo"},
#     {"序號": 53, "英文": "company_overview", "中文": "公司概覽"},
#     {"序號": 54, "英文": "amount_of_capital", "中文": "資本額"},
#     {"序號": 55, "英文": "number_of_employees", "中文": "員工人數"},
#     {"序號": 56, "英文": "og_image", "中文": "OG 圖片 (社群分享預覽圖)"},
#     {"序號": 57, "英文": "ga_tracking_code", "中文": "Google Analytics 追蹤碼"},
#     {"序號": 58, "英文": "tax_id_number", "中文": "統一編號 / 稅務識別碼"},
#     {"序號": 59, "英文": "last_active_at", "中文": "最後活躍時間"},
#     {"序號": 60, "英文": "labels", "中文": "標籤"},
#     {"序號": 61, "英文": "teches", "中文": "使用技術"},
#     {"序號": 62, "英文": "faq_items", "中文": "常見問題項目"},
#     {"序號": 63, "英文": "work_environment_images", "中文": "工作環境圖片"},
#     {"序號": 64, "英文": "currency_symbol", "中文": "貨幣符號"},
#     {"序號": 65, "英文": "followed", "中文": "已追蹤"},
#     {"序號": 66, "英文": "followed_job_notification_type", "中文": "追蹤職缺通知類型"},
#     {"序號": 67, "英文": "total_followers", "中文": "總追蹤人數"},
#     {"序號": 68, "英文": "currency_code", "中文": "貨幣代碼"},
#     {"序號": 69, "英文": "seems_spam", "中文": "疑似垃圾訊息"},
#     {"序號": 70, "英文": "noindex", "中文": "禁止搜尋引擎索引"},
#     {"序號": 71, "英文": "impression_token", "中文": "曝光權杖 (Token)"},
#     {"序號": 72, "英文": "profession_job_counts", "中文": "各職業類別職缺數"},
#     {"序號": 73, "英文": "listed_job_count", "中文": "上架中職缺數"},
#     {"序號": 74, "英文": "sanitized_description", "中文": "純文字(淨化後)的公司簡介"},
#     {"序號": 75, "英文": "sanitized_products_or_services", "中文": "純文字(淨化後)的產品或服務"},
#     {"序號": 76, "英文": "sanitized_mission", "中文": "純文字(淨化後)的公司使命"},
#     {"序號": 77, "英文": "sanitized_media_coverage", "中文": "純文字(淨化後)的媒體報導"},
#     {"序號": 78, "英文": "sanitized_employee_benefits", "中文": "純文字(淨化後)的員工福利"},
#     {"序號": 79, "英文": "sanitized_work_environment", "中文": "純文字(淨化後)的工作環境"}
# ]

# df_new = pd.json_normalize(column_names)
# df_new.columns = ["序號", "cake.me_英文", "cake.me_中文"]
# df_new

# 補充 :  網頁結構解析

url = "https://www.cake.me/jobs/雲端工程師"

response = requests.get(url)
response_soup = BeautifulSoup(response.text, 'html.parser')
scripts = response_soup.find_all('script')


# 查看篩選欄位選項
# options = []
# catagorys = 1
# DropdownButton_contents = response_soup.find_all('div', 'JobSearchPage_searchFilter__ts_A0')
# for JobSearchPage_searchFilter in DropdownButton_contents:
#     title  = JobSearchPage_searchFilter.find('div', 'DropdownButton_content__XZwFf').text
#     # print(title)
#     Checkbox_texts = JobSearchPage_searchFilter.find_all('div', 'Checkbox_text__g6TLq')
#     for Checkbox in Checkbox_texts:
#         # print(Checkbox.text)
#         option_text = Checkbox.text.strip()
#         options.append({'catagorys':catagorys, 'title': title, 'option_text': option_text})
#     catagorys = catagorys +1
# df_searchFilter = pd.DataFrame(options)
# df_searchFilter
# df_searchFilter['option_text'][(df_searchFilter['catagorys']==1)]



# 提取資料結構 使用遞歸函數提取子結構
def extract_keys_with_branch_structure(value_in, current_path=''):
    
    branch_structure = {}
    if isinstance(value_in, dict):
        for index, (key, value) in enumerate(value_in.items()):
            path = f"{current_path}-{index + 1}" if current_path else f"{index + 1}"
            branch_structure[path] = key  # 添加當前層級的鍵

            # path = f"{current_path}-{key}" if current_path else f"{key}"
            # branch_structure[path] = value  # 添加當前層級的鍵
            
            # 使用遞歸查找 子結構、提取與合併
            if isinstance(value, dict):
                branch_structure.update(extract_keys_with_branch_structure(value, path))

    return branch_structure

data = json.loads(scripts[-1].string)
branch_structure = extract_keys_with_branch_structure(data)

json.loads(scripts[-1].string)['props']['pageProps']['serverState']['initialResults']['Job']['state']['query']




================================================
FILE: crawler/project_cakeresume/Cake_me_jobcat_json.txt
================================================
{
    "initialI18nStore": {
        "zh-TW": {
            "404": {
                "title": "找不到頁面",
                "description": "我們找不到您想找的頁面。您可以嘗試回到上一頁或查看我們的 {{- help_center_link }}。",
                "description_help_center": "FAQs"
            },
            "common": {
                "symbol.glue": "",
                "symbol.separator": "、",
                "symbol.comma": "，",
                "symbol.bracket": "【{{ text }}】",
                "time_format.time": "aaah:mm",
                "time_format.weekday_time": "ccc aaah:mm",
                "time_format.weekday": "ccc",
                "time_format.month_day_time": "MMMdo aaah:mm",
                "time_format.month_day": "MMMdo",
                "time_format.year_day_time": "y年MMMdo aaah:mm",
                "time_format.year_day": "y年MMMdo",
                "time_format.year_month": "y年MMM",
                "time_format.year": "y年",
                "time_format.present": "現在",
                "time_format.n_year": "年",
                "time_format.n_year@one": "年",
                "time_format.n_year@other": "年",
                "time_format.n_month": "個月",
                "time_format.n_month@one": "個月",
                "time_format.n_month@other": "個月",
                "terminology.job": "職缺",
                "terminology.company": "公司",
                "terminology.resume_builder": "履歷工具",
                "terminology.resume_builder_plans": "履歷製作價格方案",
                "terminology.job_posting": "職缺刊登",
                "terminology.job_posting_plans": "職缺刊登價格方案",
                "terminology.talent_search": "人才搜尋引擎",
                "terminology.talent_search_plans": "人才搜尋引擎價格方案",
                "terminology.help_center": "FAQs",
                "terminology.press_kit": "新聞資料袋",
                "terminology.terms_of_service": "服務條款",
                "terminology.privacy_policy": "隱私政策",
                "terminology.app_job_search": "Cake 找工作",
                "terminology.eor_service": "名義雇主（EoR）服務",
                "terminology.employer_branding": "雇主品牌推廣",
                "terminology.employer_branding_ebook": "雇主品牌白皮書",
                "terminology.reputation_credits": "職場能力評價",
                "terminology.credit": "評價",
                "terminology.credit@one": "評價",
                "terminology.credit@other": "評價",
                "status.following": "追蹤中",
                "status.saved": "已儲存",
                "status.done": "完成",
                "status.error": "錯誤",
                "action.download_apps": "下載 Cake App",
                "action.sign_in": "登入",
                "action.sign_out": "登出",
                "action.sign_up": "註冊",
                "action.load_more": "載入更多",
                "action.add": "新增",
                "action.create": "建立",
                "action.update": "更新",
                "action.save": "儲存",
                "action.delete": "刪除",
                "action.cancel": "取消",
                "action.import": "匯入",
                "action.edit": "編輯",
                "action.confirm": "確認",
                "action.previous": "上一個",
                "action.next": "下一個",
                "action.previous_step": "上一步",
                "action.next_step": "下一步",
                "action.more": "顯示更多",
                "action.follow": "追蹤",
                "action.unfollow": "取消追蹤",
                "action.select": "選擇",
                "action.select_all": "全選",
                "action.back": "返回",
                "action.continue": "繼續",
                "action.done": "完成",
                "action.skip": "略過",
                "action.close": "關閉",
                "action.search": "搜尋",
                "action.share": "分享",
                "action.print": "列印",
                "action.publish": "發布",
                "action.copy": "複製",
                "action.view_on_dashboard": "前往主控台查看",
                "action.back_to_dashboard": "返回主控台",
                "action.talk_to_our_consultants": "和我們的顧問聊聊",
                "action.reset": "重設",
                "action.view_all": "查看全部",
                "action.view_more": "查看更多",
                "action.learn_more": "了解詳情",
                "action.read_more": "閱讀更多",
                "action.show_more": "展開",
                "action.show_less": "收合",
                "action.subscribe": "訂閱重要通知",
                "action.unsubscribe": "取消訂閱",
                "action.upload_image": "上傳圖片",
                "action.report": "檢舉",
                "action.submit": "提交",
                "action.view": "檢視",
                "action.pay": "付款",
                "action.go_complete_profile": "前往完善我的個人檔案",
                "action.looks_good": "確定",
                "action.try_now": "立即使用",
                "action.start_now": "立即開始",
                "action.sign_up_now": "立即註冊",
                "message.others": "其他",
                "message.coming_soon": "即將推出",
                "message.category": "類別",
                "message.divider_or": "或",
                "message.period_ago": "{{ period }}前",
                "message.n_selected": "已選 {{ count }} 項",
                "message.info": "提示",
                "message.success": "成功",
                "message.error": "錯誤",
                "message.copied": "已複製",
                "message.updated": "已更新",
                "message.updating": "更新中...",
                "message.updated_at": "最後更新於 {{ date }}",
                "message.uploaded_at": "上傳於 {{ date }}",
                "message.published_at": "發佈時間：{{ date }}",
                "message.loading": "載入中...",
                "message.no_results": "沒有符合的項目。",
                "message.no_content": "尚無內容。",
                "message.confirm_delete": "確定要刪除此項目嗎？",
                "message.share_to": "分享到",
                "message.new": "全新",
                "message.by": "來自",
                "message.talent_network_mobile_only": "Meet 目前為 App 專屬功能，詳細資訊請至 App 上查看",
                "message.report_success": "檢舉完成",
                "message.report_success_description": "我們已經收到您的反饋。 感謝您檢舉此可疑活動。",
                "message.report_request": "請幫助我們釐清狀況",
                "placeholder.select": "請選擇",
                "placeholder.file": "選擇檔案",
                "placeholder.describe": "請描述",
                "header_v2.find_jobs.title": "找工作",
                "header_v2.find_jobs.job_search.title": "搜尋職缺",
                "header_v2.find_jobs.job_search.subtitle": "探索不同產業和地區的所有工作機會",
                "header_v2.find_jobs.company_search.title": "搜尋公司",
                "header_v2.find_jobs.company_search.subtitle": "根據公司名稱尋找理想工作",
                "header_v2.find_jobs.themed_jobs.title": "主題專區",
                "header_v2.find_jobs.themed_jobs.subtitle": "探索依特定主題或產業分類的工作機會",
                "header_v2.tools.title": "求職工具",
                "header_v2.tools.resume.title": "履歷",
                "header_v2.tools.resume.subtitle": "使用我們的免費履歷工具，獲取理想職缺",
                "header_v2.tools.resume_builder.title": "履歷工具",
                "header_v2.tools.resume_builder.subtitle": "免費製作、下載履歷",
                "header_v2.tools.resume_templates.title": "履歷模板",
                "header_v2.tools.resume_templates.subtitle": "提供大量專業模板立即使用",
                "header_v2.tools.resume_examples.title": "履歷範例",
                "header_v2.tools.resume_examples.subtitle": "從他人履歷獲取製作靈感",
                "header_v2.tools.occupation_guide.title": "職業指南",
                "header_v2.tools.occupation_guide.subtitle": "各產業、職能的履歷教學與範例",
                "header_v2.tools.resume_help.title": "履歷協助",
                "header_v2.tools.resume_help.subtitle": "從我們的招募團隊獲取關於履歷的專業建議",
                "header_v2.tools.ai_resume_health_check.title": "Cake AI 履歷健檢",
                "header_v2.tools.ai_resume_health_check.subtitle": "全方位診斷您的履歷",
                "header_v2.tools.portfolio.title": "作品集",
                "header_v2.tools.portfolio.subtitle": "分享你的作品集展現你的成功專案",
                "header_v2.tools.portfolio_maker.title": "作品集工具",
                "header_v2.tools.portfolio_maker.subtitle": "製作一份展現個人專業的作品集",
                "header_v2.tools.portfolio_gallery.title": "作品集展示區",
                "header_v2.tools.portfolio_gallery.subtitle": "瀏覽他人的真實作品集，尋找靈感並進行人脈拓展",
                "header_v2.tools.cake_ai.title": "Cake AI",
                "header_v2.tools.cake_ai.subtitle": "使用 Cake AI 功能，加速求職與徵才效率",
                "header_v2.tools.wcru_pro.title": "你是什麼蛋糕? 2.0",
                "header_v2.tools.wcru_pro.subtitle": "3 分鐘揭曉職場 16 型人格分析，發掘你的專屬天賦與優勢！",
                "header_v2.resources.title": "資源",
                "header_v2.resources.articles.title": "資源",
                "header_v2.resources.articles.subtitle": "從豐富內容了解職業發展、求職策略等更多資訊",
                "header_v2.resources.articles.view_all": "查看全部文章",
                "header_v2.resources.white_paper.title": "白皮書",
                "header_v2.resources.featured_article.heading": "精選文章",
                "header_v2.resources.featured_article.read_more": "閱讀更多",
                "header_v2.resources.talent_connect.title": "《科技職涯》Podcast",
                "header_v2.resources.talent_connect.subtitle": "專門邀請在科技、數位等不同領域的工作者來分享他們的職涯趣事。",
                "header_v2.resources.career_adventure.title": "《職涯探險》Podcast",
                "header_v2.resources.career_adventure.subtitle": "透過分享跨域思維與職涯選擇，啟發年輕人才實踐職涯目標和理想生活",
                "header_v2.hire_v2.title": "徵才",
                "header_v2.hire_v2.cake_for_employers.title": "歡迎使用 Cake for Employers",
                "header_v2.hire_v2.cake_for_employers.subtitle": "整合人才搜尋、職缺刊登與招募管理系統，為您打造一站式的招募體驗。",
                "header_v2.hire_v2.recruitment_software": "招募軟體",
                "header_v2.hire_v2.recruitment_solutions": "招募解決方案",
                "header_v2.hire_v2.pricing_and_proof": "價格與實證",
                "header_v2.hire_v2.feature.sourcing_talent_search": "人才搜尋",
                "header_v2.hire_v2.feature.job_posting": "$t(terminology.job_posting)",
                "header_v2.hire_v2.feature.ats": "應徵者追蹤系統",
                "header_v2.hire_v2.feature.recruitment_service": "Cake 獵才顧問",
                "header_v2.hire_v2.feature.employer_branding": "$t(terminology.employer_branding)",
                "header_v2.hire_v2.feature.eor_service": "$t(terminology.eor_service)",
                "header_v2.hire_v2.feature.pricing_plans": "Cake 價格方案",
                "header_v2.hire_v2.feature.success_stories": "成功案例",
                "header_v2.pricing.title": "價格方案",
                "header_v2.pricing.job_posting_plans": "$t(terminology.job_posting_plans)",
                "header_v2.pricing.talent_search_plans": "$t(terminology.talent_search_plans)",
                "header_v2.pricing.resume_builder_plans": "$t(terminology.resume_builder_plans)",
                "header_v2.meet.title": "加入擁有超過 5,000 位用戶的 Cake Meet 人脈網絡",
                "header_v2.meet.description": "透過簡單的左右滑動，遇見各領域專業人才，拓展您的職涯人脈！",
                "header_v2.meet.cta": "瞭解更多",
                "header_v2.network.title": "建立你的人脈",
                "header_v2.network.my_network.title": "我的人脈",
                "header_v2.network.my_network.subtitle": "管理人脈及你的聯繫對象",
                "header_v2.network.meet.title": "Cake Meet",
                "header_v2.network.meet.subtitle": "透過認識並連結其他使用者，擴大你的職涯人脈",
                "header_v2.user.dashboard": "主控台",
                "header_v2.user.my_resumes": "我的履歷",
                "header_v2.user.my_portfolio": "我的作品集",
                "header_v2.user.settings": "設定",
                "header_v2.user.plan_and_billing": "方案內容和帳單",
                "header_v2.user.free_upgrade": "免費升級",
                "header_v2.user.referral": "邀請朋友",
                "header_v2.user.account_and_billing": "帳戶和付費方案",
                "header_v2.user.notification_settings": "通知設定",
                "header_v2.user.help_center": "$t(terminology.help_center)",
                "header_v2.user.upgrade": "升級",
                "header_v2.user.page.applications": "應徵者",
                "header_v2.user.page.analytics": "分析報告",
                "header_v2.user.page.create_company": "新增公司",
                "header_v2.user.page.view_all_companies": "查看所有公司",
                "header_v2.user.page.join_request_sent": "已申請加入",
                "header_v2.user.page.invitation_to_be_role": "邀請成為{{ role }}",
                "footer.company.title": "公司",
                "footer.company.about": "關於",
                "footer.company.resume_builder_plans": "$t(terminology.resume_builder_plans)",
                "footer.company.job_posting_plans": "$t(terminology.job_posting_plans)",
                "footer.company.talent_search_plans": "$t(terminology.talent_search_plans)",
                "footer.company.press_kit": "$t(terminology.press_kit)",
                "footer.company.hiring": "加入我們",
                "footer.company.contact_us": "聯繫我們",
                "footer.company.terms_of_service": "$t(terminology.terms_of_service)",
                "footer.company.privacy_policy": "$t(terminology.privacy_policy)",
                "footer.solutions.title": "解決方案",
                "footer.solutions.resume_builder": "$t(terminology.resume_builder)",
                "footer.solutions.portfolio": "線上作品集",
                "footer.solutions.job_search": "找工作",
                "footer.solutions.job_posting": "刊登職缺 – 免費開始",
                "footer.solutions.talent_search": "$t(terminology.talent_search)",
                "footer.solutions.recruitment_service": "Cake 獵才顧問",
                "footer.solutions.eor_service": "$t(terminology.eor_service)",
                "footer.solutions.employer_branding": "$t(terminology.employer_branding)",
                "footer.job_search.title": "找工作",
                "footer.job_search.categories": "職缺分類",
                "footer.job_search.remote": "遠端工作",
                "footer.job_search.internship": "實習機會",
                "footer.job_search.part_time": "兼職工作",
                "footer.resources.title": "資源",
                "footer.resources.articles": "文章",
                "footer.resources.ebooks": "免費電子書",
                "footer.resources.resume_examples": "履歷範本",
                "footer.resources.resume_templates": "履歷模板",
                "footer.resources.help_center": "$t(terminology.help_center)",
                "footer.resources.employer_branding_ebook": "$t(terminology.employer_branding_ebook)",
                "footer.languages.title": "語言",
                "footer.follow_us.title": "追蹤我們",
                "footer.follow_us.description": "在社群媒體上追蹤我們，以獲得最新履歷、求職、和招募資訊！",
                "footer.app_job_search.title": "$t(terminology.app_job_search)",
                "category.target_reader.job_seekers": "求職者",
                "category.target_reader.recruiters": "企業招募",
                "category.article.all": "全部",
                "category.article.resume": "履歷、求職信教學",
                "category.article.job_searching_guide": "求職指南",
                "category.article.job_interview": "面試技巧",
                "category.article.career": "職涯發展",
                "category.article.job_searching_resource": "求職管道",
                "category.article.company_interview": "人物或企業專訪",
                "category.article.recruitment_guides": "人才招募技巧",
                "category.article.recruitment_partnership": "獵頭合作指南",
                "category.resume_example.administrative": "行政",
                "category.resume_example.design": "設計",
                "category.resume_example.engineering": "工程",
                "category.resume_example.finance": "金融",
                "category.resume_example.management": "管理",
                "category.resume_example.media": "媒體",
                "category.resume_example.sales": "銷售",
                "category.resume_example.service": "服務",
                "category.resume_example.technology": "科技",
                "meta.description": "找工作，從製作一份專業履歷開始。Cake 提供免費履歷/CV工具、履歷範本、履歷模板、職缺搜尋、求職教學、企業徵才招募。百萬求職者的選擇。",
                "meta.image": "common/cover-default-zh-tw-v3.png",
                "error_401.title": "您需要權限",
                "flash_message.global.premium_snippets_hint": "個人檔案已完成，恭喜您獲得免費的進階履歷板塊！進入履歷編輯器後即可開始使用。",
                "flash_message.job_saved.follow_company_hint": "建議您也可以追蹤公司以收到最新職缺通知",
                "validation.mixed.default": "「{{ path }}」的值無效",
                "validation.mixed.required": "「{{ path }}」是必填欄位",
                "validation.mixed.one_of": "「{{ path }}」必須為下列值之一： {{ values }}",
                "validation.mixed.not_one_of": "「{{ path }}」不能為下列值之一： {{ values }}",
                "validation.mixed.defined": "「{{ path }}」必須有定義",
                "validation.string.length": "「{{ path }}」長度必須剛好 {{ length }} 個字元",
                "validation.string.min": "「{{ path }}」長度最短為 {{ min }} 個字元",
                "validation.string.max": "「{{ path }}」長度最長為 {{ max }} 個字元",
                "validation.string.matches": "「{{ path }}」格式必須為： \"{{ regex }}\"",
                "validation.string.email": "「{{ path }}」必須是有效 Email",
                "validation.string.url": "「{{ path }}」必須是有效 URL",
                "validation.string.uuid": "「{{ path }}」必須是有效 UUID",
                "validation.string.trim": "「{{ path }}」首尾不能有空白字元",
                "validation.string.lowercase": "「{{ path }}」必須是小寫字串",
                "validation.string.uppercase": "「{{ path }}」必須是大寫字串",
                "validation.number.min": "「{{ path }}」必須大於或等於 {{ min }}",
                "validation.number.max": "「{{ path }}」必須小於或等於 {{ max }}",
                "validation.number.less_than": "「{{ path }}」必須小於 {{ less }}",
                "validation.number.more_than": "「{{ path }}」必須大於 {{ more }}",
                "validation.number.positive": "「{{ path }}」必須為正值",
                "validation.number.negative": "「{{ path }}」必須為負值",
                "validation.number.integer": "「{{ path }}」必須為正整數",
                "validation.date.min": "「{{ path }}」必須晚於 {{ min }}",
                "validation.date.max": "「{{ path }}」必須早於 {{ max }}",
                "validation.boolean.is_value": "「{{ path }}」必須是 {{ value }}",
                "validation.array.min": "「{{ path }}」最少必須有 {{ min }} 個項目",
                "validation.array.max": "「{{ path }}」最多只能有 {{ max }} 個項目",
                "validation.array.length": "「{{ path }}」必須剛好有 {{ length }} 個項目",
                "validation.custom.password": "「{{ path }}」必須包含至少以下兩種：大寫字母、小寫字母、數字、特殊符號 ('-!\"#$%&()*,./:;?@[]^_`{|}~+<=>)",
                "validation.custom.equal_to": "「{{ path }}」必須與「{{ field }}」相同",
                "validation.custom.gte_to": "「{{ path }}」必須大於或等於「{{ field }}」",
                "validation.custom.lte_to": "「{{ path }}」必須小於或等於「{{ field }}」",
                "validation.custom.file_size": "「{{ path }}」檔案大小限制為 {{ size }}",
                "validation.custom.record_path": "「{{ path }}」只能使用字母、數字與 _.-",
                "validation.custom.empty_or_length_range": "「{{ path }}」必須為空值或是 {{ min }} 至 {{ max }} 個字元",
                "validation.custom.domain_suffix": "「{{ path }}」必須包含網域 {{ suffix }}",
                "validation.custom.phone_number": "「{{ path }}」不是有效電話號碼",
                "validation.custom.must_be_checked": "必須勾選此欄位",
                "validation.custom.credit_card_cvc": "無效的信用卡驗證碼",
                "validation.custom.credit_card_expiration_date": "無效的卡片到期日",
                "validation.custom.credit_card_number": "無效的信用卡號",
                "landing_page.faq.title": "常見問題",
                "landing_page.testimonial.title": "聽聽使用者怎麼說"
            },
            "widget": {
                "messenger.title": "即時訊息",
                "messenger.header.messages": "訊息",
                "messenger.header.archived": "封存",
                "messenger.header.unread": "未讀",
                "messenger.filter.messages": "所有訊息",
                "messenger.filter.archived": "封存",
                "messenger.filter.unread": "未讀",
                "messenger.success.archive.message": "訊息已封存",
                "messenger.success.archive.action.view_archived": "查看所有封存訊息",
                "messenger.success.restore.message": "訊息已還原",
                "messenger.success.restore.action.view_all": "查看所有訊息",
                "messenger.confirm.mark_all_as_read.title": "將所有對話標示為已讀",
                "messenger.confirm.mark_all_as_read.description": "您確定要將所有未讀訊息標示為已讀嗎？",
                "messenger.confirm.mark_all_as_read.action.proceed": "確定",
                "messenger.confirm.reject_connection_and_decline_conversation.title": "確定要拒絕建立關係與訊息請求嗎？",
                "messenger.confirm.reject_connection_and_decline_conversation.description": "拒絕後您將不會收到對方任何訊息與通知。",
                "messenger.message.draft": "草稿",
                "messenger.message.users": "使用者",
                "messenger.message.messages": "訊息",
                "messenger.message.n_messages_found": "找到 {{ count }} 則訊息",
                "messenger.message.selected": "已選取",
                "messenger.message.no_messages": "沒有訊息",
                "messenger.message.no_archived_messages": "沒有封存訊息",
                "messenger.message.no_unread_messages": "沒有未讀訊息",
                "messenger.message.new_message_document_title": "{{ name }} 傳了訊息給你",
                "messenger.message.my_message": "你：{{ content }}",
                "messenger.message.new_message": "新訊息",
                "messenger.message.attachment": "傳送了一個附件",
                "messenger.message.replied_image": "圖片",
                "messenger.message.replied_attachment": "附件",
                "messenger.message.current_user_emoji": "你傳達了{{ emoji }}",
                "messenger.message.opposite_user_emoji": "{{ name }}傳達了{{ emoji }}",
                "messenger.message.both_users_emoji": "{{ name }}和你傳達了{{ emoji }}",
                "messenger.message.cancellable_emoji": "點一下可移除",
                "messenger.message.empty": "建立你的人脈",
                "messenger.message.empty_description": "在企業團隊頁面或人才搜尋引擎點擊 {{- icon }} 圖示以開啟對話。",
                "messenger.message.conversation_invited": "{{ name }} 想傳送訊息給您",
                "messenger.message.conversation_declined": "已經忽略訊息請求。您將不會收到新訊息通知。",
                "messenger.message.connection_invited": "回覆來自 {{ name }} 的建立關係邀請",
                "messenger.message.connection_and_conversation_invited": "{{ name }} 想傳訊息給您並建立關係",
                "messenger.message.new_conversation_title": "向 {{ name }} 打聲招呼吧",
                "messenger.message.new_conversation_description": "撰寫一則好的 Cake 訊息，有助於提高 40% 回覆率。如果您不確認如何適當開啟對話，請參考{{- article }}。",
                "messenger.message.new_conversation_description_article": "這篇文章",
                "messenger.message.new_conversation_hint": "輸入收件人名稱開始新對話",
                "messenger.message.upgrade_title": "升級您的方案以開始與 {{ name }} 的對話",
                "messenger.message.upgrade_hint": "您目前的方案為每日限制 10 則新對話。",
                "messenger.message.press_enter_to_send": "按 Enter 送出",
                "messenger.message.search_hint": "搜尋訊息",
                "messenger.message.request_pending": "因對方還不是您的聯絡人，需等待對方答應訊息/建立關係請求。",
                "messenger.message.request_accepted_by_current_user": "您已接受對方的聊天訊息",
                "messenger.message.request_accepted_by_opposite_user": "對方已接受您的聊天訊息",
                "messenger.message.connection_request_accepted_by_current_user": "您已接受對方的聊天請求並建立關係",
                "messenger.message.connection_request_accepted_by_opposite_user": "對方已接受您的聊天與建立關係請求",
                "messenger.message.talent_network_matched": "配對成功！",
                "messenger.message.talent_network_matched_by_current_user": "{{- name }} 以 {{- pool }} 的身份與您配對成功",
                "messenger.message.talent_network_matched_by_current_user_all": "{{- name }} 與您配對成功",
                "messenger.message.talent_network_matched_by_opposite_user": "{{- name }} 對於您 {{- pool }} 的身分有興趣進一步交流",
                "messenger.message.talent_network_matched_by_opposite_user_all": "{{- name }} 有興趣進一步交流",
                "messenger.placeholder": "輸入訊息...",
                "messenger.action.reply": "回覆",
                "messenger.action.emoji": "傳達心情",
                "messenger.action.new_message": "新訊息",
                "messenger.action.manage": "管理對話",
                "messenger.action.archive": "封存",
                "messenger.action.restore": "還原",
                "messenger.action.mark_as_read": "標示為已讀取",
                "messenger.action.mark_as_unread": "標示為未讀取",
                "messenger.action.mark_all_as_read": "全部標為已讀",
                "messenger.action.search_companies": "搜尋企業",
                "messenger.action.search_talents": "搜尋人才",
                "messenger.action.send": "送出訊息",
                "messenger.action.accept_conversation": "接受",
                "messenger.action.decline_conversation": "忽略",
                "messenger.action.accept_connection_and_accept_conversation": "接受",
                "messenger.action.reject_connection_and_decline_conversation": "拒絕",
                "messenger.action.reject_connection_but_accept_conversation": "僅接受對話請求",
                "messenger.action.resume_conversation": "重啟對話",
                "messenger.action.start_conversation": "開始對話",
                "messenger.action.upgrade": "立即升級",
                "messenger.action.find_connections": "搜尋人脈",
                "messenger.instant_reply.title": "點擊下方，立即回覆！",
                "messenger.instant_reply.description": "主動回覆訊息，讓雇主對你留下好的印象，未來求職成功機率更高！",
                "messenger.instant_reply.template.interested": "我有興趣了解更多此職位的資訊。",
                "messenger.instant_reply.template.not_now": "我暫時沒有看新機會的意願。",
                "messenger.instant_reply.template.set_up_date": "好，我們約一個時間來討論這個職位。",
                "messenger.new_message.label": "給：",
                "messenger.new_message.placeholder": "輸入收件人名稱",
                "messenger.emoji.thumbs_up": "讚",
                "messenger.emoji.clap": "拍手",
                "messenger.emoji.heart": "愛心",
                "messenger.emoji.joy": "快樂",
                "messenger.emoji.smile": "微笑",
                "messenger.emoji.other": "心情",
                "message_channel_list.title": "訊息",
                "message_channel_list.action.view_all": "全部訊息",
                "notification.title": "通知",
                "notification.message.empty": "尚無任何通知",
                "notification.message.empty_description": "當有重要訊息時，我們會通知您。",
                "notification.action.view_all": "查看所有通知",
                "notification.action.mark_all_as_read": "將所有標為已讀",
                "response_statistic.badge": "快速回覆訊息",
                "response_statistic.badge_tip": "縮短回覆訊息的時間來獲得快速回覆徽章",
                "response_statistic.rate": "回覆率 {{ percentage }}",
                "response_statistic.rate_low": "回覆率約 75％",
                "response_statistic.time": "平均 {{ duration }}內回覆",
                "response_statistic.time_minute": "{{ count }} 分鐘",
                "response_statistic.time_minute@one": "{{ count }} 分鐘",
                "response_statistic.time_minute@other": "{{ count }} 分鐘",
                "response_statistic.time_hour": "{{ count }} 小時",
                "response_statistic.time_hour@one": "{{ count }} 小時",
                "response_statistic.time_hour@other": "{{ count }} 小時",
                "response_statistic.rate_and_time": "{{ rate }}（{{ time }}）",
                "connection.tooltip.add_to_folder": "加入招募專案",
                "connection.tooltip.invite": "邀請應徵職缺",
                "connection.tooltip.message": "傳送訊息",
                "connection.action.connect": "建立關係",
                "connection.action.pending": "等候回覆",
                "connection.action.received": "回覆",
                "connection.action.connected": "聯絡人",
                "connection.action.accept": "接受邀請",
                "connection.action.reject": "拒絕",
                "connection.action.cancel": "取消送出邀請",
                "connection.action.remove": "取消關係",
                "connection.connection_message.customize_connection_request": "自訂邀請訊息",
                "connection.connection_message.customize_connection_description": "利用個人化邀請訊息，將提高會員接受您的建立連結邀請",
                "connection.connection_message.customize_connection_cancel": "直接傳送邀請",
                "connection.connection_message.customize_connection_proceed": "自訂訊息",
                "connection.connection_message.customize_connection_input_title": "輸入您的自訂訊息",
                "connection.connection_message.customize_connection_input_placeholder": "例：您好，[用戶名稱]，在 [某活動] 上見到真是太棒了。我們關於 [某話題] 的交談讓我想到了 [某話題] 。真的很喜歡我們的討論，希望能夠進一步交流，繼續保持聯繫。",
                "connection.connection_message.connection_invitation_success": "建立關係邀請已傳送",
                "connection.connection_message.customize_message_cancel": "取消",
                "connection.connection_message.customize_message_send": "傳送",
                "recommendations.title": "推薦",
                "recommendations.action.view_all": "看更多",
                "network_import.title": "建立您的專業人脈網絡",
                "network_import.description": "匯入您的聯絡人，追蹤您認識的人，關注他們的職涯動向。",
                "network_import.google_modal.title": "同意數據資料分享",
                "network_import.google_modal.access_approval": "點擊「繼續」即表示您同意 Cake 存取並使用與您的 Google 帳號相關資料（將安全透過 Google APIs 拿取）",
                "network_import.google_modal.policy_adherence_1": "所有的資料將採取",
                "network_import.google_modal.policy_adherence_2": "， 包含「有限使用（Limited Use）」。",
                "network_import.import_contacts": "從 {{ provider }} 匯入聯絡人",
                "profile_sidebar.current_job_progress": "目前狀態",
                "profile_sidebar.tooltip.completion_ratio": "個人檔案完成度",
                "profile_sidebar.link.profile": "個人檔案",
                "profile_sidebar.link.resume": "履歷",
                "profile_sidebar.link.portfolio": "作品集",
                "profile_sidebar.link.recommendations": "推薦信",
                "profile_sidebar.link.credits": "職場能力評價",
                "profile_sidebar.link.posts": "貼文",
                "profile_sidebar.link.profile_views": "個人檔案瀏覽人次",
                "profile_sidebar.link.profile_views@other": "個人檔案瀏覽人次",
                "profile_sidebar.link.connections": "個聯絡人",
                "profile_sidebar.link.connections@other": "個聯絡人",
                "profile_sidebar.link.mutual_connections": "個共同聯絡人",
                "profile_sidebar.link.mutual_connections@other": "個共同聯絡人",
                "company_sidebar.link.applicants": "應徵者",
                "company_sidebar.link.applicants@other": "應徵者",
                "company_sidebar.link.jobs": "職缺",
                "company_sidebar.link.jobs@other": "職缺",
                "company_sidebar.link.page_views": "公司職缺瀏覽人次",
                "company_sidebar.link.page_views@other": "公司職缺瀏覽人次",
                "company_sidebar.link.analytics": "分析報告",
                "hashtag_info.message.posts": "則貼文",
                "hashtag_info.message.posts@other": "則貼文",
                "hashtag_info.action.follow": "追蹤",
                "hashtag_info.action.following": "追蹤中",
                "popular_hashtags.title": "探索 Hashtags",
                "popular_hashtags.message.posts": "則貼文",
                "popular_hashtags.message.posts@other": "則貼文",
                "dialog.message.confirm": "請輸入「{{ answer }}」以繼續。",
                "dialog.action.cancel": "$t(common:action.cancel)",
                "dialog.action.proceed": "$t(common:action.confirm)",
                "recent_search.title": "近期",
                "recent_search.action.clear_all": "清空",
                "global_search.placeholder": "搜尋",
                "user_card.message.present": "現在",
                "user_card.message.high_completion_ratio": "個人檔案完成度超過 75%",
                "user_card.message.past_experience": "曾任",
                "user_card.message.past_experience_at": "於",
                "user_card.message.past_education": "曾就讀",
                "user_card.message.message_you_sent": "您給 {{ username }} 的自訂訊息",
                "company_card.message.jobs": "職缺",
                "company_card.message.jobs@other": "職缺",
                "useful_links.title": "更多求職和招募資源",
                "useful_links.link.resume_builder": "製作您的履歷",
                "useful_links.link.job_search": "搜尋職缺",
                "useful_links.link.job_posting": "刊登職缺",
                "useful_links.link.talent_search": "搜尋人才",
                "spam_appeal_banner.description": "我們注意到您的個人檔案因檔案內容資訊量不足或品質不佳而被暫時從平台上被隱藏（包括您的個人檔案、履歷以及作品集），我們建議您優化您的個人檔案內容，完善填寫檔案資訊。\n如果您認為我們的判斷有誤，請務必點擊\"提出異議\"，我們將盡快對您的情況進行審核回應。在審核期間，請放心，您的帳戶不會受到任何影響，您仍可以正常使用平台的功能。如果此舉給您帶來不便，我們深感抱歉。",
                "spam_appeal_banner.appeal_success": "已收到您提出的異議，我們將進行審核並儘快回覆",
                "spam_appeal_banner.action.appeal": "提出異議",
                "spam_appeal_banner.action.appealed": "已提出異議",
                "spam_display_confirmation.title": "警告",
                "spam_display_confirmation.description": "我們的系統偵測出您即將訪問的這個頁面可能含有可疑內容。請您在繼續前進之前再次確認。您的網路安全對我們來說至關重要，請慎重考慮。",
                "spam_display_confirmation.action.continue": "繼續訪問",
                "spam_display_confirmation.action.leave": "離開頁面",
                "app_redirect_banner.description": "使用 Cake App，\n在手機版上獲得更好的使用體驗。",
                "app_redirect_banner.open_app": "開啟 App",
                "resume_template_select.action.new_resume": "新增履歷",
                "resume_template_select.action.choose_template": "使用此履歷模板",
                "resume_template_select.action.upgrade_to_use": "升級後即可啟用",
                "confirm_email.title.verify_email": "請認證您的 Email",
                "confirm_email.title.update_email": "更新 email",
                "confirm_email.title.check_code": "請檢查您的信箱，取得認證碼",
                "confirm_email.description.verify_email": "在使用此功能前，需麻煩您先認證您的 email，以幫助我們確保您的帳戶安全。",
                "confirm_email.description.check_code": "請檢查您的信箱，並在此輸入認證信中的認證碼。（認證碼有效期為 10 分鐘）",
                "confirm_email.field.code.label": "認證碼",
                "confirm_email.field.code.placeholder": "請輸入認證碼",
                "confirm_email.action.send_confirmation": "發送 Email 認證信",
                "confirm_email.action.resend_confirmation": "再次發送認證信",
                "confirm_email.action.update_email": "更新 email",
                "confirm_email.footnote.not_your_email": "這不是您的 email？",
                "confirm_email.footnote.did_not_get_email": "沒收到信嗎？",
                "promote_resume_builder.title": "線上履歷工具",
                "promote_resume_builder.description": "製作一份能幫你獲得面試機會的履歷。免費下載 PDF。",
                "promote_resume_builder.action.create_resume": "製作履歷",
                "popular_companies.title": "熱門徵才企業",
                "popular_companies.description": "搜尋數萬筆職缺。薪資公開透明，加速找到適合您的工作。",
                "popular_companies.jobs": "職缺",
                "popular_companies.jobs@one": "職缺",
                "popular_companies.jobs@other": "職缺",
                "popular_companies.action.browse": "搜尋職缺",
                "interview_invitation.title": "選擇你想邀請 {{ name }} 應徵的職缺",
                "interview_invitation.tooltip": "邀請應徵職缺",
                "interview_invitation.message.preview": "預覽訊息",
                "interview_invitation.message.message_default": "{first_name} 您好，\n\n我是 {company} 的 {my_first_name}。我在 Cake 上看到您的履歷後，想邀請您應徵 {job_title} 一職！麻煩您看看職缺描述，若有任何問題歡迎隨時跟我說。謝謝！\n\nRegards,\n{my_first_name}",
                "interview_invitation.message.message_description": "您可以在訊息中加入參數。可使用的參數",
                "interview_invitation.message.remaining_invitations": "這個月您還可以發出 {{ count }} 個邀請",
                "interview_invitation.message.hint_max_invitations": "每月最多發出 {{ count }} 個邀請。",
                "interview_invitation.message.hint_upgrade": "升級以提高上限",
                "interview_invitation.message.no_companies": "沒有可以選擇的公司。",
                "interview_invitation.message.no_jobs": "沒有可以選擇的職缺。",
                "interview_invitation.message.success": "成功送出邀請！",
                "interview_invitation.field.company": "公司",
                "interview_invitation.field.job": "職缺名稱",
                "interview_invitation.field.message": "訊息",
                "interview_invitation.placeholder.company": "請選擇公司",
                "interview_invitation.placeholder.job": "請選擇職缺",
                "interview_invitation.action.post_jobs": "刊登職缺",
                "interview_invitation.action.submit": "發出邀請",
                "auth_modal.title": "輕鬆製作履歷",
                "auth_modal.title@tw": "國際人才社群",
                "auth_modal.slogan.career_pages": "企業方案",
                "auth_modal.slogan.talent_search": "人才搜尋引擎",
                "auth_modal.slogan.job_posting": "職缺刊登",
                "auth_modal.slogan.jobs": "找工作",
                "auth_modal.slogan.writing_for_cakeresume": "成為貢獻者",
                "auth_modal.slogan.portfolio": "免費製作線上作品集",
                "auth_modal.slogan.chat_with": "開啟與 {{ name }} 的對話",
                "auth_modal.message.or": "或",
                "auth_modal.message.forgot_password": "忘記密碼？",
                "auth_modal.message.already_signed_up": "已經註冊過了？",
                "auth_modal.message.not_yet_signed_up": "還沒有帳號嗎？",
                "auth_modal.message.accept": "註冊帳號即表示您同意我們的{{- terms }}以及{{- privacy }}",
                "auth_modal.message.accept_terms": "服務條款",
                "auth_modal.message.accept_privacy": "隱私政策",
                "auth_modal.message.subscribe_newsletter": "我同意訂閱 Cake 電子報",
                "auth_modal.field.remember": "自動登入",
                "auth_modal.placeholder.name": "您的名字",
                "auth_modal.placeholder.email": "電子郵件",
                "auth_modal.placeholder.password": "密碼",
                "auth_modal.oauth_sign_in": "使用 {{ provider }} 帳戶登入",
                "auth_modal.oauth_sign_up": "使用 {{ provider }} 帳戶註冊",
                "verify_code.title": "輸入您的驗證代碼",
                "verify_code.description": "在您行動裝置上打開驗證應用程式，以查看您的驗證代碼。",
                "verify_code.placeholder.code": "輸入代碼",
                "verify_code.note": "如果您無法使用您的驗證應用程式，您可以改用備用代碼。",
                "verify_code.action.submit": "確認",
                "viewed_and_saved_jobs.viewed_jobs": "最近瀏覽",
                "viewed_and_saved_jobs.saved_jobs": "儲存的職缺",
                "image_upload_modal.title": "新增圖片",
                "image_upload_modal.dropzone_hint": "將圖片拖拉到這裡或選擇檔案。",
                "image_upload_modal.delete_existing": "刪除現有圖片",
                "image_upload_modal.confirm_delete": "確定要刪除現有圖片嗎？",
                "image_edit_modal.title": "編輯圖片",
                "image_edit_modal.cropping_option.original": "原始",
                "image_edit_modal.cropping_option.wide": "寬",
                "image_edit_modal.cropping_option.square": "方形",
                "job_search_guest_hint.title": "Cake 找工作",
                "job_search_guest_hint.description": "加入 Cake 社群，搜尋上萬筆職缺，快速找到適合你的工作。",
                "job_search_guest_hint.action.sign_up": "註冊",
                "job_search_guest_hint.action.post_jobs": "刊登職缺",
                "tech_stack_list.title": "公司使用技術",
                "folder_list.title": "近期招募專案",
                "folder_list.empty": "您目前沒有任何專案。",
                "folder_list.action.view_all": "查看全部專案",
                "folder_list.action.create_project": "建立專案",
                "share_to_feed.success.title": "已分享動態",
                "share_to_feed.success.action.view_post": "查看貼文",
                "social_share.select_job_to_share": "選擇要分享的職缺",
                "social_share.facebook_group": "Cake FB 求職社團",
                "social_share.share_on_social_media": "一般社群分享",
                "portfolio_list.empty": "尚沒有任何作品。",
                "searchable_select.no_options": "沒有項目",
                "published_resume_select.title": "選擇已發布的履歷",
                "published_resume_select.empty.title": "您目前沒有 Cake 履歷",
                "published_resume_select.empty.action": "製作一份 Cake 履歷",
                "reusable_pdf_select.title": "選擇已上傳的 PDF",
                "reusable_pdf_select.empty.title": "您目前沒有已上傳的 PDF",
                "file_dropzone.title": "上傳檔案",
                "file_dropzone.description": "將檔案拖到這裡或是點擊此處來上傳檔案。",
                "work_experience_list.none": "我沒有工作經驗",
                "education_list.none": "我沒有學歷",
                "education_list.activities_and_societies": "社團活動",
                "education_list.description": "簡介",
                "certification_list.none": "我沒有證照或執照",
                "certification_list.credential_id": "證照編號",
                "certification_list.issued_at": "發照日期 {{ year_month }}",
                "certification_list.expires_at": "{{ year_month }} 到期",
                "certification_list.no_expiration": "永久有效",
                "fresh_chat.welcome_message": "歡迎發問！",
                "fresh_chat.welcome_sub_message": "Cake 客服團隊將及時提供協助:-)",
                "fresh_chat.chat_title": "你遇到什麼種類的問題呢？",
                "fresh_chat.faq_title": "常見問題集",
                "message_preview.to": "給：",
                "message_preview.action.send": "送出訊息",
                "share_attachment_form.title": "分享至貼文",
                "share_attachment_form.placeholder": "分享您的意見想法⋯⋯",
                "share_attachment_form.action.publish": "發表",
                "broadcast.new_reputation_credit.message": "{{- receiver }} 收到了來自 {{- giver }} 的{{ category }}職場能力評價",
                "dashboard_sidebar.section.my.title": "我的",
                "dashboard_sidebar.section.my.item.profile": "個人檔案",
                "dashboard_sidebar.section.my.item.resumes": "履歷",
                "dashboard_sidebar.section.my.item.portfolio": "作品集",
                "dashboard_sidebar.section.my.item.posts": "貼文",
                "dashboard_sidebar.section.my.item.credits": "職場能力評價",
                "dashboard_sidebar.section.my.item.recommendations": "推薦信",
                "dashboard_sidebar.section.my.item.articles": "文章",
                "dashboard_sidebar.section.my.item.community": "社群",
                "dashboard_sidebar.section.recruitment.title": "招募",
                "dashboard_sidebar.section.recruitment.item.career_pages": "徵才專頁",
                "dashboard_sidebar.section.recruitment.item.folders": "招募專案",
                "dashboard_sidebar.section.recruitment.item.resume_reviews": "履歷評論",
                "dashboard_sidebar.section.network.title": "網路",
                "dashboard_sidebar.section.network.item.connections": "我的人脈",
                "dashboard_sidebar.section.network.item.insights": "洞察報告",
                "dashboard_sidebar.section.job_search.title": "求職",
                "dashboard_sidebar.section.job_search.item.following_companies": "追蹤的企業",
                "dashboard_sidebar.section.job_search.item.my_jobs": "我的職缺",
                "dashboard_sidebar.section.favorite.title": "最愛",
                "dashboard_sidebar.section.favorite.item.resumes": "履歷",
                "dashboard_sidebar.section.favorite.item.portfolios": "作品集",
                "dashboard_sidebar.section.assessment.title": "測驗",
                "dashboard_sidebar.section.assessment.item.coding_tests": "程式測驗",
                "dashboard_sidebar.section.personality_tests.title": "心理測驗",
                "dashboard_sidebar.section.personality_tests.item.game": "你是什麼蛋糕？2.0",
                "dashboard_sidebar.section.personality_tests.item.analysis": "測驗分析儀表板",
                "dashboard_sidebar.action.view_public_profile": "查看公開個人檔案",
                "dashboard_sidebar.badge.ai_check": "Cake AI 檢測",
                "reputation.tooltip.most_received_reputation": "{{ name }} 收到最多的{{ category }} 職場評價",
                "reputation.tooltip.technical_skills": "專業技能 Technical Skills",
                "reputation.tooltip.problem_solving": "問題解決能力 Problem-Solving",
                "reputation.tooltip.adaptability": "變通能力 Adaptability",
                "reputation.tooltip.communication": "溝通能力 Communication",
                "reputation.tooltip.time_management": "時間管理能力 Time Management",
                "reputation.tooltip.teamwork": "團隊合作能力 Teamwork",
                "reputation.tooltip.leadership": "領導力 Leadership",
                "account_type.title": "歡迎使用 Cake！",
                "account_type.description": "請告訴我們將使用 Cake 來做什麼，以便我們優化您的使用體驗。",
                "account_type.modal_title": "請選擇您的使用身份",
                "account_type.action.answer": "選擇角色",
                "project_form.create.title": "新增招募專案",
                "project_form.create.description": "透過招募專管理人選，清楚掌握招募進度。",
                "project_form.update.title": "編輯招募專案",
                "project_form.field.name": "招募專案名稱",
                "project_form.field.description": "說明",
                "project_form.field.disable_notification": "發送通知給被新增至專案的人選",
                "project_form.message.upgrade": "升級方案以關閉通知",
                "project_form.placeholder.name": "例如：Q1 全端工程師招募",
                "project_form.placeholder.description": "招募專案說明",
                "resume_folders_form.title": "將人選 {{ name }} 加入招募專案",
                "resume_folders_form.message.no_projects": "您目前沒有任何專案。",
                "resume_folders_form.message.candidates": "人選",
                "resume_folders_form.message.candidates@one": "人選",
                "resume_folders_form.message.candidates@other": "人選",
                "resume_folders_form.message.updated": "專案更新成功",
                "resume_folders_form.action.create_project": "建立專案",
                "resume_folders_form.action.manage_projects": "管理專案",
                "report_spam.report_submitted": "檢舉完成",
                "report_spam.archive_message": "感謝您檢舉此可疑活動。您希望將訊息封存嗎？",
                "report_spam.detail": "詳細內容",
                "promo_code_applied_hint.description": "您有一張 {{ percentage }}% off 優惠券，將於結帳時計算折扣後金額。",
                "promo_code_applied_hint.description_one_dollar": "您有一張一元購買優惠券，將於結帳時計算折扣後金額。",
                "update_credit_card_alert_banner.declined": "信用卡交易失敗，請於 {{ date }} 前更新信用卡資訊並完成付款，以避免服務中斷。",
                "update_credit_card_alert_banner.declined_and_downgraded": "由於系統無法完成付款，因此已於 {{ date }} 將您降為免費方案。",
                "update_credit_card_alert_banner.action.update": "更新付款資訊",
                "update_credit_card_alert_banner.action.retry_payment": "付款並重啟方案",
                "update_credit_card_alert_banner.close_confirmation_dialog.title": "關閉並不再提醒？",
                "update_credit_card_alert_banner.close_confirmation_dialog.body": "您已於 {{ date }} 降為「免費方案」，如關閉此提醒，您將維持「免費方案」且不再收到付款內容與重啟 {{ plan }} 方案的提醒。",
                "update_credit_card_alert_banner.close_confirmation_dialog.cancel": "取消",
                "update_credit_card_alert_banner.close_confirmation_dialog.confirm": "確認關閉",
                "in_app_browser_hint.sign_in": "為確保順利登入，請使用手機瀏覽器（如 Chrome 或 Safari）開啟此頁面，避免使用 App 內建瀏覽器可能出現的錯誤。",
                "in_app_browser_hint.sign_up": "為確保順利註冊，請使用手機瀏覽器（如 Chrome 或 Safari）開啟此頁面，避免使用 App 內建瀏覽器可能出現的錯誤。",
                "in_app_browser_hint.action.open_browser": "用瀏覽器開啟",
                "job_application_response.employer_with_high_application_response_rate": "履歷讀取率高",
                "job_application_response.highly_responsive_to_job_applicants": "積極審核履歷中",
                "job_recommendations.title": "您可能感興趣的職缺",
                "job_recommendations.action.refresh": "更多推薦職缺",
                "user_recommendations.title": "拓展您的人脈",
                "user_recommendations.reason.education": "根據您的學校經歷",
                "user_recommendations.reason.work_experience": "根據您的工作經歷",
                "user_recommendations.reason.second_degree_connection": "根據您的人脈",
                "user_recommendations.action.refresh": "更多推薦人脈",
                "braze_content_card.announcements_widget.title": "最新消息",
                "braze_content_card.events_widget.title": "最新活動",
                "braze_content_card.resources_widget.title": "精選文章",
                "resume_health_check_button.ai_check": "Cake AI 檢測",
                "resume_health_check_button.view_ai_resume_report": "瀏覽Cake AI履歷健檢報告",
                "resume_health_check_button.tooltip.disabled": "此功能需先發佈履歷才能使用",
                "quota_limit_reached_modal.title": "已達到使用上限",
                "quota_limit_reached_modal.ai_cover_letter_generation.description": "Cake AI 生成求職信使用上限為每 24 小時 10 次",
                "quota_limit_reached_modal.ai_cover_letter_generation.hint": "如有興趣取得更多使用次數，請點擊「我有興趣」",
                "quota_limit_reached_modal.resume_health_check.description": "您已在24小時內達到了10次使用限制。",
                "quota_limit_reached_modal.resume_health_check.hint": "如果您有興趣獲取更多使用次數，請點擊“我有興趣！”",
                "quota_limit_reached_modal.action.interested": "我有興趣",
                "quota_limit_reached_modal.action.not_now": "下次再說",
                "quota_limit_reached_modal.flash_message.interested.title": "感謝您對此功能有興趣",
                "quota_limit_reached_modal.flash_message.interested.description": "Cake 即將推出進階 Cake AI 服務，屆時我們將優先通知您。",
                "generation_content_display.feedback.hows_the_result": "您覺得結果內容如何？",
                "generation_content_display.feedback.what_do_you_think": "您覺得結果內容 {{ current }} 如何？",
                "generation_content_display.feedback.share_feedback": "和 Cake 分享您的喜好",
                "generation_content_display.feedback.thanks_for_submitting": "謝謝您的分享",
                "company_follow_button.follow_options.notify_new_jobs_opening.title": "當公司有新職缺時通知我",
                "company_follow_button.follow_options.notify_new_jobs_opening_related_to_my_profile.title": "當公司有與我相關的新職缺時通知我",
                "company_follow_button.follow_options.notify_new_jobs_opening_related_to_my_profile.description": "為提高職缺推薦的準確性，系統將自動分析您個人檔案中的「專業背景」欄位，請確保該欄位填寫完整",
                "company_follow_button.flash_message.success_title": "已開啟通知",
                "company_follow_button.flash_message.success_description_notify_new_jobs_opening": "當公司發布新職缺時，我們將主動通知您。",
                "company_follow_button.flash_message.success_description_notify_new_jobs_opening_related_to_my_profile": "當公司發布與您專業背景相關的新職缺時，我們將主動通知您，請確保個人檔案填寫完整。"
            },
            "attribute": {
                "language_proficiency.0": "初階",
                "language_proficiency.1": "中階",
                "language_proficiency.2": "進階",
                "language_proficiency.3": "專業",
                "language_proficiency.4": "母語或雙語",
                "salary_type.per_day": "日",
                "salary_type.per_hour": "小時",
                "salary_type.per_month": "月",
                "salary_type.per_year": "年",
                "salary_type.piece_rate_pay": "按件計酬",
                "negotiable_salary": "面議",
                "regular_salary_4k": "經常性薪資達 NT$40,000",
                "regular_salary_4k_tooltip": "經常性薪資包括本薪、按月給付的固定津貼和獎金、按月發放的工作獎金、全勤獎金及其他經常性給與。詳見勞動部網站。",
                "salary_type_unit.per_day": "日薪",
                "salary_type_unit.per_hour": "時薪",
                "salary_type_unit.per_month": "月薪",
                "salary_type_unit.per_year": "年薪",
                "salary_type_unit.piece_rate_pay": "按件計酬",
                "job_type.full_time": "全職",
                "job_type.part_time": "兼職",
                "job_type.intern": "實習生",
                "job_type.internship": "實習生",
                "job_type.contract": "約聘",
                "job_type.freelance": "接案",
                "job_type.temporary": "臨時工",
                "job_type.volunteer": "志願者",
                "seniority_level.entry_level": "初階",
                "seniority_level.mid_senior_level": "中高階",
                "seniority_level.internship_level": "實習",
                "seniority_level.associate": "助理",
                "seniority_level.director": "經理 / 總監",
                "seniority_level.executive": "經營層 (VP, GM, C-Level)",
                "online_status.online": "正在線上",
                "online_status.active": "活躍",
                "online_status.offline": "離線",
                "online_status.active_ago": "{{ period }}前在線上",
                "folder_user_role.owner": "擁有者",
                "folder_user_role.collaborator": "協作者",
                "folder_user_role.viewer": "檢視者",
                "update_days_ago_level.7": "一周內",
                "update_days_ago_level.14": "兩周內",
                "update_days_ago_level.30": "一個月內",
                "update_days_ago_level.60": "兩個月內",
                "update_days_ago_level.90": "三個月內",
                "update_days_ago_level.180": "半年內",
                "update_days_ago_level.365": "一年內",
                "update_days_ago_level.366": "超過一年",
                "employment_status.employed": "就職中",
                "employment_status.unemployed": "待業中",
                "employment_status.studying": "就學中",
                "employment_status.in_military_service": "服兵役中",
                "job_search_progress.not_open_to_opportunities": "目前沒有興趣尋找新的機會",
                "job_search_progress.open_to_opportunities": "目前會考慮了解新的機會",
                "job_search_progress.ready_to_interview": "正在積極求職中",
                "remote.not_interested": "暫不考慮遠端工作",
                "remote.interested": "對遠端工作有興趣",
                "remote.remote_only": "我只想遠端工作",
                "work_experience.description_relevant": "{{ experience }}相關工作經驗",
                "work_experience._one": "小於 1 年",
                "work_experience.one_two": "1 到 2 年",
                "work_experience.two_four": "2 到 4 年",
                "work_experience.four_six": "4 到 6 年",
                "work_experience.six_ten": "6 到 10 年",
                "work_experience.ten_fifteen": "10 到 15 年",
                "work_experience.fifteen_": "15 年以上",
                "freelance.full_time_freelancer": "全職接案者",
                "freelance.part_time_freelancer": "兼職接案者",
                "freelance.not_freelancer": "不提供接案服務",
                "freelance.full_time_freelancer@self": "是，我目前是全職接案者",
                "freelance.part_time_freelancer@self": "是，我利用業餘時間接案",
                "freelance.not_freelancer@self": "否",
                "degree_type.associate_s_degree": "副學士學位",
                "degree_type.bachelor_of_arts_ba": "文學士（BA）",
                "degree_type.bachelor_of_business_administration_bba": "工商管理學士（BBA）",
                "degree_type.bachelor_of_engineering_beng": "工學學士（BEng）",
                "degree_type.bachelor_of_fine_arts_bfa": "美術學士（BFA）",
                "degree_type.bachelor_of_science_bs": "理學士（BS）",
                "degree_type.bachelor_s_degree": "學士學位",
                "degree_type.engineer_s_degree": "工程師學位",
                "degree_type.master_of_arts_ma": "文學碩士（MA）",
                "degree_type.master_of_business_administration_mba": "工商管理碩士（MBA）",
                "degree_type.master_of_fine_arts_mfa": "美術碩士（MFA）",
                "degree_type.master_of_science_ms": "科學碩士（MS）",
                "degree_type.master_s_degree": "碩士學位",
                "degree_type.doctor_of_philosophy_phd": "哲學博士（PhD）",
                "degree_type.doctor_of_medicine_md": "醫學博士（MD）",
                "degree_type.juris_doctor_jd": "法學博士（JD）",
                "degree_type.high_school_diploma": "高中文憑",
                "degree_type.non_degree_program_e_g_coursera_certificate": "非學位課程（例如 Coursera 證書）",
                "degree_type.other": "其他",
                "read_status.read": "已讀",
                "read_status.unread": "未讀",
                "management.description": "我有管理 {{ number }}的經驗",
                "management.none": "無",
                "management.one_five": "1～5 人",
                "management.five_ten": "5～10 人",
                "management.ten_fifteen": "10～15 人",
                "management.fifteen_": "15 人以上",
                "management.not_specified": "未定",
                "work_exp_year.no_requirement": "不限年資",
                "work_exp_year.n_years_required": "需具備 {{ n }} 年以上工作經驗",
                "year_of_seniority.0_1": "小於 1 年",
                "year_of_seniority.1_3": "1〜3 年",
                "year_of_seniority.3_5": "3〜5 年",
                "year_of_seniority.5_10": "5〜10 年",
                "year_of_seniority.10_": "10 年以上",
                "number_of_employees.1_10": "1〜10人",
                "number_of_employees.11_50": "11〜50人",
                "number_of_employees.51_200": "51〜200人",
                "number_of_employees.201_500": "201〜500人",
                "number_of_employees.501_1000": "501〜1,000人",
                "number_of_employees.1001_5000": "1,001〜5,000人",
                "number_of_employees.5001_": "5,000人以上",
                "education_level.less_than_high_school": "高中職以下",
                "education_level.high_school": "高中職",
                "education_level.associate": "專科",
                "education_level.bachelor": "大學",
                "education_level.master": "碩士",
                "education_level.doctoral": "博士",
                "human_number.0": "",
                "human_number.3": "千",
                "human_number.4": "萬",
                "human_number.8": "億",
                "human_number.12": "兆",
                "short_number.0": "",
                "short_number.4": "萬",
                "tech_usage_level.low": "很少",
                "tech_usage_level.medium": "偶爾",
                "tech_usage_level.high": "常常",
                "number_of_management.not_specified": "管理人數未定",
                "number_of_management.none": "不需負擔管理責任",
                "number_of_management.one_five": "管理 1 ~ 5 人",
                "number_of_management.five_ten": "管理 5 ~ 10 人",
                "number_of_management.ten_fifteen": "管理 10 ~ 15 人",
                "number_of_management.fifteen_": "管理 15 人以上",
                "reputation_category.technical_skills": "專業技能",
                "reputation_category.problem_solving": "問題解決能力",
                "reputation_category.adaptability": "變通能力",
                "reputation_category.communication": "溝通能力",
                "reputation_category.time_management": "時間管理能力",
                "reputation_category.teamwork": "團隊合作能力",
                "reputation_category.leadership": "領導力",
                "network_identity_type.employer": "企業雇主",
                "network_identity_type.headhunter": "獵頭",
                "network_identity_type.job_seeker": "求職者",
                "network_identity_type.mentor": "職場前輩",
                "network_identity_type.professional": "專業領域人士",
                "network_identity_type.intern": "實習生",
                "network_identity_type.freelancer": "自由工作者",
                "network_identity_type.entrepreneur": "創業家",
                "network_identity_type.kol": "網紅 / KOL",
                "profile_privacy_level.public": "任何網路上的人都可以找到並檢視。無須登入。",
                "profile_privacy_level.logged_in_users_can_view": "僅限登入的 Cake 使用者可檢視。",
                "post_privacy.anyone": "公開",
                "plan_period.annual": "年",
                "plan_period.monthly": "月",
                "plan_period.quarterly": "季",
                "privacy_level.level_0_title": "公開",
                "privacy_level.level_0_description": "任何網路上的人都可以找到並檢視（包含 Google 等搜尋引擎也可能找得到）。無須登入。",
                "privacy_level.level_0_tooltip": "公開的履歷就像您的個人網站，幫助您打造線上個人品牌。",
                "privacy_level.level_1_title": "任何有連結的人及平台上的雇主",
                "privacy_level.level_1_description": "任何有連結的（無須登入）、以及 Cake 平台上的雇主都可以檢視。",
                "privacy_level.level_2_title": "僅限有連結的人",
                "privacy_level.level_2_description": "任何有連結的都可以檢視。無須登入。",
                "article_category_group_v2.job_search_guide": "求職指南",
                "article_category_group_v2.career_guidance": "職涯發展",
                "article_category_group_v2.business_excellence": "雇主人資",
                "article_category_group_v2.success_stories": "人物／企業專訪",
                "article_category_group_v2.about_cakeresume": "Cake 特輯",
                "article_category_v2.resume": "履歷",
                "article_category_v2.cover_letter": "求職信",
                "article_category_v2.portfolio": "作品集 ＆ 個人品牌",
                "article_category_v2.interview_guide": "面試技巧",
                "article_category_v2.job_searching_guide": "求職新知",
                "article_category_v2.industry_job_overview": "產業 ＆ 職位介紹",
                "article_category_v2.career_planning": "職涯規劃",
                "article_category_v2.career_tool": "職涯工具模板",
                "article_category_v2.networking": "職場人際溝通",
                "article_category_v2.management": "職場管理學",
                "article_category_v2.people_operations": "人資營運",
                "article_category_v2.recruitment": "人資招募",
                "article_category_v2.success_stories": "人物／企業專訪",
                "article_category_v2.cakeresume_people_culture": "團隊與企業文化",
                "article_category_v2.cakeresume_news_updates": "最新消息",
                "article_category_v2.cakeresume_events": "活動分享",
                "inclusivity_trait.foreign_talents.title": "外籍人才",
                "inclusivity_trait.foreign_talents.description": "非公民及持簽證者。",
                "inclusivity_trait.career_change.title": "友善轉職人士",
                "inclusivity_trait.career_change.description": "剛踏入新產業的求職者。",
                "inclusivity_trait.back_to_work.title": "重返職場",
                "inclusivity_trait.back_to_work.description": "職涯暫停後重返工作的人士。",
                "inclusivity_trait.disability_confident.title": "友善包容身心障礙者",
                "inclusivity_trait.disability_confident.description": "身心障礙者。",
                "inclusivity_trait.lgbtq.title": "LGBTQ+ 群體",
                "inclusivity_trait.lgbtq.description": "LGBTQ+ 個人。",
                "inclusivity_trait.gender_equity.title": "性別平等",
                "inclusivity_trait.gender_equity.description": "所有性別都有平等機會。",
                "inclusivity_trait.senior_citizen.title": "年長者",
                "inclusivity_trait.senior_citizen.description": "符合社會或法定退休年齡的年長者。",
                "work_title.business_owner": "企業主",
                "work_title.executive": "高層主管（C-level、VP、總監）",
                "work_title.hiring_manager_or_team_leader": "招聘經理/團隊主管",
                "work_title.team_member": "團隊成員",
                "work_title.recruiter": "招募專員",
                "work_title.freelancer": "自由工作者",
                "page_admin_role.owner.name": "擁有者",
                "page_admin_role.owner.description": "擁有者",
                "page_admin_role.admin.name": "超級管理員",
                "page_admin_role.admin.description": "可管理公司頁面、成員權限、可新增職缺、管理職缺，管理應徵者",
                "page_admin_role.job_and_application_manager.name": "職缺管理員",
                "page_admin_role.job_and_application_manager.description": "可新增職缺、管理職缺，管理應徵者",
                "page_admin_role.application_manager.name": "指定職缺管理員",
                "page_admin_role.application_manager.description": "可管理被指派的職缺，和這些職缺的應徵者",
                "page_admin_role.member.name": "一般成員",
                "page_admin_role.member.description": "無法管理職缺或應徵者"
            },
            "sector": {
                "sector_groups.advertising-marketing-agency": "廣告 / 行銷 / 代理",
                "sector_groups.agriculture": "農林漁牧業",
                "sector_groups.architecture": "建築設計",
                "sector_groups.banking-insurance-finance": "銀行 / 保險 / 金融",
                "sector_groups.consulting-audit": "顧問 / 審計",
                "sector_groups.corporate-services": "公司服務",
                "sector_groups.culture-media-entertainment": "文化 / 媒體 / 娛樂",
                "sector_groups.design-art": "設計 / 藝術",
                "sector_groups.distribution": "分銷",
                "sector_groups.education-training-recruitment": "教育 / 培訓 / 招聘",
                "sector_groups.fashion-luxury-beauty-lifestyle": "時尚 / 奢華 / 美膚 / 生活方式",
                "sector_groups.food-and-beverage": "食品和飲料",
                "sector_groups.health-social-environment": "健康 / 社會 / 環境",
                "sector_groups.hotel-tourism-leisure": "飯店 / 旅遊 / 休閒",
                "sector_groups.industry": "工業",
                "sector_groups.legal-law": "法律 / 法規",
                "sector_groups.medical": "生醫 / 醫療",
                "sector_groups.mobility-transport": "移動 / 運輸",
                "sector_groups.non-profit-association": "非營利 / 社團組織",
                "sector_groups.public-administration": "公共行政",
                "sector_groups.real-estate": "房地產",
                "sector_groups.service-industry": "服務",
                "sector_groups.tech": "科技",
                "sectors.advertising-marketing-agency_adtech-martech": "廣告技術 / 行銷技術",
                "sectors.advertising-marketing-agency_advertising": "廣告",
                "sectors.advertising-marketing-agency_design": "設計",
                "sectors.advertising-marketing-agency_digital": "數位",
                "sectors.advertising-marketing-agency_event-management": "事件管理",
                "sectors.advertising-marketing-agency_marketing-communications": "行銷 / 溝通",
                "sectors.advertising-marketing-agency_public-relations": "公共關係",
                "sectors.agriculture_agricultural-technology": "農業科技",
                "sectors.agriculture_dairy": "乳製品 / 酪農",
                "sectors.agriculture_farming": "農業",
                "sectors.agriculture_fishery": "漁業",
                "sectors.architecture_architecture": "建築設計",
                "sectors.architecture_interior-design": "室內設計",
                "sectors.banking-insurance-finance_banking": "銀行",
                "sectors.banking-insurance-finance_finance": "財務",
                "sectors.banking-insurance-finance_fintech-insurtech": "財務技術 / 保險技術",
                "sectors.banking-insurance-finance_insurance": "保險",
                "sectors.banking-insurance-finance_investment-banking": "投資銀行",
                "sectors.banking-insurance-finance_investment-management": "投資管理",
                "sectors.banking-insurance-finance_venture-capital-private-equity": "風險投資與私募股權",
                "sectors.consulting-audit_accounting": "會計",
                "sectors.consulting-audit_audit": "審計",
                "sectors.consulting-audit_change-management": "變更管理",
                "sectors.consulting-audit_design-engineering-office": "設計與工程辦公室",
                "sectors.consulting-audit_digital-marketing-data-marketing": "數位行銷 / 數據行銷",
                "sectors.consulting-audit_events-services": "活動服務",
                "sectors.consulting-audit_it-digital": "IT / 數位",
                "sectors.consulting-audit_organization-management": "組織 / 管理",
                "sectors.consulting-audit_strategy": "策略",
                "sectors.consulting-audit_supply-chain": "供應鏈",
                "sectors.consulting-audit_transaction-services": "交易服務",
                "sectors.corporate-services_corporate-concierge-services": "公司禮賓服務",
                "sectors.corporate-services_corporate-support": "企業支持",
                "sectors.corporate-services_coworking": "協調工作",
                "sectors.corporate-services_incubator-accelerator": "孵化器 / 加速器",
                "sectors.corporate-services_recruitment-services-worker-dispatch": "人力代招 / 派遣",
                "sectors.culture-media-entertainment_film": "電影",
                "sectors.culture-media-entertainment_media": "媒體",
                "sectors.culture-media-entertainment_museums-cultural-institutions": "博物館 / 文化機構",
                "sectors.culture-media-entertainment_music": "音樂",
                "sectors.culture-media-entertainment_print-media": "印刷媒體",
                "sectors.culture-media-entertainment_publishing": "出版",
                "sectors.culture-media-entertainment_sports": "運動",
                "sectors.culture-media-entertainment_television-film-production": "影視製作",
                "sectors.culture-media-entertainment_theater": "劇場",
                "sectors.culture-media-entertainment_video-games": "電子遊戲",
                "sectors.design-art_design": "設計",
                "sectors.design-art_fine-art": "藝術 / 美術",
                "sectors.design-art_graphic-design": "平面設計",
                "sectors.design-art_performing-arts": "表演藝術",
                "sectors.design-art_photography": "攝影",
                "sectors.distribution_e-commerce": "電子商務",
                "sectors.distribution_mass-distribution": "大規模經銷",
                "sectors.distribution_retail": "零售",
                "sectors.distribution_selective-distribution": "選擇性經銷",
                "sectors.distribution_wholesale": "批發",
                "sectors.education-training-recruitment_edtech": "教育科技",
                "sectors.education-training-recruitment_education": "教育",
                "sectors.education-training-recruitment_human-resources": "人力資源",
                "sectors.education-training-recruitment_job-training": "工作培訓",
                "sectors.education-training-recruitment_recruitment": "招聘",
                "sectors.fashion-luxury-beauty-lifestyle_cosmetics": "美妝品",
                "sectors.fashion-luxury-beauty-lifestyle_fashion": "時尚",
                "sectors.fashion-luxury-beauty-lifestyle_jewelry": "珠寶",
                "sectors.fashion-luxury-beauty-lifestyle_lifestyle": "生活方式",
                "sectors.fashion-luxury-beauty-lifestyle_luxury": "奢侈品",
                "sectors.food-and-beverage_beverage": "飲品",
                "sectors.food-and-beverage_consumer-goods": "消費品",
                "sectors.food-and-beverage_food-craft": "食品工藝",
                "sectors.food-and-beverage_food-production": "食品生產",
                "sectors.food-and-beverage_foodservice": "餐飲服務",
                "sectors.food-and-beverage_foodtech": "食物科技",
                "sectors.food-and-beverage_gourmet-grocery": "美食雜貨店",
                "sectors.health-social-environment_collaborative-economy": "合作經濟",
                "sectors.health-social-environment_environment-sustainable-development": "環境 / 永續發展",
                "sectors.health-social-environment_health": "健康",
                "sectors.health-social-environment_home-care-services": "家庭護理服務",
                "sectors.health-social-environment_socialtech-greentech": "社交科技 / 綠色科技",
                "sectors.hotel-tourism-leisure_hotel": "飯店",
                "sectors.hotel-tourism-leisure_leisure": "休閒",
                "sectors.hotel-tourism-leisure_tourism": "旅行",
                "sectors.industry_aeronautics-space": "航空 / 航太",
                "sectors.industry_agri-food-animal-nutrition": "農業食品 / 動物營養",
                "sectors.industry_automotive": "汽車",
                "sectors.industry_building-public-works": "建築 / 營造 / 公共工程",
                "sectors.industry_chemicals": "化學品",
                "sectors.industry_electronics-telecommunications": "電子 / 電信",
                "sectors.industry_energy": "能源",
                "sectors.industry_furniture": "家具",
                "sectors.industry_manufacturing": "製造",
                "sectors.industry_material-science": "材料科學",
                "sectors.industry_mechanical-or-industrial-engineering": "機械或工業工程",
                "sectors.industry_metallurg": "冶金",
                "sectors.industry_mining-metals": "礦業金屬",
                "sectors.industry_nanotechnology": "奈米科技",
                "sectors.industry_pharmaceutical-biotech": "製藥 / 生物科技",
                "sectors.industry_rail": "鐵軌",
                "sectors.legal-law_law": "法律",
                "sectors.legal-law_legal-department": "法務部門",
                "sectors.medical_medical-devices": "醫療器材",
                "sectors.medical_medical-practice": "醫療實踐",
                "sectors.medical_mental-health-care": "心理保健",
                "sectors.mobility-transport_logistics": "物流",
                "sectors.mobility-transport_mobility": "移動",
                "sectors.mobility-transport_shipping-and-ground-transport": "海運和陸運",
                "sectors.non-profit-association_foundation": "基金會",
                "sectors.non-profit-association_ngo": "非政府組織",
                "sectors.non-profit-association_nonprofit": "非營利",
                "sectors.public-administration_public-administration": "公共行政",
                "sectors.public-administration_public-and-local-agencies": "公共和地方機構",
                "sectors.public-administration_unions-and-labor-organisation": "工會和勞工組織",
                "sectors.real-estate_commercial-real-estate": "商業地產",
                "sectors.real-estate_residential-real-estate": "住宅地產",
                "sectors.service-industry_car-maintenance-services-scooter-and-motorcycle-maintenance-services": "汽機車維修 / 汽機車服務",
                "sectors.service-industry_hairdressing-and-beauty-services": "美容美髮",
                "sectors.service-industry_housekeeping-services": "家事服務",
                "sectors.service-industry_pet-care-services": "寵物服務",
                "sectors.service-industry_private-detective-services": "徵信",
                "sectors.service-industry_rental-and-leasing-services": "租賃",
                "sectors.service-industry_security-services": "保全",
                "sectors.tech_ar-vr": "擴增實境 / 虛擬實境 (AR/VR)",
                "sectors.tech_artificial-intelligence-machine-learning": "人工智慧 / 機器學習",
                "sectors.tech_automatic-control": "自動控制",
                "sectors.tech_big-data": "大數據",
                "sectors.tech_blockchain": "區塊鏈",
                "sectors.tech_computer-networking": "電腦網路",
                "sectors.tech_connected-objects": "物聯網／IoT",
                "sectors.tech_consumer-electronics": "消費性電子產品",
                "sectors.tech_cyber-security": "網路安全",
                "sectors.tech_ecommerce": "電子商務 (EC)",
                "sectors.tech_gambling-casinos": "博弈 / 賭場",
                "sectors.tech_games": "遊戲",
                "sectors.tech_hardware": "硬體",
                "sectors.tech_information-services": "資訊服務",
                "sectors.tech_internet": "網際網路",
                "sectors.tech_mobile-apps": "手機應用程式",
                "sectors.tech_robotics": "機器人科學",
                "sectors.tech_saas-cloud-services": "軟體即服務 / 雲服務",
                "sectors.tech_semiconductor": "半導體",
                "sectors.tech_software": "軟體"
            }
        },
        "en": {
            "404": {
                "title": "Page not found",
                "description": "We can’t find the page you’re looking for. Try going back to the previous page or see our {{- help_center_link }} for more information.",
                "description_help_center": "Help Center"
            },
            "common": {
                "language.en": "English",
                "language.es": "Español",
                "language.fr": "Français",
                "language.id": "Bahasa Indonesia",
                "language.ja": "日本語",
                "language.ko": "한국어",
                "language.vi": "Tiếng Việt",
                "language.zh-CN": "中文（简体）",
                "language.zh-TW": "中文（繁體）",
                "language_short.en": "EN",
                "language_short.es": "ES",
                "language_short.fr": "FR",
                "language_short.id": "ID",
                "language_short.ja": "日語",
                "language_short.ko": "KO",
                "language_short.vi": "VI",
                "language_short.zh-CN": "简中",
                "language_short.zh-TW": "繁中",
                "symbol.glue": " ",
                "symbol.separator": ", ",
                "symbol.comma": ", ",
                "symbol.hyphen": "-",
                "symbol.bracket": "[{{ text }}]",
                "time_format.time": "h:mmaaa",
                "time_format.weekday_time": "ccc h:mmaaa",
                "time_format.weekday": "ccc",
                "time_format.month_day_time": "MMM do h:mmaaa",
                "time_format.month_day": "MMM do",
                "time_format.year_day_time": "y MMM do h:mmaaa",
                "time_format.year_day": "MMM do y",
                "time_format.year_month": "MMM y",
                "time_format.year_day_numeric": "yyyy-MM-dd",
                "time_format.year": "y",
                "time_format.present": "Present",
                "time_format.n_year": "yrs",
                "time_format.n_year@one": "yr",
                "time_format.n_year@other": "yrs",
                "time_format.n_month": "mos",
                "time_format.n_month@one": "mo",
                "time_format.n_month@other": "mos",
                "terminology.job": "Jobs",
                "terminology.company": "Companies",
                "terminology.resume_builder": "Resume Builder",
                "terminology.resume_builder_plans": "Resume Builder Plans",
                "terminology.job_posting": "Job Posting",
                "terminology.job_posting_plans": "Job Posting Plans",
                "terminology.talent_search": "Talent Search",
                "terminology.talent_search_plans": "Talent Search Plans",
                "terminology.help_center": "Help Center",
                "terminology.press_kit": "Press Kit",
                "terminology.terms_of_service": "Terms of Service",
                "terminology.privacy_policy": "Privacy Policy",
                "terminology.app_job_search": "Cake Job Search",
                "terminology.eor_service": "Employer of Record (EOR)",
                "terminology.employer_branding": "Employer Branding",
                "terminology.employer_branding_ebook": "Employer Branding Ebook",
                "terminology.reputation_credits": "Reputation Credits",
                "terminology.credit": "Credits",
                "terminology.credit@one": "Credit",
                "terminology.credit@other": "Credits",
                "status.following": "Following",
                "status.saved": "Saved",
                "status.done": "Done",
                "status.error": "Error",
                "action.download_apps": "Download our App",
                "action.download_app_store": "Download on the App Store",
                "action.download_google_play": "Get It On Google Play",
                "action.sign_in": "Log In",
                "action.sign_out": "Sign Out",
                "action.sign_up": "Sign Up",
                "action.load_more": "Load more",
                "action.add": "Add",
                "action.create": "Create",
                "action.update": "Update",
                "action.save": "Save",
                "action.delete": "Delete",
                "action.cancel": "Cancel",
                "action.import": "Import",
                "action.edit": "Edit",
                "action.confirm": "Confirm",
                "action.previous": "Previous",
                "action.next": "Next",
                "action.previous_step": "Previous",
                "action.next_step": "Next",
                "action.more": "See more",
                "action.follow": "Follow",
                "action.unfollow": "Unfollow",
                "action.select": "Select",
                "action.select_all": "Select all",
                "action.back": "Back",
                "action.continue": "Continue",
                "action.done": "Done",
                "action.skip": "Skip",
                "action.close": "Close",
                "action.search": "Search",
                "action.share": "Share",
                "action.print": "Print",
                "action.publish": "Publish",
                "action.copy": "Copy",
                "action.view_on_dashboard": "View on dashboard",
                "action.back_to_dashboard": "Back to dashboard",
                "action.talk_to_our_consultants": "Talk to Our Consultants",
                "action.reset": "Reset",
                "action.view_all": "View all",
                "action.view_more": "View More",
                "action.learn_more": "Learn more",
                "action.read_more": "Read more",
                "action.show_more": "Show more",
                "action.show_less": "Show less",
                "action.subscribe": "Subscribe",
                "action.unsubscribe": "Unsubscribe",
                "action.upload_image": "Upload Image",
                "action.report": "Report",
                "action.submit": "Submit",
                "action.view": "View",
                "action.pay": "Pay",
                "action.go_complete_profile": "Go to complete my profile",
                "action.looks_good": "Looks Good",
                "action.try_now": "Try Now",
                "action.start_now": "Start Now",
                "action.sign_up_now": "Sign Up Now",
                "message.others": "Others",
                "message.coming_soon": "Coming Soon",
                "message.category": "Category",
                "message.divider_or": "or",
                "message.state_on": "On",
                "message.state_off": "Off",
                "message.period_ago": "{{ period }} ago",
                "message.n_selected": "{{ count }} selected",
                "message.info": "Info",
                "message.success": "Success",
                "message.error": "Error",
                "message.copied": "Copied",
                "message.updated": "Updated",
                "message.updating": "Updating...",
                "message.updated_at": "Last updated on {{ date }}",
                "message.uploaded_at": "Uploaded on {{ date }}",
                "message.published_at": "Published: {{ date }}",
                "message.loading": "Loading...",
                "message.no_results": "No matches found.",
                "message.no_content": "No content as of now.",
                "message.confirm_delete": "Are you sure to delete this item?",
                "message.share_to": "Share to",
                "message.new": "New",
                "message.by": "By",
                "message.talent_network_mobile_only": "Meet is currently exclusive to Cake App. For more details, please visit the app.",
                "message.report_success": "Report submitted",
                "message.report_success_description": "We have received your feedback. Thank you for reporting this suspicious activity.",
                "message.report_request": "Help us understand what's happening",
                "message.powered_by_ai": "Powered by Cake AI",
                "placeholder.select": "Please select",
                "placeholder.file": "Choose file",
                "placeholder.describe": "Please describe",
                "header_v2.find_jobs.title": "Jobs",
                "header_v2.find_jobs.job_search.title": "Job Search",
                "header_v2.find_jobs.job_search.subtitle": "Explore all available job openings across industries and locations.",
                "header_v2.find_jobs.company_search.title": "Company Search",
                "header_v2.find_jobs.company_search.subtitle": "Find your dream jobs categorized by company names.",
                "header_v2.find_jobs.themed_jobs.title": "Themed Jobs",
                "header_v2.find_jobs.themed_jobs.subtitle": "Discover job opportunities organized by specific themes or industries.",
                "header_v2.tools.title": "Tools",
                "header_v2.tools.resume.title": "Resume",
                "header_v2.tools.resume.subtitle": "Create your job-winning resume using our free resume builder.",
                "header_v2.tools.resume_builder.title": "Resume Builder",
                "header_v2.tools.resume_builder.subtitle": "Make a resume for free.",
                "header_v2.tools.resume_templates.title": "Resume Templates",
                "header_v2.tools.resume_templates.subtitle": "Access our extensive library of professional & ready-to-use templates.",
                "header_v2.tools.resume_examples.title": "Resume Examples",
                "header_v2.tools.resume_examples.title@cv": "CV Examples",
                "header_v2.tools.resume_examples.subtitle": "Get inspired by real resume examples to create your own.",
                "header_v2.tools.resume_examples.subtitle@cv": "Get inspired by real CV examples to create your own.",
                "header_v2.tools.occupation_guide.title": "Occupation Guide",
                "header_v2.tools.occupation_guide.subtitle": "Access resume writing guides tailored for different professions.",
                "header_v2.tools.resume_help.title": "Resume Help",
                "header_v2.tools.resume_help.subtitle": "Get expert advice on all things resume from our team of recruitment specialists.",
                "header_v2.tools.ai_resume_health_check.title": "Cake AI Resume Checker",
                "header_v2.tools.ai_resume_health_check.subtitle": "Analyzes your resume using Cake AI for detailed feedback.",
                "header_v2.tools.portfolio.title": "Portfolio",
                "header_v2.tools.portfolio.subtitle": "Showcase your skills and projects with a professional portfolio.",
                "header_v2.tools.portfolio_maker.title": "Portfolio Maker",
                "header_v2.tools.portfolio_maker.subtitle": "Create a professional portfolio to highlight your skills and projects.",
                "header_v2.tools.portfolio_gallery.title": "Portfolio Gallery",
                "header_v2.tools.portfolio_gallery.subtitle": "Browse through our collection of real portfolios for inspiration and networking.",
                "header_v2.tools.cake_ai.title": "Cake AI",
                "header_v2.tools.cake_ai.subtitle": "Utilize our Cake AI-powered products to accelerate job search and talent match.",
                "header_v2.tools.wcru_pro.title": "What Cake R U? Pro",
                "header_v2.tools.wcru_pro.subtitle": "Unlock your workplace personality in 3 minutes and discover your unique strengths!",
                "header_v2.resources.title": "Resources",
                "header_v2.resources.articles.title": "Articles",
                "header_v2.resources.articles.subtitle": "Read insightful articles on career development, job search strategies, and more.",
                "header_v2.resources.articles.view_all": "View All Articles",
                "header_v2.resources.white_paper.title": "White Paper",
                "header_v2.resources.featured_article.heading": "Featured Reads",
                "header_v2.resources.featured_article.read_more": "Read More",
                "header_v2.resources.talent_connect.title": "Talent Connect Podcast",
                "header_v2.resources.talent_connect.subtitle": "We invite talents in different fields to share their interesting career stories.",
                "header_v2.resources.career_adventure.title": "Career Adventure Podcast",
                "header_v2.resources.career_adventure.subtitle": "We inspire young professionals by showcasing diverse career journeys.",
                "header_v2.resources.podcast_apple": "Apple Podcasts",
                "header_v2.resources.podcast_google": "Google Podcasts",
                "header_v2.resources.podcast_spotify": "Spotify",
                "header_v2.hire_v2.title": "Hire",
                "header_v2.hire_v2.cake_for_employers.title": "Welcome to Cake for Employers",
                "header_v2.hire_v2.cake_for_employers.subtitle": "Consolidating Talent Search, Job Posting, and ATS to provide you with a seamless, unified hiring experience.",
                "header_v2.hire_v2.recruitment_software": "Recruitment Software",
                "header_v2.hire_v2.recruitment_solutions": "Recruitment Solutions",
                "header_v2.hire_v2.pricing_and_proof": "Pricing & proof",
                "header_v2.hire_v2.feature.sourcing_talent_search": "Sourcing Talent Search",
                "header_v2.hire_v2.feature.job_posting": "$t(terminology.job_posting)",
                "header_v2.hire_v2.feature.ats": "ATS",
                "header_v2.hire_v2.feature.recruitment_service": "Recruitment Consulting",
                "header_v2.hire_v2.feature.employer_branding": "$t(terminology.employer_branding)",
                "header_v2.hire_v2.feature.eor_service": "$t(terminology.eor_service)",
                "header_v2.hire_v2.feature.pricing_plans": "Cake Pricing Plans",
                "header_v2.hire_v2.feature.success_stories": "Success Stories",
                "header_v2.pricing.title": "Pricing",
                "header_v2.pricing.job_posting_plans": "$t(terminology.job_posting_plans)",
                "header_v2.pricing.talent_search_plans": "$t(terminology.talent_search_plans)",
                "header_v2.pricing.resume_builder_plans": "$t(terminology.resume_builder_plans)",
                "header_v2.meet.title": "Join the Growing \"Cake Meet\" Network With Over 5K Users!",
                "header_v2.meet.description": "With a simple swipe left or right, meet professionals from various fields and expand your career network!",
                "header_v2.meet.cta": "Learn More",
                "header_v2.network.title": "Build your Network",
                "header_v2.network.my_network.title": "My Network",
                "header_v2.network.my_network.subtitle": "Access your personal network connections and manage your contacts.",
                "header_v2.network.meet.title": "Cake Meet",
                "header_v2.network.meet.subtitle": "Expand your professional network by meeting and connecting with other users.",
                "header_v2.user.dashboard": "Dashboard",
                "header_v2.user.my_resumes": "My Resumes",
                "header_v2.user.my_portfolio": "My Portfolio",
                "header_v2.user.settings": "Settings",
                "header_v2.user.plan_and_billing": "Plan Details & Billing",
                "header_v2.user.free_upgrade": "Free Upgrade",
                "header_v2.user.referral": "Invite a Friend",
                "header_v2.user.account_and_billing": "Account & Billing",
                "header_v2.user.notification_settings": "Notification Settings",
                "header_v2.user.help_center": "$t(terminology.help_center)",
                "header_v2.user.upgrade": "Upgrade",
                "header_v2.user.page.applications": "Applications",
                "header_v2.user.page.analytics": "Analytics",
                "header_v2.user.page.create_company": "Create Company",
                "header_v2.user.page.view_all_companies": "View All Companies",
                "header_v2.user.page.join_request_sent": "Request Sent",
                "header_v2.user.page.invitation_to_be_role": "Invited to be {{ role, en_indefinite_article }} {{ role }}",
                "footer.company.title": "Company",
                "footer.company.about": "About",
                "footer.company.resume_builder_plans": "$t(terminology.resume_builder_plans)",
                "footer.company.job_posting_plans": "$t(terminology.job_posting_plans)",
                "footer.company.talent_search_plans": "$t(terminology.talent_search_plans)",
                "footer.company.press_kit": "$t(terminology.press_kit)",
                "footer.company.hiring": "We’re Hiring",
                "footer.company.contact_us": "Contact Us",
                "footer.company.terms_of_service": "$t(terminology.terms_of_service)",
                "footer.company.privacy_policy": "$t(terminology.privacy_policy)",
                "footer.solutions.title": "Solutions",
                "footer.solutions.resume_builder": "$t(terminology.resume_builder)",
                "footer.solutions.portfolio": "Portfolio Maker",
                "footer.solutions.job_search": "Job Search",
                "footer.solutions.cake_ai": "Cake AI",
                "footer.solutions.job_posting": "Job Posting – Get Started for Free",
                "footer.solutions.talent_search": "$t(terminology.talent_search)",
                "footer.solutions.recruitment_service": "Recruitment Consulting",
                "footer.solutions.eor_service": "$t(terminology.eor_service)",
                "footer.solutions.employer_branding": "$t(terminology.employer_branding)",
                "footer.job_search.title": "Job Search",
                "footer.job_search.categories": "Job Categories",
                "footer.job_search.remote": "Remote Work",
                "footer.job_search.internship": "Internship",
                "footer.job_search.part_time": "Part-Time Jobs",
                "footer.resources.title": "Resources",
                "footer.resources.articles": "Articles",
                "footer.resources.ebooks": "Free eBook",
                "footer.resources.resume_examples": "Resume Examples",
                "footer.resources.resume_templates": "Resume Templates",
                "footer.resources.help_center": "$t(terminology.help_center)",
                "footer.resources.employer_branding_ebook": "$t(terminology.employer_branding_ebook)",
                "footer.languages.title": "Languages",
                "footer.follow_us.title": "Follow Us",
                "footer.follow_us.description": "Follow us on social media to get the latest information about resume writing, job hunting, and recruitment!",
                "footer.app_job_search.title": "$t(terminology.app_job_search)",
                "category.target_reader.job_seekers": "Job seekers",
                "category.target_reader.recruiters": "Companies",
                "category.article.all": "All",
                "category.article.resume": "Resume, Cover Letter Tutorials",
                "category.article.job_searching_guide": "Job Search Tips",
                "category.article.job_interview": "Interview Skills",
                "category.article.career": "Career Development",
                "category.article.job_searching_resource": "Job Search Channels",
                "category.article.company_interview": "People or Company Interviews",
                "category.article.recruitment_guides": "Hiring Tips",
                "category.article.recruitment_partnership": "Work with Recruitment Agency",
                "category.resume_example.administrative": "Administrative",
                "category.resume_example.design": "Design",
                "category.resume_example.engineering": "Engineering",
                "category.resume_example.finance": "Finance",
                "category.resume_example.management": "Management",
                "category.resume_example.media": "Media",
                "category.resume_example.sales": "Sales",
                "category.resume_example.service": "Service Industry",
                "category.resume_example.technology": "Technology",
                "meta.description": "Build a professional resume to kick start your job search. Free resume builder/CV maker, resume templates and resume samples, job-search tool, career blog, hire & recruitment solutions. Millions of job seekers’ top choice.",
                "meta.image": "common/cover-default-en-v3.png",
                "error_401.title": "You need permission",
                "flash_message.global.premium_snippets_hint": "Your profile is complete. Congratulations on receiving your free premium resume snippets! Enter the resume editor to get started.",
                "flash_message.job_saved.follow_company_hint": "Follow companies to receive instant job notifications",
                "validation.mixed.default": "{{ path }} is invalid",
                "validation.mixed.required": "{{ path }} is a required field",
                "validation.mixed.one_of": "{{ path }} must be one of the following values: {{ values }}",
                "validation.mixed.not_one_of": "{{ path }} must not be one of the following values: {{ values }}",
                "validation.mixed.defined": "{{ path }} must be defined",
                "validation.string.length": "{{ path }} must be exactly {{ length }} characters",
                "validation.string.min": "{{ path }} must be at least {{ min }} characters",
                "validation.string.max": "{{ path }} must be at most {{ max }} characters",
                "validation.string.matches": "{{ path }} must match the following: \"{{ regex }}\"",
                "validation.string.email": "{{ path }} must be a valid email",
                "validation.string.url": "{{ path }} must be a valid URL",
                "validation.string.uuid": "{{ path }} must be a valid UUID",
                "validation.string.trim": "{{ path }} must be a trimmed string",
                "validation.string.lowercase": "{{ path }} must be a lowercase string",
                "validation.string.uppercase": "{{ path }} must be a uppercase string",
                "validation.number.min": "{{ path }} must be greater than or equal to {{ min }}",
                "validation.number.max": "{{ path }} must be less than or equal to {{ max }}",
                "validation.number.less_than": "{{ path }} must be less than {{ less }}",
                "validation.number.more_than": "{{ path }} must be greater than {{ more }}",
                "validation.number.positive": "{{ path }} must be a positive number",
                "validation.number.negative": "{{ path }} must be a negative number",
                "validation.number.integer": "{{ path }} must be an integer",
                "validation.date.min": "{{ path }} field must be later than {{ min }}",
                "validation.date.max": "{{ path }} field must be at earlier than {{ max }}",
                "validation.boolean.is_value": "{{ path }} field must be {{ value }}",
                "validation.array.min": "{{ path }} field must have at least {{ min }} items",
                "validation.array.max": "{{ path }} field must have less than or equal to {{ max }} items",
                "validation.array.length": "{{ path }} must have {{ length }} items",
                "validation.custom.password": "{{ path }} must contain at least two of the following character types: upper case English, lower case English, Arabic numerals, and non-alphanumeric characters ('-!\"#$%&()*,./:;?@[]^_`{|}~+<=>)",
                "validation.custom.equal_to": "{{ path }} must be equal to {{ field }}",
                "validation.custom.gte_to": "{{ path }} must be greater than or equal to {{ field }}",
                "validation.custom.lte_to": "{{ path }} must be less than or equal to {{ field }}",
                "validation.custom.file_size": "{{ path }} file size limit is {{ size }}",
                "validation.custom.record_path": "{{ path }} can only use letters, numbers, underscores, dashes, and periods.",
                "validation.custom.empty_or_length_range": "{{ path }} should be blank or {{ min }} to {{ max }} characters",
                "validation.custom.domain_suffix": "{{ path }} should contain a domain of {{ suffix }}",
                "validation.custom.phone_number": "{{ path }} is not a valid phone number.",
                "validation.custom.must_be_checked": "This field must be checked.",
                "validation.custom.credit_card_cvc": "Invalid card verification code (CVC).",
                "validation.custom.credit_card_expiration_date": "Invalid card expiration date.",
                "validation.custom.credit_card_number": "Invalid card number.",
                "landing_page.faq.title": "Frequently Asked Questions",
                "landing_page.testimonial.title": "Listen to what they have say"
            },
            "widget": {
                "messenger.title": "Messaging",
                "messenger.header.messages": "Messages",
                "messenger.header.archived": "Archived",
                "messenger.header.unread": "Unread",
                "messenger.filter.messages": "All Messages",
                "messenger.filter.archived": "Archived",
                "messenger.filter.unread": "Unread",
                "messenger.success.archive.message": "Message has been archived",
                "messenger.success.archive.action.view_archived": "View all archived messages",
                "messenger.success.restore.message": "Message has been restored",
                "messenger.success.restore.action.view_all": "View all messages",
                "messenger.confirm.mark_all_as_read.title": "Mark all conversations as read",
                "messenger.confirm.mark_all_as_read.description": "Are you sure you want to mark all unread messages as read?",
                "messenger.confirm.mark_all_as_read.action.proceed": "OK",
                "messenger.confirm.reject_connection_and_decline_conversation.title": "Are you certain you want to decline the message and connection request?",
                "messenger.confirm.reject_connection_and_decline_conversation.description": "You will no longer receive their their message notifications upon confirming the rejection.",
                "messenger.message.draft": "Draft",
                "messenger.message.users": "Users",
                "messenger.message.messages": "Messages",
                "messenger.message.n_messages_found": "{{ count }} messages found",
                "messenger.message.selected": "Selected",
                "messenger.message.no_messages": "No messages",
                "messenger.message.no_archived_messages": "No archived messages",
                "messenger.message.no_unread_messages": "No unread messages",
                "messenger.message.new_message_document_title": "{{ name }} send messages to you.",
                "messenger.message.my_message": "You: {{ content }}",
                "messenger.message.new_message": "New message",
                "messenger.message.attachment": "Sent an attachment",
                "messenger.message.replied_image": "Image",
                "messenger.message.replied_attachment": "Attachment",
                "messenger.message.current_user_emoji": "You sent an {{ emoji }}",
                "messenger.message.opposite_user_emoji": "{{ name }} sent an {{ emoji }}",
                "messenger.message.both_users_emoji": "{{ name }} and you sent an {{ emoji }}",
                "messenger.message.cancellable_emoji": "tap to remove",
                "messenger.message.empty": "Build Your Professional Network",
                "messenger.message.empty_description": "Click {{- icon }} icon on the company page or under talent search engine to start the conversation.",
                "messenger.message.job_preview_title": "APPLY HERE:",
                "messenger.message.conversation_invited": "{{ name }} would like to send you a message.",
                "messenger.message.conversation_declined": "Conversation request ignored. You will no longer receive new message notifications.",
                "messenger.message.connection_invited": "Please respond to the connection invitation from {{ name }}.",
                "messenger.message.connection_and_conversation_invited": "{{ name }} wants to send you a message and connect with you",
                "messenger.message.new_conversation_title": "Say Hello to {{ name }}",
                "messenger.message.new_conversation_description": "A well written Cake message could improve your response rate by 40%. If you have a hard time writing an approach message, please refer to {{- article }}.",
                "messenger.message.new_conversation_description_article": "this article",
                "messenger.message.new_conversation_hint": "Type a name to start a new conversation",
                "messenger.message.upgrade_title": "Upgrade your plan to start a conversation with {{ name }}",
                "messenger.message.upgrade_hint": "Your current plan has a daily limit of 10 new conversations.",
                "messenger.message.press_enter_to_send": "Press Enter to send",
                "messenger.message.search_hint": "Search messages",
                "messenger.message.request_pending": "Since the other user is not a contact of yours, you will need to wait until the other user accepts your message/connection request.",
                "messenger.message.request_accepted_by_current_user": "You have accepted the message request",
                "messenger.message.request_accepted_by_opposite_user": "Your message request has been accepted",
                "messenger.message.connection_request_accepted_by_current_user": "You have accepted the message and connection request",
                "messenger.message.connection_request_accepted_by_opposite_user": "Your message and connection request has been accepted",
                "messenger.message.talent_network_matched": "You MATCHED!",
                "messenger.message.talent_network_matched_by_current_user": "You have found and matched with {{- name }} from the {{- pool }} category",
                "messenger.message.talent_network_matched_by_current_user_all": "You have found and matched with {{- name }} from the General (All) category",
                "messenger.message.talent_network_matched_by_opposite_user": "{{- name }} has matched with you from the {{- pool }} category",
                "messenger.message.talent_network_matched_by_opposite_user_all": "{{- name }} has matched with you from the General (All) category",
                "messenger.placeholder": "Type a message...",
                "messenger.action.reply": "Reply",
                "messenger.action.emoji": "Emoji",
                "messenger.action.new_message": "New message",
                "messenger.action.manage": "Manage conversations",
                "messenger.action.archive": "Archive",
                "messenger.action.restore": "Restore",
                "messenger.action.mark_as_read": "Mark as read",
                "messenger.action.mark_as_unread": "Mark as unread",
                "messenger.action.mark_all_as_read": "Mark all as read",
                "messenger.action.search_companies": "Search for companies",
                "messenger.action.search_talents": "Search for talents",
                "messenger.action.send": "Send",
                "messenger.action.accept_conversation": "Accept",
                "messenger.action.decline_conversation": "Ignore",
                "messenger.action.accept_connection_and_accept_conversation": "Accept",
                "messenger.action.reject_connection_and_decline_conversation": "Reject",
                "messenger.action.reject_connection_but_accept_conversation": "Accept message request only",
                "messenger.action.resume_conversation": "Resume conversation",
                "messenger.action.start_conversation": "Start conversation",
                "messenger.action.upgrade": "Upgrade Now",
                "messenger.action.find_connections": "Find my connections",
                "messenger.instant_reply.title": "Instant reply with a click of a button!",
                "messenger.instant_reply.description": "Leave a positive first impression and increase your future career prospects by taking the initiative to respond to messages from employers.",
                "messenger.instant_reply.template.interested": "I’m interested in learning more about the position.",
                "messenger.instant_reply.template.not_now": "I’m not seeking new opportunities at this time.",
                "messenger.instant_reply.template.set_up_date": "Sure, let’s set-up a date to chat about the position.",
                "messenger.new_message.label": "To:",
                "messenger.new_message.placeholder": "Type a name",
                "messenger.emoji.thumbs_up": "thumbs up",
                "messenger.emoji.clap": "clap",
                "messenger.emoji.heart": "heart",
                "messenger.emoji.joy": "joy",
                "messenger.emoji.smile": "smile",
                "messenger.emoji.other": "emoji",
                "message_channel_list.title": "Messages",
                "message_channel_list.action.view_all": "All messages",
                "notification.title": "Notifications",
                "notification.message.empty": "No notifications yet",
                "notification.message.empty_description": "We will keep you posted when there’s an important notification.",
                "notification.action.view_all": "View all",
                "notification.action.mark_all_as_read": "Mark all as read",
                "response_statistic.badge": "Very Responsive to Messages",
                "response_statistic.badge_tip": "Improve your message response time to earn a coveted \"Very Responsive to Messages\" badge",
                "response_statistic.rate": "{{ percentage }} response rate",
                "response_statistic.rate_low": "Response rate is within 75%",
                "response_statistic.time": "responds within {{ duration }}",
                "response_statistic.time_minute": "{{ count }} minutes",
                "response_statistic.time_minute@one": "{{ count }} minute",
                "response_statistic.time_minute@other": "{{ count }} minutes",
                "response_statistic.time_hour": "{{ count }} hours",
                "response_statistic.time_hour@one": "{{ count }} hour",
                "response_statistic.time_hour@other": "{{ count }} hours",
                "response_statistic.rate_and_time": "{{ rate }} ({{ time }})",
                "connection.tooltip.add_to_folder": "Add to a project",
                "connection.tooltip.invite": "Invite talents to your job",
                "connection.tooltip.message": "Send messages",
                "connection.action.connect": "Connect",
                "connection.action.pending": "Pending",
                "connection.action.received": "Respond",
                "connection.action.connected": "Connection",
                "connection.action.accept": "Accept",
                "connection.action.reject": "Reject",
                "connection.action.cancel": "Cancel request",
                "connection.action.remove": "Remove Connection",
                "connection.connection_message.customize_connection_request": "Customize Your Connection Request",
                "connection.connection_message.customize_connection_description": "Cake members are more likely to accept invitations that include a personal message.",
                "connection.connection_message.customize_connection_cancel": "Send Request",
                "connection.connection_message.customize_connection_proceed": "Customize Message",
                "connection.connection_message.customize_connection_input_title": "Customize Message",
                "connection.connection_message.customize_connection_input_placeholder": "Ex: Hello [first name], it was great meeting you at [x event]. Our conversation about [y topic] made me think about [z topic]. I really enjoyed our discussion and love to chat further. Let’s connect and keep in touch.",
                "connection.connection_message.connection_invitation_success": "Connection invitation has been sent.",
                "connection.connection_message.customize_message_cancel": "Cancel",
                "connection.connection_message.customize_message_send": "Send",
                "recommendations.title": "Recommendations",
                "recommendations.action.view_all": "View All",
                "network_import.title": "Build your professional network",
                "network_import.description": "Import your contacts and connect with people you know to follow their career activities.",
                "network_import.google_modal.title": "Approve Data Sharing",
                "network_import.google_modal.access_approval": "By continuing, you approve Cake’s access, use, and receipt of data (secured using Google APIs) related to your Google Account.",
                "network_import.google_modal.policy_adherence_1": "All of which will adhere to the",
                "network_import.google_modal.policy_adherence_2": ", including the Limited Use requirements.",
                "network_import.import_contacts": "Import contacts from {{ provider }}",
                "profile_sidebar.current_job_progress": "Current Status",
                "profile_sidebar.tooltip.completion_ratio": "Profile completion ratio",
                "profile_sidebar.link.profile": "Profile",
                "profile_sidebar.link.resume": "Resume",
                "profile_sidebar.link.portfolio": "Portfolio",
                "profile_sidebar.link.recommendations": "Recommendations",
                "profile_sidebar.link.credits": "Reputation Credits",
                "profile_sidebar.link.posts": "Posts",
                "profile_sidebar.link.profile_views": "Profile Views",
                "profile_sidebar.link.profile_views@one": "Profile View",
                "profile_sidebar.link.profile_views@other": "Profile Views",
                "profile_sidebar.link.connections": "Connections",
                "profile_sidebar.link.connections@one": "Connection",
                "profile_sidebar.link.connections@other": "Connections",
                "profile_sidebar.link.mutual_connections": "Mutual connections",
                "profile_sidebar.link.mutual_connections@one": "Mutual connection",
                "profile_sidebar.link.mutual_connections@other": "Mutual connections",
                "company_sidebar.link.applicants": "Applicants",
                "company_sidebar.link.applicants@one": "Applicant",
                "company_sidebar.link.applicants@other": "Applicants",
                "company_sidebar.link.jobs": "Job items",
                "company_sidebar.link.jobs@one": "Job item",
                "company_sidebar.link.jobs@other": "Job items",
                "company_sidebar.link.page_views": "Company Views",
                "company_sidebar.link.page_views@one": "Company View",
                "company_sidebar.link.page_views@other": "Company Views",
                "company_sidebar.link.analytics": "Analytics",
                "hashtag_info.message.posts": "posts",
                "hashtag_info.message.posts@one": "post",
                "hashtag_info.message.posts@other": "posts",
                "hashtag_info.action.follow": "Follow",
                "hashtag_info.action.following": "Following",
                "popular_hashtags.title": "Explore hashtags",
                "popular_hashtags.message.posts": "posts",
                "popular_hashtags.message.posts@one": "post",
                "popular_hashtags.message.posts@other": "posts",
                "dialog.message.confirm": "Please enter \"{{ answer }}\" to proceed.",
                "dialog.action.cancel": "$t(common:action.cancel)",
                "dialog.action.proceed": "$t(common:action.confirm)",
                "recent_search.title": "Recent",
                "recent_search.action.clear_all": "Clear all",
                "global_search.placeholder": "Search",
                "user_card.message.present": "Present",
                "user_card.message.high_completion_ratio": "Profile completion rate over 75%",
                "user_card.message.past_experience": "Past",
                "user_card.message.past_experience_at": "at",
                "user_card.message.past_education": "Past",
                "user_card.message.message_you_sent": "Your customized request to {{ username }}",
                "company_card.message.jobs": "Jobs",
                "company_card.message.jobs@one": "Job",
                "company_card.message.jobs@other": "Jobs",
                "useful_links.title": "More Career and Recruitment Resources",
                "useful_links.link.resume_builder": "Build your resume",
                "useful_links.link.job_search": "Search for jobs",
                "useful_links.link.job_posting": "Post your jobs",
                "useful_links.link.talent_search": "Search for talents",
                "spam_appeal_banner.description": "We have noticed that your profile has been temporarily hidden from the platform due to insufficient or poor quality content information in your profile (including your personal profile, resume, and portfolio). We suggest you optimize the content of your profile and complete filling out the profile information.\nIf you believe our judgment is incorrect, please be sure to click \"File a Dispute,\" and we will promptly review and respond to your situation. During the review period, your account will not be affected, and you can still use the platform's features normally. If this action causes you any inconvenience, we sincerely apologize.",
                "spam_appeal_banner.appeal_success": "Your dispute has been received, and we will review it and respond as soon as possible.",
                "spam_appeal_banner.action.appeal": "File a Dispute",
                "spam_appeal_banner.action.appealed": "Dispute Received",
                "spam_display_confirmation.title": "Warning",
                "spam_display_confirmation.description": "Our system has detected that the page you are about to visit may contain suspicious content. Please confirm again before proceeding. Please consider carefully.",
                "spam_display_confirmation.action.continue": "Continue to visit",
                "spam_display_confirmation.action.leave": "Leave the page",
                "app_redirect_banner.description": "Optimize Your Experience with Cake App.",
                "app_redirect_banner.open_app": "Open App",
                "resume_template_select.action.new_resume": "New resume",
                "resume_template_select.action.choose_template": "Choose template",
                "resume_template_select.action.upgrade_to_use": "Upgrade to use",
                "confirm_email.title.verify_email": "Verify your email",
                "confirm_email.title.update_email": "Update email",
                "confirm_email.title.check_code": "Please check your email to find the verification code.",
                "confirm_email.description.verify_email": "To proceed further, please verify your email a security measure.",
                "confirm_email.description.check_code": "Please check your email and enter the verification code. (The code will expire in 10 minutes.)",
                "confirm_email.field.code.label": "Verification code",
                "confirm_email.field.code.placeholder": "Enter the verification code",
                "confirm_email.action.send_confirmation": "Send verification email",
                "confirm_email.action.resend_confirmation": "Resend verification code",
                "confirm_email.action.update_email": "Update your email",
                "confirm_email.footnote.not_your_email": "It's not your email?",
                "confirm_email.footnote.did_not_get_email": "Didn't receive the email?",
                "promote_resume_builder.title": "Resume Builder",
                "promote_resume_builder.description": "Build your resume only in minutes!",
                "promote_resume_builder.action.create_resume": "Create my resume for free",
                "popular_companies.title": "Job search",
                "popular_companies.description": "Tens of thousands of jobs listings. Salary transparency. Accurate and instant search experience.",
                "popular_companies.jobs": "Jobs",
                "popular_companies.jobs@one": "Job",
                "popular_companies.jobs@other": "Jobs",
                "popular_companies.action.browse": "Browse all jobs & companies",
                "interview_invitation.title": "Invite {{ name }} to apply for this position",
                "interview_invitation.tooltip": "Invite talents to your job",
                "interview_invitation.message.preview": "Preview Message",
                "interview_invitation.message.message_default": "Hi {first_name},\n\nI'm {my_first_name} from {company}. I came across your profile on Cake and would like to invite you to take a look at our {job_title} opening. Please submit an application if you're interested, and feel free to let me know if you have any questions. Thanks!\n\nRegards,\n{my_first_name}",
                "interview_invitation.message.message_description": "You can add the following parameters to your messages",
                "interview_invitation.message.remaining_invitations": "{{ count }} invites left for this month",
                "interview_invitation.message.remaining_invitations@one": "{{ count }} invite left for this month",
                "interview_invitation.message.remaining_invitations@other": "{{ count }} invites left for this month",
                "interview_invitation.message.hint_max_invitations": "Send up to {{ count }} invites per month.",
                "interview_invitation.message.hint_upgrade": "Upgrade your plan to get more invites!",
                "interview_invitation.message.no_companies": "No company pages found.",
                "interview_invitation.message.no_jobs": "No job openings found.",
                "interview_invitation.message.success": "Invitation sent!",
                "interview_invitation.field.company": "Company",
                "interview_invitation.field.job": "Job title",
                "interview_invitation.field.message": "Message",
                "interview_invitation.placeholder.company": "Please select a company",
                "interview_invitation.placeholder.job": "Please select a job",
                "interview_invitation.action.post_jobs": "Post Jobs",
                "interview_invitation.action.submit": "Send invitation",
                "auth_modal.title": "Making Your Resume a Piece of Cake",
                "auth_modal.title@tw": "Career Community",
                "auth_modal.slogan.career_pages": "Business Solutions",
                "auth_modal.slogan.talent_search": "Talent Search",
                "auth_modal.slogan.job_posting": "Job Posting",
                "auth_modal.slogan.jobs": "Job Search",
                "auth_modal.slogan.writing_for_cakeresume": "Become a Contributor",
                "auth_modal.slogan.portfolio": "Build a Free Online Portfolio",
                "auth_modal.slogan.chat_with": "Start a Conversation With {{ name }}",
                "auth_modal.message.or": "or",
                "auth_modal.message.forgot_password": "Forgot your password?",
                "auth_modal.message.already_signed_up": "Already have an account?",
                "auth_modal.message.not_yet_signed_up": "Don’t have an account?",
                "auth_modal.message.accept": "By signing up, you are agreeing to our {{- terms }} and {{- privacy }}",
                "auth_modal.message.accept_terms": "Terms of Service",
                "auth_modal.message.accept_privacy": "Privacy Policy",
                "auth_modal.message.subscribe_newsletter": "I consent to subscribing to Cake's newsletter",
                "auth_modal.field.remember": "Remember me",
                "auth_modal.placeholder.name": "Your Name",
                "auth_modal.placeholder.email": "Email",
                "auth_modal.placeholder.password": "Password",
                "auth_modal.oauth_sign_in": "Log in with {{ provider }}",
                "auth_modal.oauth_sign_up": "Sign up with {{ provider }}",
                "verify_code.title": "Enter the verification code",
                "verify_code.description": "Open the authenticator app on your mobile device to receive your verification code.",
                "verify_code.placeholder.code": "Enter the code",
                "verify_code.note": "If you can't access your authenticator app, please enter a backup code.",
                "verify_code.action.submit": "Verify",
                "viewed_and_saved_jobs.viewed_jobs": "Recently Viewed",
                "viewed_and_saved_jobs.saved_jobs": "Saved Jobs",
                "image_upload_modal.title": "Add image",
                "image_upload_modal.dropzone_hint": "Drop an image here or choose a file.",
                "image_upload_modal.delete_existing": "Delete the existing image",
                "image_upload_modal.confirm_delete": "Are you sure you want to delete the existing image?",
                "image_edit_modal.title": "Edit image",
                "image_edit_modal.cropping_option.original": "Original",
                "image_edit_modal.cropping_option.wide": "Wide",
                "image_edit_modal.cropping_option.square": "Square",
                "job_search_guest_hint.title": "Cake Job Search",
                "job_search_guest_hint.description": "Join Cake now! Search tens of thousands of job listings to find your perfect job.",
                "job_search_guest_hint.action.sign_up": "Sign Up",
                "job_search_guest_hint.action.post_jobs": "Post Jobs",
                "tech_stack_list.title": "Company tech stack",
                "folder_list.title": "Recent Projects",
                "folder_list.empty": "You haven't create any projects.",
                "folder_list.action.view_all": "View All Projects",
                "folder_list.action.create_project": "Create Project",
                "share_link.facebook": "Facebook",
                "share_link.twitter": "Twitter",
                "share_link.linkedin": "LinkedIn",
                "share_link.copy_link": "Copy link",
                "share_link.email": "Email",
                "share_to_feed.success.title": "Shared to Feed",
                "share_to_feed.success.action.view_post": "View post",
                "social_share.select_job_to_share": "Select a Job you want to share",
                "social_share.facebook_group": "Cake FB Job Search Group",
                "social_share.share_on_social_media": "Share on Social Media",
                "portfolio_list.empty": "No project yet.",
                "searchable_select.no_options": "No options",
                "published_resume_select.title": "Select a published resume",
                "published_resume_select.empty.title": "You don't have a Cake Resume yet.",
                "published_resume_select.empty.action": "Create a Cake Resume",
                "reusable_pdf_select.title": "Select an uploaded pdf file",
                "reusable_pdf_select.empty.title": "You don't have any uploaded pdf files",
                "file_dropzone.title": "Upload new files",
                "file_dropzone.description": "Drop files here or click to upload.",
                "work_experience_list.none": "I don’t have any work experience",
                "education_list.none": "I don’t have a degree",
                "education_list.activities_and_societies": "Activities and societies",
                "education_list.description": "Description",
                "certification_list.none": "I don’t have any certificates or licenses",
                "certification_list.credential_id": "Credential ID",
                "certification_list.issued_at": "Issued {{ year_month }}",
                "certification_list.expires_at": "Expiration Date: {{ year_month }}",
                "certification_list.no_expiration": "No Expiration Date",
                "fresh_chat.welcome_message": "Welcome!",
                "fresh_chat.welcome_sub_message": "Is there anything we can help you with?",
                "fresh_chat.chat_title": "Come chat with us!",
                "fresh_chat.faq_title": "FAQs",
                "message_preview.to": "To:",
                "message_preview.action.send": "Send",
                "share_attachment_form.title": "Share to Feed",
                "share_attachment_form.placeholder": "Share your thoughts on ...",
                "share_attachment_form.action.publish": "Post",
                "broadcast.new_reputation_credit.message": "{{- receiver }} was given credit for {{ category }} by {{- giver }}",
                "whatsapp_subscription.title": "Subscribe to WhatsApp",
                "whatsapp_subscription.description": "Receive tailored job alerts according to your preference on your WhatsApp: {{ phone }}.",
                "whatsapp_subscription.field.phone": "Phone",
                "whatsapp_subscription.action.subscribe": "Subscribe Now",
                "whatsapp_subscription.action.update_phone": "Change my Number",
                "whatsapp_subscription.success.title": "Thanks for Subscribing!",
                "whatsapp_subscription.success.description": "You will soon receive job alerts and recommendations on WhatsApp: {{ phone }}.",
                "whatsapp_subscription.success.action.view_cakeresume_whatsapp": "Reach out to Cake on WhatsApp",
                "dashboard_sidebar.section.my.title": "My",
                "dashboard_sidebar.section.my.item.profile": "Profile",
                "dashboard_sidebar.section.my.item.resumes": "Resumes",
                "dashboard_sidebar.section.my.item.portfolio": "Portfolio",
                "dashboard_sidebar.section.my.item.posts": "Posts",
                "dashboard_sidebar.section.my.item.credits": "Reputation Credits",
                "dashboard_sidebar.section.my.item.recommendations": "Recommendations",
                "dashboard_sidebar.section.my.item.articles": "Articles",
                "dashboard_sidebar.section.my.item.community": "Community",
                "dashboard_sidebar.section.recruitment.title": "Recruitment",
                "dashboard_sidebar.section.recruitment.item.career_pages": "Company Pages",
                "dashboard_sidebar.section.recruitment.item.folders": "Talent Projects",
                "dashboard_sidebar.section.recruitment.item.resume_reviews": "Resume Reviews",
                "dashboard_sidebar.section.network.title": "Network",
                "dashboard_sidebar.section.network.item.connections": "Connections",
                "dashboard_sidebar.section.network.item.insights": "Analytics",
                "dashboard_sidebar.section.job_search.title": "Job Search",
                "dashboard_sidebar.section.job_search.item.following_companies": "Following Companies",
                "dashboard_sidebar.section.job_search.item.my_jobs": "My Jobs",
                "dashboard_sidebar.section.favorite.title": "Favorite",
                "dashboard_sidebar.section.favorite.item.resumes": "Resumes",
                "dashboard_sidebar.section.favorite.item.portfolios": "Portfolios",
                "dashboard_sidebar.section.assessment.title": "Assessment",
                "dashboard_sidebar.section.assessment.item.coding_tests": "Coding Tests",
                "dashboard_sidebar.section.personality_tests.title": "Personality Test",
                "dashboard_sidebar.section.personality_tests.item.game": "What Cake R U? Pro",
                "dashboard_sidebar.section.personality_tests.item.analysis": "Analysis Dashboard",
                "dashboard_sidebar.action.view_public_profile": "View Public Profile",
                "dashboard_sidebar.badge.ai_check": "Cake AI Check",
                "reputation.tooltip.most_received_reputation": "{{ name }} has received the most reputation credits for {{ category }}.",
                "reputation.tooltip.technical_skills": "Technical Skills",
                "reputation.tooltip.problem_solving": "Problem-Solving",
                "reputation.tooltip.adaptability": "Adaptability",
                "reputation.tooltip.communication": "Communication",
                "reputation.tooltip.time_management": "Time Management",
                "reputation.tooltip.teamwork": "Teamwork",
                "reputation.tooltip.leadership": "Leadership",
                "account_type.title": "Welcome to Cake!",
                "account_type.description": "Please share what you will be using Cake for. This information will help us personalize your experience.",
                "account_type.modal_title": "Please select your account type",
                "account_type.action.answer": "Got it",
                "project_form.create.title": "Create new Recruiting Project",
                "project_form.create.description": "Keep close track of the recruiting process through projects.",
                "project_form.create.action.submit": "$t(common:action.create)",
                "project_form.update.title": "Edit the Recruiting Project",
                "project_form.update.action.submit": "$t(common:action.update)",
                "project_form.field.name": "Project name",
                "project_form.field.description": "Description",
                "project_form.field.disable_notification": "Send notification to candidates when they are added to projects.",
                "project_form.message.upgrade": "Upgrade your plan to toggle of the notification.",
                "project_form.placeholder.name": "e.g. Q1 full stack engineer hiring",
                "project_form.placeholder.description": "Enter a short description for your project",
                "resume_folders_form.title": "Add {{ name }} to recruiting project",
                "resume_folders_form.message.no_projects": "You have no project yet.",
                "resume_folders_form.message.candidates": "candidates",
                "resume_folders_form.message.candidates@one": "candidate",
                "resume_folders_form.message.candidates@other": "candidates",
                "resume_folders_form.message.updated": "Updated successfully",
                "resume_folders_form.action.create_project": "Create Project",
                "resume_folders_form.action.manage_projects": "Manage Projects",
                "report_spam.report_submitted": "Report Submitted",
                "report_spam.archive_message": "Thank you for reporting this suspicious activity. Do you want to archive this message?",
                "report_spam.detail": "Additional details",
                "promo_code_applied_hint.description": "You have a coupon for {{ percentage }}% off. Discount will be applied at checkout.",
                "promo_code_applied_hint.description_one_dollar": "You can purchase one of the plans shown below for NTD$1 (one-time deal).",
                "update_credit_card_alert_banner.declined": "[Credit Card Payment Failed] Please update your credit card information and process your payment by {{ date }} to avoid a suspension of services.",
                "update_credit_card_alert_banner.declined_and_downgraded": "[Unable to Process Payment] You will be downgraded to Cake Basic as of {{ date }}.",
                "update_credit_card_alert_banner.action.update": "Update Payment Information",
                "update_credit_card_alert_banner.action.retry_payment": "Retry Payment",
                "update_credit_card_alert_banner.close_confirmation_dialog.title": "Don't remind me again after closing",
                "update_credit_card_alert_banner.close_confirmation_dialog.body": "As of {{ date }} you have been downgraded to Cake Basic. By closing this reminder, you are agreeing to the downgrade and you will no longer receive reminders on updating your payment method nor reactivating your {{ plan }} plan.",
                "update_credit_card_alert_banner.close_confirmation_dialog.cancel": "Cancel",
                "update_credit_card_alert_banner.close_confirmation_dialog.confirm": "Confirm",
                "in_app_browser_hint.sign_in": "To ensure smooth login, please use your mobile browser (such as Chrome or Safari) instead of the in-app browser to avoid potential errors.",
                "in_app_browser_hint.sign_up": "To ensure smooth registration, please use your mobile browser (such as Chrome or Safari) instead of the in-app browser to avoid potential errors.",
                "in_app_browser_hint.action.open_browser": "Open Browser",
                "job_application_response.employer_with_high_application_response_rate": "High job application open rate",
                "job_application_response.highly_responsive_to_job_applicants": "Actively Reviewing Resumes",
                "job_recommendations.title": "Jobs you might be interested in",
                "job_recommendations.action.refresh": "Refresh for more results",
                "user_recommendations.title": "Expand your network",
                "user_recommendations.reason.education": "Based on your school experience",
                "user_recommendations.reason.work_experience": "Based on your work experience",
                "user_recommendations.reason.second_degree_connection": "Based on your network",
                "user_recommendations.action.refresh": "Refresh for more results",
                "braze_content_card.announcements_widget.title": "Announcements",
                "braze_content_card.events_widget.title": "Events",
                "braze_content_card.resources_widget.title": "Resources",
                "resume_health_check_button.ai_check": "Cake AI Check",
                "resume_health_check_button.view_ai_resume_report": "View Cake AI Resume Report",
                "resume_health_check_button.tooltip.disabled": "You must publish your resume to use this AI tool.",
                "quota_limit_reached_modal.title": "Quota Limit Reached",
                "quota_limit_reached_modal.ai_cover_letter_generation.description": "You can only use Cake AI to generate your cover letter up to 10 times within 24 hours.",
                "quota_limit_reached_modal.ai_cover_letter_generation.hint": "Please click on the 'Interested' button if you would like to extend your usage limits",
                "quota_limit_reached_modal.resume_health_check.description": "You have reach the 10 times scan limit within 24 hours",
                "quota_limit_reached_modal.resume_health_check.hint": "If you’re interested in getting more quota please click “I am interested!”",
                "quota_limit_reached_modal.action.interested": "I am interested",
                "quota_limit_reached_modal.action.not_now": "Not now",
                "quota_limit_reached_modal.flash_message.interested.title": "Thank you for your interest",
                "quota_limit_reached_modal.flash_message.interested.description": "We'll release our advanced Cake AI features soon and will prioritize notifying you as soon as they become available.",
                "generation_content_display.feedback.hows_the_result": "How's the result?",
                "generation_content_display.feedback.what_do_you_think": "What do you think of result {{ current }}?",
                "generation_content_display.feedback.share_feedback": "Share your feedback with Cake",
                "generation_content_display.feedback.thanks_for_submitting": "Thanks for submitting your feedback",
                "company_follow_button.follow_options.notify_new_jobs_opening.title": "Notify me when there're new job openings.",
                "company_follow_button.follow_options.notify_new_jobs_opening_related_to_my_profile.title": "Notify me when there're new job openings related to my profile.",
                "company_follow_button.follow_options.notify_new_jobs_opening_related_to_my_profile.description": "To enhance the accuracy of job recommendations, our system will automatically analyze the \"profession\" section of your profile. Please ensure this section is fully completed.",
                "company_follow_button.flash_message.success_title": "Notification enabled",
                "company_follow_button.flash_message.success_description_notify_new_jobs_opening": "We will notify you when the company posts new job openings.",
                "company_follow_button.flash_message.success_description_notify_new_jobs_opening_related_to_my_profile": "We will notify you when the company posts new job openings related to your professional background. Please ensure your profile information is complete."
            },
            "attribute": {
                "language_proficiency.0": "Beginner",
                "language_proficiency.1": "Intermediate",
                "language_proficiency.2": "Fluent",
                "language_proficiency.3": "Professional",
                "language_proficiency.4": "Native or Bilingual",
                "salary_type.per_day": "day",
                "salary_type.per_hour": "hour",
                "salary_type.per_month": "month",
                "salary_type.per_year": "year",
                "salary_type.piece_rate_pay": "piece rate",
                "negotiable_salary": "Negotiable",
                "regular_salary_4k": "Regular earnings reach NT$40,000",
                "regular_salary_4k_tooltip": "Regular earnings include gross pay for actual hours worked, fixed allowance and bonus paid by month, and other regular payment. Please check the Ministry of Labor website for more details.",
                "salary_type_unit.per_day": "Daily",
                "salary_type_unit.per_hour": "Hourly",
                "salary_type_unit.per_month": "Monthly",
                "salary_type_unit.per_year": "Annual",
                "salary_type_unit.piece_rate_pay": "Piece rate pay",
                "job_type.full_time": "Full-time",
                "job_type.part_time": "Part-time",
                "job_type.intern": "Intern",
                "job_type.internship": "Internship",
                "job_type.contract": "Contract",
                "job_type.freelance": "Freelance",
                "job_type.temporary": "Temporary",
                "job_type.volunteer": "Volunteer",
                "seniority_level.entry_level": "Entry level",
                "seniority_level.mid_senior_level": "Mid-Senior level",
                "seniority_level.internship_level": "Internship",
                "seniority_level.associate": "Assistant",
                "seniority_level.director": "Director",
                "seniority_level.executive": "Executive (VP, GM, C-Level)",
                "online_status.online": "Online",
                "online_status.active": "Active",
                "online_status.offline": "Offline",
                "online_status.active_ago": "Active {{ period }} ago",
                "folder_user_role.owner": "Owner",
                "folder_user_role.collaborator": "Collaborator",
                "folder_user_role.viewer": "Viewer",
                "update_days_ago_level.7": "Within one week",
                "update_days_ago_level.14": "Within two weeks",
                "update_days_ago_level.30": "Within one month",
                "update_days_ago_level.60": "Within two months",
                "update_days_ago_level.90": "Within three months",
                "update_days_ago_level.180": "Within six months",
                "update_days_ago_level.365": "Within one year",
                "update_days_ago_level.366": "More than one year",
                "employment_status.employed": "Employed",
                "employment_status.unemployed": "Unemployed",
                "employment_status.studying": "Studying",
                "employment_status.in_military_service": "In military service",
                "job_search_progress.not_open_to_opportunities": "Not open to opportunities",
                "job_search_progress.open_to_opportunities": "Open to opportunities",
                "job_search_progress.ready_to_interview": "Ready to interview",
                "remote.not_interested": "Not interested in working remotely",
                "remote.interested": "Interested in working remotely",
                "remote.remote_only": "Remote Only",
                "work_experience.description_relevant": "{{ experience }} relevant",
                "work_experience._one": "Less than 1 year",
                "work_experience.one_two": "1-2 years",
                "work_experience.two_four": "2-4 years",
                "work_experience.four_six": "4-6 years",
                "work_experience.six_ten": "6-10 years",
                "work_experience.ten_fifteen": "10-15 years",
                "work_experience.fifteen_": "More than 15 years",
                "freelance.full_time_freelancer": "Full-time freelancer",
                "freelance.part_time_freelancer": "Part-time freelancer",
                "freelance.not_freelancer": "Non-freelancer",
                "freelance.full_time_freelancer@self": "Yes, I'm currently a full-time freelancer",
                "freelance.part_time_freelancer@self": "Yes, I freelance in my spare time",
                "freelance.not_freelancer@self": "No",
                "degree_type.associate_s_degree": "Associate’s Degree",
                "degree_type.bachelor_of_arts_ba": "Bachelor of Arts (BA)",
                "degree_type.bachelor_of_business_administration_bba": "Bachelor of Business Administration (BBA)",
                "degree_type.bachelor_of_engineering_beng": "Bachelor of Engineering (BEng)",
                "degree_type.bachelor_of_fine_arts_bfa": "Bachelor of Fine Arts (BFA)",
                "degree_type.bachelor_of_science_bs": "Bachelor of Science (BS)",
                "degree_type.bachelor_s_degree": "Bachelor’s Degree",
                "degree_type.engineer_s_degree": "Engineer’s Degree",
                "degree_type.master_of_arts_ma": "Master of Arts (MA)",
                "degree_type.master_of_business_administration_mba": "Master of Business Administration (MBA)",
                "degree_type.master_of_fine_arts_mfa": "Master of Fine Arts (MFA)",
                "degree_type.master_of_science_ms": "Master of Science (MS)",
                "degree_type.master_s_degree": "Master’s Degree",
                "degree_type.doctor_of_philosophy_phd": "Doctor of Philosophy (PhD)",
                "degree_type.doctor_of_medicine_md": "Doctor of Medicine (MD)",
                "degree_type.juris_doctor_jd": "Juris Doctor (JD)",
                "degree_type.high_school_diploma": "High School Diploma",
                "degree_type.non_degree_program_e_g_coursera_certificate": "Non-Degree Program (e.g. Coursera certificate)",
                "degree_type.other": "Other",
                "read_status.read": "Read",
                "read_status.unread": "Unread",
                "management.description": "I've had experience in managing {{ number }}",
                "management.none": "None",
                "management.one_five": "1-5 people",
                "management.five_ten": "5-10 people",
                "management.ten_fifteen": "10-15 people",
                "management.fifteen_": "15+ people",
                "management.not_specified": "Not specified",
                "work_exp_year.no_requirement": "No requirement for relevant working experience",
                "work_exp_year.n_years_required": "{{ n }} years of experience required",
                "year_of_seniority.0_1": "Less than 1 year",
                "year_of_seniority.1_3": "1~3 years",
                "year_of_seniority.3_5": "3~5 years",
                "year_of_seniority.5_10": "5~10 years",
                "year_of_seniority.10_": "More than 10 years",
                "number_of_employees.1_10": "1 ~ 10",
                "number_of_employees.11_50": "11 ~ 50",
                "number_of_employees.51_200": "51 ~ 200",
                "number_of_employees.201_500": "201 ~ 500",
                "number_of_employees.501_1000": "501 ~ 1000",
                "number_of_employees.1001_5000": "1001 ~ 5000",
                "number_of_employees.5001_": "> 5000",
                "education_level.less_than_high_school": "Less than high school",
                "education_level.high_school": "High school",
                "education_level.associate": "Associate",
                "education_level.bachelor": "Bachelor",
                "education_level.master": "Master",
                "education_level.doctoral": "Doctoral",
                "human_number.0": "",
                "human_number.3": "Thousand",
                "human_number.4": "Ten thousand",
                "human_number.6": "Million",
                "human_number.8": "Hundred million",
                "human_number.9": "Billion",
                "human_number.12": "Trillion",
                "short_number.0": "",
                "short_number.3": "K",
                "short_number.4": "(10K)",
                "short_number.6": "M",
                "tech_usage_level.low": "Seldom",
                "tech_usage_level.medium": "Occasionally",
                "tech_usage_level.high": "Frequently",
                "number_of_management.not_specified": "Managing staff numbers: not specified",
                "number_of_management.none": "No management responsibility",
                "number_of_management.one_five": "Managing 1-5 staff",
                "number_of_management.five_ten": "Managing 5-10 staff",
                "number_of_management.ten_fifteen": "Managing 10-15 staff",
                "number_of_management.fifteen_": "Managing 15+ staff",
                "reputation_category.technical_skills": "Technical Skills",
                "reputation_category.problem_solving": "Problem-Solving",
                "reputation_category.adaptability": "Adaptability",
                "reputation_category.communication": "Communication",
                "reputation_category.time_management": "Time Management",
                "reputation_category.teamwork": "Teamwork",
                "reputation_category.leadership": "Leadership",
                "network_identity_type.employer": "Employer",
                "network_identity_type.headhunter": "Headhunter",
                "network_identity_type.job_seeker": "Job Seeker",
                "network_identity_type.mentor": "Mentor",
                "network_identity_type.professional": "Professional",
                "network_identity_type.intern": "Intern",
                "network_identity_type.freelancer": "Freelancer",
                "network_identity_type.entrepreneur": "Entrepreneur",
                "network_identity_type.kol": "KOL / Influencer",
                "profile_privacy_level.public": "Anyone on the Internet can find and access. No sign-in required.",
                "profile_privacy_level.logged_in_users_can_view": "Only logged-in Cake users can view.",
                "post_privacy.anyone": "Anyone",
                "plan_period.annual": "Annual",
                "plan_period.monthly": "Monthly",
                "plan_period.quarterly": "Quarterly",
                "privacy_level.level_0_title": "Public",
                "privacy_level.level_0_description": "Anyone on the Internet (including Google search engine) can find and view it. No sign-in is required.",
                "privacy_level.level_0_tooltip": "A public resume is like your personal website. It can help you build an online personal brand.",
                "privacy_level.level_1_title": "Any person with the link and any employer on the platform",
                "privacy_level.level_1_description": "Any employer with the link (without having to sign in) and any employer on Cake can view the information.",
                "privacy_level.level_2_title": "Only for those with the link",
                "privacy_level.level_2_description": "Anyone who has the link can access. No sign-in required.",
                "article_category_group_v2.job_search_guide": "Job Search Guide",
                "article_category_group_v2.career_guidance": "Career Guidance",
                "article_category_group_v2.business_excellence": "Business Excellence",
                "article_category_group_v2.success_stories": "Success Stories",
                "article_category_group_v2.about_cakeresume": "About Cake",
                "article_category_v2.resume": "Resume & CV",
                "article_category_v2.cover_letter": "Cover Letter",
                "article_category_v2.portfolio": "Portfolio",
                "article_category_v2.interview_guide": "Interview Skills",
                "article_category_v2.job_searching_guide": "Job Search Tips",
                "article_category_v2.industry_job_overview": "Industry & Job Overview",
                "article_category_v2.career_planning": "Career Planning",
                "article_category_v2.workplace": "Workplace",
                "article_category_v2.career_tool": "Career Tools",
                "article_category_v2.career_development": "Career Development",
                "article_category_v2.news": "News",
                "article_category_v2.student_guide": "Student Guide",
                "article_category_v2.personal_branding": "Personal Branding",
                "article_category_v2.personal_development": "Personal Development",
                "article_category_v2.networking": "Networking",
                "article_category_v2.management": "Management",
                "article_category_v2.people_operations": "People Operations",
                "article_category_v2.recruitment": "Recruitment & HR",
                "article_category_v2.success_stories": "Success Stories",
                "article_category_v2.cakeresume_people_culture": "People & Culture",
                "article_category_v2.cakeresume_news_updates": "News & Updates",
                "article_category_v2.cakeresume_events": "Events",
                "inclusivity_trait.foreign_talents.title": "Foreign Talents",
                "inclusivity_trait.foreign_talents.description": "Non-citizens and visa holders.",
                "inclusivity_trait.career_change.title": "Career Changers Friendly",
                "inclusivity_trait.career_change.description": "Candidates who are just starting out.",
                "inclusivity_trait.back_to_work.title": "Return to Work",
                "inclusivity_trait.back_to_work.description": "Individuals returning to work after taking a career break or managing family matters.",
                "inclusivity_trait.disability_confident.title": "Disability Confident / Inclusive",
                "inclusivity_trait.disability_confident.description": "People with disabilities.",
                "inclusivity_trait.lgbtq.title": "LGBTQ+",
                "inclusivity_trait.lgbtq.description": "LGBTQ+ individuals.",
                "inclusivity_trait.gender_equity.title": "Gender Equity",
                "inclusivity_trait.gender_equity.description": "Equal opportunities for all genders.",
                "inclusivity_trait.senior_citizen.title": "Senior Citizens",
                "inclusivity_trait.senior_citizen.description": "Retired elders as per societal or legal age standards.",
                "work_title.business_owner": "Business Owner",
                "work_title.executive": "Executive (C-Level, VP, Director)",
                "work_title.hiring_manager_or_team_leader": "Hiring Manager/Team Leader",
                "work_title.team_member": "Team Member",
                "work_title.recruiter": "Recruiter",
                "work_title.freelancer": "Freelancer",
                "page_admin_role.owner.name": "Owner",
                "page_admin_role.owner.description": "Owner",
                "page_admin_role.admin.name": "Super Admin",
                "page_admin_role.admin.description": "Manage company page, team member permissions, job projects, posts, and candidates.",
                "page_admin_role.job_and_application_manager.name": "Job Admin",
                "page_admin_role.job_and_application_manager.description": "Create and manage job projects, job posts, and candidates.",
                "page_admin_role.application_manager.name": "Assigned Job Manager",
                "page_admin_role.application_manager.description": "Manage specific job projects, job posts, and candidates",
                "page_admin_role.member.name": "Member",
                "page_admin_role.member.description": "No permissions to manage jobs and candidates."
            },
            "sector": {
                "sector_groups.advertising-marketing-agency": "Advertising / Marketing / Agency",
                "sector_groups.agriculture": "Agriculture",
                "sector_groups.architecture": "Architecture",
                "sector_groups.banking-insurance-finance": "Banking / Insurance / Finance",
                "sector_groups.consulting-audit": "Consultant / Audit",
                "sector_groups.corporate-services": "Corporate services",
                "sector_groups.culture-media-entertainment": "Culture / Media / Entertainment",
                "sector_groups.design-art": "Design / Art",
                "sector_groups.distribution": "Distribution",
                "sector_groups.education-training-recruitment": "Education / Training / Recruitment",
                "sector_groups.fashion-luxury-beauty-lifestyle": "Fashion / Luxury / Beauty / Lifestyle",
                "sector_groups.food-and-beverage": "Food and Beverage",
                "sector_groups.health-social-environment": "Health / Social / Environment",
                "sector_groups.hotel-tourism-leisure": "Hotel / Tourism / Leisure",
                "sector_groups.industry": "Industry",
                "sector_groups.legal-law": "Legal / Law",
                "sector_groups.medical": "Medical",
                "sector_groups.mobility-transport": "Mobility / Transport",
                "sector_groups.non-profit-association": "Non-profit / Association",
                "sector_groups.public-administration": "Public administration",
                "sector_groups.real-estate": "Real estate",
                "sector_groups.service-industry": "Service Industry",
                "sector_groups.tech": "Tech",
                "sectors.advertising-marketing-agency_adtech-martech": "AdTech / MarTech",
                "sectors.advertising-marketing-agency_advertising": "Advertising",
                "sectors.advertising-marketing-agency_design": "Design",
                "sectors.advertising-marketing-agency_digital": "Digital",
                "sectors.advertising-marketing-agency_event-management": "Event Management",
                "sectors.advertising-marketing-agency_marketing-communications": "Marketing / Communications",
                "sectors.advertising-marketing-agency_public-relations": "Public Relations",
                "sectors.agriculture_agricultural-technology": "Agricultural Technology",
                "sectors.agriculture_dairy": "Dairy",
                "sectors.agriculture_farming": "Farming",
                "sectors.agriculture_fishery": "Fishery",
                "sectors.architecture_architecture": "Architecture",
                "sectors.architecture_interior-design": "Interior Design",
                "sectors.banking-insurance-finance_banking": "Banking",
                "sectors.banking-insurance-finance_finance": "Finance",
                "sectors.banking-insurance-finance_fintech-insurtech": "FinTech / InsurTech",
                "sectors.banking-insurance-finance_insurance": "Insurance",
                "sectors.banking-insurance-finance_investment-banking": "Investment Banking",
                "sectors.banking-insurance-finance_investment-management": "Investment Management",
                "sectors.banking-insurance-finance_venture-capital-private-equity": "Venture Capital & Private Equity",
                "sectors.consulting-audit_accounting": "Accounting",
                "sectors.consulting-audit_audit": "Audit",
                "sectors.consulting-audit_change-management": "Change Management",
                "sectors.consulting-audit_design-engineering-office": "Design & Engineering Office",
                "sectors.consulting-audit_digital-marketing-data-marketing": "Digital Marketing / Data Marketing",
                "sectors.consulting-audit_events-services": "Events Services",
                "sectors.consulting-audit_it-digital": "IT / Digital",
                "sectors.consulting-audit_organization-management": "Organization / Management",
                "sectors.consulting-audit_strategy": "Strategy",
                "sectors.consulting-audit_supply-chain": "Supply Chain",
                "sectors.consulting-audit_transaction-services": "Transaction Services",
                "sectors.corporate-services_corporate-concierge-services": "Corporate Concierge Services",
                "sectors.corporate-services_corporate-support": "Corporate Support",
                "sectors.corporate-services_coworking": "Coworking",
                "sectors.corporate-services_incubator-accelerator": "Incubator / Accelerator",
                "sectors.corporate-services_recruitment-services-worker-dispatch": "Recruitment Services / Worker Dispatch",
                "sectors.culture-media-entertainment_film": "Film",
                "sectors.culture-media-entertainment_media": "Media",
                "sectors.culture-media-entertainment_museums-cultural-institutions": "Museums / Cultural Institutions",
                "sectors.culture-media-entertainment_music": "Music",
                "sectors.culture-media-entertainment_print-media": "Print Media",
                "sectors.culture-media-entertainment_publishing": "Publishing",
                "sectors.culture-media-entertainment_sports": "Sports",
                "sectors.culture-media-entertainment_television-film-production": "Television & Film Production",
                "sectors.culture-media-entertainment_theater": "Theater",
                "sectors.culture-media-entertainment_video-games": "Video Games",
                "sectors.design-art_design": "Design",
                "sectors.design-art_fine-art": "Fine Art",
                "sectors.design-art_graphic-design": "Graphic Design",
                "sectors.design-art_performing-arts": "Performing Arts",
                "sectors.design-art_photography": "Photography",
                "sectors.distribution_e-commerce": "E-Commerce",
                "sectors.distribution_mass-distribution": "Mass Distribution",
                "sectors.distribution_retail": "Retail",
                "sectors.distribution_selective-distribution": "Selective Distribution",
                "sectors.distribution_wholesale": "Wholesale",
                "sectors.education-training-recruitment_edtech": "EdTech",
                "sectors.education-training-recruitment_education": "Education",
                "sectors.education-training-recruitment_human-resources": "Human Resources",
                "sectors.education-training-recruitment_job-training": "Job Training",
                "sectors.education-training-recruitment_recruitment": "Recruitment",
                "sectors.fashion-luxury-beauty-lifestyle_cosmetics": "Cosmetics",
                "sectors.fashion-luxury-beauty-lifestyle_fashion": "Fashion",
                "sectors.fashion-luxury-beauty-lifestyle_jewelry": "Jewelry",
                "sectors.fashion-luxury-beauty-lifestyle_lifestyle": "Lifestyle",
                "sectors.fashion-luxury-beauty-lifestyle_luxury": "Luxury",
                "sectors.food-and-beverage_beverage": "Beverage",
                "sectors.food-and-beverage_consumer-goods": "Consumer Goods",
                "sectors.food-and-beverage_food-craft": "Food Craft",
                "sectors.food-and-beverage_food-production": "Food Production",
                "sectors.food-and-beverage_foodservice": "Foodservice",
                "sectors.food-and-beverage_foodtech": "FoodTech",
                "sectors.food-and-beverage_gourmet-grocery": "Gourmet Grocery",
                "sectors.health-social-environment_collaborative-economy": "Collaborative Economy",
                "sectors.health-social-environment_environment-sustainable-development": "Environment / Sustainable Development",
                "sectors.health-social-environment_health": "Health",
                "sectors.health-social-environment_home-care-services": "Home Care Services",
                "sectors.health-social-environment_socialtech-greentech": "SocialTech / GreenTech",
                "sectors.hotel-tourism-leisure_hotel": "Hotel",
                "sectors.hotel-tourism-leisure_leisure": "Leisure",
                "sectors.hotel-tourism-leisure_tourism": "Tourism",
                "sectors.industry_aeronautics-space": "Aeronautics / Space",
                "sectors.industry_agri-food-animal-nutrition": "Agri-food / Animal Nutrition",
                "sectors.industry_automotive": "Automotive",
                "sectors.industry_building-public-works": "Building / Public Works",
                "sectors.industry_chemicals": "Chemicals",
                "sectors.industry_electronics-telecommunications": "Electronics / Telecommunications",
                "sectors.industry_energy": "Energy",
                "sectors.industry_furniture": "Furniture",
                "sectors.industry_manufacturing": "Manufacturing",
                "sectors.industry_material-science": "Material Science",
                "sectors.industry_mechanical-or-industrial-engineering": "Mechanical or Industrial Engineering",
                "sectors.industry_metallurg": "Metallurg",
                "sectors.industry_mining-metals": "Mining Metals",
                "sectors.industry_nanotechnology": "Nanotechnology",
                "sectors.industry_pharmaceutical-biotech": "Pharmaceutical / Biotech",
                "sectors.industry_rail": "Rail",
                "sectors.legal-law_law": "Law",
                "sectors.legal-law_legal-department": "Legal Department",
                "sectors.medical_medical-devices": "Medical Devices",
                "sectors.medical_medical-practice": "Medical Practice",
                "sectors.medical_mental-health-care": "Mental Health Care",
                "sectors.mobility-transport_logistics": "Logistics",
                "sectors.mobility-transport_mobility": "Mobility",
                "sectors.mobility-transport_shipping-and-ground-transport": "Shipping and Ground Transport",
                "sectors.non-profit-association_foundation": "Foundation",
                "sectors.non-profit-association_ngo": "NGO",
                "sectors.non-profit-association_nonprofit": "Non-profit",
                "sectors.public-administration_public-administration": "Public Administration",
                "sectors.public-administration_public-and-local-agencies": "Public and Local Agencies",
                "sectors.public-administration_unions-and-labor-organisation": "Unions and Labor Organisation",
                "sectors.real-estate_commercial-real-estate": "Commercial Real Estate",
                "sectors.real-estate_residential-real-estate": "Residential Real Estate",
                "sectors.service-industry_car-maintenance-services-scooter-and-motorcycle-maintenance-services": "Car Maintenance Services / Scooter and Motorcycle Maintenance Services",
                "sectors.service-industry_hairdressing-and-beauty-services": "Hairdressing and Beauty Services",
                "sectors.service-industry_housekeeping-services": "Housekeeping Services",
                "sectors.service-industry_pet-care-services": "Pet Care Services",
                "sectors.service-industry_private-detective-services": "Private Detective Services",
                "sectors.service-industry_rental-and-leasing-services": "Rental and Leasing Services",
                "sectors.service-industry_security-services": "Security Services",
                "sectors.tech_ar-vr": "AR/VR",
                "sectors.tech_artificial-intelligence-machine-learning": "Artificial Intelligence / Machine Learning",
                "sectors.tech_automatic-control": "Automatic Control",
                "sectors.tech_big-data": "Big Data",
                "sectors.tech_blockchain": "Blockchain",
                "sectors.tech_computer-networking": "Computer Networking",
                "sectors.tech_connected-objects": "Internet of Things (IoT)",
                "sectors.tech_consumer-electronics": "Consumer Electronics",
                "sectors.tech_cyber-security": "Cyber Security",
                "sectors.tech_ecommerce": "Ecommerce",
                "sectors.tech_gambling-casinos": "Gambling Casinos",
                "sectors.tech_games": "Games",
                "sectors.tech_hardware": "Hardware",
                "sectors.tech_information-services": "Information Services",
                "sectors.tech_internet": "Internet",
                "sectors.tech_mobile-apps": "Mobile Apps",
                "sectors.tech_robotics": "Robotics",
                "sectors.tech_saas-cloud-services": "SaaS / Cloud Services",
                "sectors.tech_semiconductor": "Semiconductor",
                "sectors.tech_software": "Software"
            }
        }
    },
    "initialLocale": "zh-TW",
    "ns": [
        "common",
        "widget",
        "attribute",
        "sector",
        "404"
    ],
    "userConfig": null
}


================================================
FILE: crawler/project_cakeresume/client_cakeresume.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from bs4 import BeautifulSoup

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_cakeresume.config_cakeresume import (
    HEADERS_CAKERESUME,
    JOB_LISTING_BASE_URL_CAKERESUME,
    JOB_CAT_URL_CAKERESUME,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_web_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[str]: # Return HTML content as string
    """
    通用的網頁請求函式，處理隨機延遲、請求發送、和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before web request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        return response.text
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during web request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except Exception as e:
        logger.error(
            "Unexpected error during web request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None

def fetch_cakeresume_category_data(
    url: str = JOB_CAT_URL_CAKERESUME, headers: Dict[str, str] = HEADERS_CAKERESUME
) -> Optional[Dict[str, Any]]:
    """
    從 CakeResume 獲取職務分類的原始數據。
    """
    html_content = _make_web_request("GET", url, headers=headers, log_context={"api_type": "cakeresume_category_data"})
    if html_content:
        soup = BeautifulSoup(html_content, 'html.parser')
        scripts = soup.find_all('script')
        if scripts and len(scripts) > 0 and scripts[-1].string:
            try:
                data = json.loads(scripts[-1].string)
                data = json.loads(scripts[-1].string)
                data = json.loads(scripts[-1].string)
                i18n_store = data['props']['pageProps']['_nextI18Next']['initialI18nStore']['zh-TW']
                
                categories = []
                
                # Extract from 'sector'
                for key, value in i18n_store.get('sector', {}).items():
                    if key.startswith('sector_groups.') or key.startswith('sectors.'):
                        categories.append({
                            'source_category_id': key,
                            'source_category_name': value
                        })
                
                # Extract from 'profession'
                for key, value in i18n_store.get('profession', {}).items():
                    if key.startswith('profession_groups.') or key.startswith('professions.'):
                        categories.append({
                            'source_category_id': key,
                            'source_category_name': value
                        })
                
                return {"categories": categories}
            except (json.JSONDecodeError, KeyError) as e:
                logger.error("Failed to parse CakeResume category JSON data.", error=e, exc_info=True)
                return None
    return None

def cake_me_url(KEYWORDS: str, CATEGORY: str, ORDER: Optional[str] = None) -> str:
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (str, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。
    """

    BASE_URL = JOB_LISTING_BASE_URL_CAKERESUME

    if KEYWORDS and CATEGORY:
        url = f"{BASE_URL}/{KEYWORDS}?profession[0]={CATEGORY}&page="
    elif KEYWORDS:
        url = f"{BASE_URL}/{KEYWORDS}?page="
    elif CATEGORY:
        url = f"{BASE_URL}/categories/{CATEGORY}?page="
    else:
        url = f"{BASE_URL}?page="

    if ORDER:  # 只在 ORDER 不為 None 時添加
        url = url.replace("?page=", f"?order={ORDER}&page=")

    return url

def fetch_cakeresume_job_urls(
    KEYWORDS: str,
    CATEGORY: str,
    ORDER: Optional[str] = None,
    PAGE_NUM: int = 0,
) -> Optional[str]: # Returns HTML content of the job listing page
    """
    從 CakeResume 獲取職缺 URL 列表的原始數據 (HTML 內容)。
    """
    url = cake_me_url(KEYWORDS, CATEGORY, ORDER) + str(PAGE_NUM)
    return _make_web_request(
        "GET",
        url,
        headers=HEADERS_CAKERESUME,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "cakeresume_job_urls",
            "keywords": KEYWORDS,
            "category": CATEGORY,
            "page": PAGE_NUM,
        },
    )

def fetch_cakeresume_job_data(job_url: str) -> Optional[str]: # Returns HTML content of the job detail page
    """
    從 CakeResume 職缺頁面抓取單一 URL 的資料 (HTML 內容)。
    """
    return _make_web_request(
        "GET",
        job_url,
        headers=HEADERS_CAKERESUME,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "cakeresume_job_detail",
            "url": job_url,
        },
    )




================================================
FILE: crawler/project_cakeresume/config_cakeresume.py
================================================
# crawler/project_cakeresume/config_cakeresume.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# CakeResume 平台相關設定
WEB_NAME_CAKERESUME = config_section.get("WEB_NAME_CAKERESUME", "CakeResume")
JOB_CAT_URL_CAKERESUME = config_section.get("JOB_CAT_URL_CAKERESUME", "https://www.cakeresume.com/jobs")
JOB_LISTING_BASE_URL_CAKERESUME = config_section.get("JOB_LISTING_BASE_URL_CAKERESUME", "https://www.cakeresume.com/jobs")
JOB_DETAIL_BASE_URL_CAKERESUME = config_section.get("JOB_DETAIL_BASE_URL_CAKERESUME", "https://www.cakeresume.com") # Job URLs are constructed as base + path

HEADERS_CAKERESUME = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.cakeresume.com",
}

URL_CRAWLER_PAGE_SIZE_CAKERESUME = int(config_section.get("URL_CRAWLER_PAGE_SIZE_CAKERESUME", "20")) # Not directly used by CakeResume, but for consistency
URL_CRAWLER_ORDER_BY_CAKERESUME = config_section.get("URL_CRAWLER_ORDER_BY_CAKERESUME", "latest") # "latest" or other options



================================================
FILE: crawler/project_cakeresume/demo_platform_cakeresume_tasks.py
================================================
# crawler/projects/platform_cakeresume/tasks.py
"""
Celery task for Cakeresume. Extracts categories by parsing the
__NEXT_DATA__ script tag from the main jobs page HTML.
This method is fast, reliable, and avoids browser automation.
"""
import logging
import json
from typing import List, Dict, Any

from bs4 import BeautifulSoup

from crawler.app import app
from crawler.enums import SourcePlatform
from crawler.database import repository
from crawler.utils import make_request
from crawler.settings import settings

logger = logging.getLogger(__name__)


def parse_next_data_for_i18n_categories(html_content: str) -> List[Dict[str, Any]]:
    """
    Finds the __NEXT_DATA__ script tag, parses its JSON content,
    and extracts the hierarchical category data from the i18n (internationalization) object.
    This is the most reliable method.
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    next_data_script = soup.find('script', id='__NEXT_DATA__')
    
    if not next_data_script or not hasattr(next_data_script, 'string') or not next_data_script.string:
        raise ValueError("Could not find __NEXT_DATA__ script tag or it is empty in the HTML.")

    try:
        data = json.loads(next_data_script.string)
    except json.JSONDecodeError:
        raise ValueError("Failed to parse JSON from __NEXT_DATA__ script tag.")

    try:
        # The definitive path to the translation data
        i18n_data = data['props']['pageProps']['_nextI18Next']['initialI18nStore']['zh-TW']['profession']
    except KeyError as e:
        raise ValueError(f"Unexpected JSON structure in __NEXT_DATA__. Missing key: {e}")

    flat_list = []
    parent_map = {}

    # First pass: Get all parent categories (e.g., "profession_groups.it": "軟體")
    for key, value in i18n_data.items():
        if key.startswith("profession_groups."):
            parent_id = key.replace("profession_groups.", "")
            parent_name = value
            parent_map[parent_id] = parent_name
            flat_list.append({
                "source_platform": SourcePlatform.PLATFORM_CAKERESUME,
                "source_category_id": parent_id,
                "source_category_name": parent_name,
                "parent_source_id": None,
            })

    # Second pass: Get all sub-categories and link them to parents
    for key, value in i18n_data.items():
        if key.startswith("professions."):
            full_id = key.replace("professions.", "")
            parts = full_id.split('_', 1)
            if len(parts) > 1:
                parent_id = parts[0]
                if parent_id in parent_map:
                    flat_list.append({
                        "source_platform": SourcePlatform.PLATFORM_CAKERESUME,
                        "source_category_id": full_id,
                        "source_category_name": value,
                        "parent_source_id": parent_id,
                    })
                else:
                    logger.warning(f"Found orphan sub-category '{full_id}' with no matching parent '{parent_id}'. Skipping.")

    logger.info(f"Successfully extracted {len(flat_list)} categories from __NEXT_DATA__.")
    return flat_list


@app.task(bind=True, name="platform_cakeresume.run_category_pipeline", acks_late=True, time_limit=300, queue='category_queue')
def run_category_pipeline(self) -> None:
    """
    Fetches Cakeresume job categories by parsing the initial
    server-side rendered page data from the __NEXT_DATA__ tag.
    """
    try:
        logger.info("[Cakeresume] Running category pipeline from __NEXT_DATA__ (Final Version)...")
        cfg = settings.pcake
        
        res = make_request("https://www.cakeresume.com/jobs", headers=cfg.headers)
        
        categories = parse_next_data_for_i18n_categories(res.text)
        
        if not categories:
            logger.warning("[Cakeresume] No categories were extracted from __NEXT_DATA__. Aborting sync.")
            return

        result = repository.sync_source_categories(SourcePlatform.PLATFORM_CAKERESUME, categories)
        logger.info(f"[Cakeresume] Category pipeline finished successfully. Sync result: {result}")

    except Exception as e:
        logger.error(f"[Cakeresume] Category pipeline failed: {e}", exc_info=True)
        raise self.retry(exc=e, countdown=180)


================================================
FILE: crawler/project_cakeresume/producer_category_cakeresume.py
================================================
from crawler.project_cakeresume.task_category_cakeresume import fetch_and_sync_cakeresume_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_cakeresume.config_cakeresume import JOB_CAT_URL_CAKERESUME

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_cakeresume_categories.s(JOB_CAT_URL_CAKERESUME).apply_async(queue='producer_category_cakeresume')
logger.info("send task_category_cakeresume url", url=JOB_CAT_URL_CAKERESUME, queue='producer_category_cakeresume')


================================================
FILE: crawler/project_cakeresume/producer_jobs_cakeresume.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_cakeresume.task_jobs_cakeresume import fetch_url_data_cakeresume
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_cakeresume_job_urls():
    """
    從資料庫讀取待處理或失敗的 CakeResume 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 CakeResume 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_CAKERESUME,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 CakeResume 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 CakeResume URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 CakeResume URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_cakeresume.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_cakeresume")

        logger.info(
            "已成功分發一批 CakeResume 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_cakeresume"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_cakeresume/producer_urls_cakeresume.py
================================================
from crawler.database.repository import get_all_categories_for_platform
from crawler.project_cakeresume.task_urls_cakeresume import crawl_and_store_cakeresume_category_urls
from crawler.database.models import SourcePlatform
import structlog

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Starting URL task distribution for all CakeResume categories.")

all_cakeresume_categories = get_all_categories_for_platform(SourcePlatform.PLATFORM_CAKERESUME)

if all_cakeresume_categories:
    logger.info("Found categories for PLATFORM_CAKERESUME.", count=len(all_cakeresume_categories))
    root_categories = [
        cat for cat in all_cakeresume_categories if cat.parent_source_id is None
    ]

    if root_categories:
        logger.info(
            "Found root categories for PLATFORM_CAKERESUME.", count=len(root_categories)
        )

        for category_info in root_categories:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id)
            crawl_and_store_cakeresume_category_urls.delay(category_info.model_dump())
    else:
        logger.info("No root categories found for PLATFORM_CAKERESUME.")
else:
    logger.info("No categories found for PLATFORM_CAKERESUME.")


================================================
FILE: crawler/project_cakeresume/task_category_cakeresume.py
================================================
import os
import structlog

# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

from crawler.worker import app
from crawler.database.schemas import SourcePlatform
from crawler.database.repository import get_source_categories, sync_source_categories
from crawler.project_cakeresume.config_cakeresume import JOB_CAT_URL_CAKERESUME
from crawler.project_cakeresume.client_cakeresume import (
    fetch_cakeresume_category_data,
)

logger = structlog.get_logger(__name__)


def flatten_cakeresume_categories(data):
    """
    Flattens the CakeResume category data structure.
    """
    flattened = []
    sectors_path = data.get('initialI18nStore', {}).get('zh-TW', {}).get('sector', {}).get('sectors', {})
    for key, name in sectors_path.items():
        source_category_id = key.split('_')[-1] if '_' in key else key.replace('.', '')
        flattened.append({
            "parent_source_id": None,
            "source_category_id": source_category_id,
            "source_category_name": name,
            "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
        })

    sector_groups_path = data.get('initialI18nStore', {}).get('zh-TW', {}).get('sector', {}).get('sector_groups', {})
    for group_key, group_data in sector_groups_path.items():
        for sub_key, sub_name in group_data.items():
            source_category_id = sub_key.split('_')[-1] if '_' in sub_key else sub_key.replace('.', '')
            flattened.append({
                "parent_source_id": group_key,
                "source_category_id": source_category_id,
                "source_category_name": sub_name,
                "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
            })

    return flattened


@app.task()
def fetch_and_sync_cakeresume_categories(url_JobCat: str = JOB_CAT_URL_CAKERESUME):
    logger.info("Starting CakeResume category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = get_source_categories(SourcePlatform.PLATFORM_CAKERESUME)

        jobcat_data = fetch_cakeresume_category_data(url_JobCat)
        if jobcat_data is None:
            logger.error("Failed to fetch CakeResume category data from web.", url=url_JobCat)
            return

        flattened_data = flatten_cakeresume_categories(jobcat_data)

        if not existing_categories:
            logger.info("CakeResume category database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            sync_source_categories(SourcePlatform.PLATFORM_CAKERESUME, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            logger.info(
                "Found new or updated CakeResume categories to sync.",
                count=len(categories_to_sync),
            )
            sync_source_categories(SourcePlatform.PLATFORM_CAKERESUME, categories_to_sync)
        else:
            logger.info("No new or updated CakeResume categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during CakeResume category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    # To run this script for local testing, execute:
    # python -m crawler.project_cakeresume.task_category_cakeresume
    # This will automatically use the 'test_db' as configured at the top of the script.

    from crawler.database.connection import initialize_database
    initialize_database()

    logger.info("Dispatching fetch_and_sync_cakeresume_categories task for local testing.", url=JOB_CAT_URL_CAKERESUME)
    fetch_and_sync_cakeresume_categories(JOB_CAT_URL_CAKERESUME)


================================================
FILE: crawler/project_cakeresume/task_jobs_cakeresume.py
================================================
import os
# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

import structlog
from typing import Optional
import re
import json
from bs4 import BeautifulSoup
from datetime import datetime, timezone

from crawler.worker import app
from crawler.database.schemas import (
    CrawlStatus,
    SourcePlatform,
    JobPydantic,
    JobStatus,
    JobType,
    SalaryType,
)
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_cakeresume.client_cakeresume import fetch_cakeresume_job_data
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

# CakeResume 的 job_type 到我們內部 JobType Enum 的映射
JOB_TYPE_MAPPING_CAKERESUME = {
    "full-time": JobType.FULL_TIME,
    "part-time": JobType.PART_TIME,
    "internship": JobType.INTERNSHIP,
    "contract": JobType.CONTRACT,
    "other": JobType.OTHER,
}

# CakeResume 的 salary_type 到我們內部 SalaryType Enum 的映射
SALARY_TYPE_MAPPING_CAKERESUME = {
    "monthly": SalaryType.MONTHLY,
    "yearly": SalaryType.YEARLY,
    "hourly": SalaryType.HOURLY,
    "daily": SalaryType.DAILY,
}

def clean_html_if_string(value):
    if isinstance(value, str):
        return BeautifulSoup(value, "html.parser").get_text(separator=' ', strip=True)
    return value


def parse_cakeresume_job_data_to_pydantic(job_details: dict, url: str) -> Optional[JobPydantic]:
    try:
        job_id_match = re.search(r'/jobs/([a-zA-Z0-9]+)', url)
        job_id = job_id_match.group(1) if job_id_match else None

        if not job_id:
            logger.error("Failed to extract job_id from URL for parsing.", url=url)
            return None

        title = job_details.get('title')
        description = clean_html_if_string(job_details.get('description'))
        if not description:
            description = clean_html_if_string(job_details.get('job_responsibilities'))
        if not description:
            description = clean_html_if_string(job_details.get('requirements'))

        job_type_raw = job_details.get('job_type')
        job_type = JOB_TYPE_MAPPING_CAKERESUME.get(job_type_raw)
        if job_type is None:
            job_type = JobType.OTHER

        locations = job_details.get('locations', [])
        location_text = ', '.join(loc.get('name') for loc in locations if loc.get('name')) if locations else None

        posted_at = None
        created_at_str = job_details.get('created_at')
        if created_at_str:
            try:
                posted_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00')).astimezone(timezone.utc).replace(tzinfo=None)
            except ValueError:
                logger.warning("Could not parse posted_at date format.", created_at=created_at_str, job_id=job_id)

        salary_min = job_details.get('salary_min')
        salary_max = job_details.get('salary_max')
        salary_type_raw = job_details.get('salary_type')
        salary_type = SALARY_TYPE_MAPPING_CAKERESUME.get(salary_type_raw)
        salary_currency = job_details.get('salary_currency')
        salary_text = f"{salary_currency} {salary_min}-{salary_max} ({salary_type_raw})" if salary_min and salary_max else None
        if job_details.get('hide_salary_completely'):
            salary_text = "面議"
            salary_type = SalaryType.NEGOTIABLE
            salary_min = None
            salary_max = None

        experience_required_text = job_details.get('seniority_level')
        min_work_exp_year = job_details.get('min_work_exp_year')
        if min_work_exp_year is not None:
            experience_required_text = f"{min_work_exp_year} 年以上" if min_work_exp_year > 0 else "不拘"
        
        if experience_required_text is None:
            experience_required_text = "不拘"

        education_required_text = None
        if education_required_text is None:
            education_required_text = "不拘"

        company_info = job_details.get('company', {})
        company_source_id = company_info.get('id')
        company_name = company_info.get('name')
        company_url = company_info.get('website_url')

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_CAKERESUME,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=str(company_source_id) if company_source_id else None,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing CakeResume job data to Pydantic.",
            error=e,
            url=url,
            exc_info=True,
        )
        return None
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)


@app.task()
def fetch_url_data_cakeresume(url: str) -> Optional[dict]:
    job_id = None
    try:
        job_id_match = re.search(r'/jobs/([a-zA-Z0-9]+)', url)
        job_id = job_id_match.group(1) if job_id_match else None

        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        html_content = fetch_cakeresume_job_data(url)
        if html_content is None:
            logger.error("Failed to fetch job data from CakeResume web.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        soup = BeautifulSoup(html_content, 'html.parser')
        data_script = soup.find('script', id='__NEXT_DATA__')

        if not data_script:
            logger.error("Error: Could not find job data (script#__NEXT_DATA__) on the page.", url=url, job_id=job_id)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        page_props = json.loads(data_script.string)['props']['pageProps']
        job_details = page_props.get('job')

        if not job_details:
            logger.error("Error: Could not parse job data ('job' key not found) from JSON.", url=url, job_id=job_id)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        job_pydantic_data = parse_cakeresume_job_data_to_pydantic(job_details, url)

        if not job_pydantic_data:
            logger.error(
                "Failed to parse job data to Pydantic.",
                job_id=job_id,
                url=url,
            )
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        upsert_jobs([job_pydantic_data])

        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when processing CakeResume job data.",
            error=e,
            job_id=job_id if 'job_id' in locals() else "N/A",
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    # To run this script for local testing, execute:
    # python -m crawler.project_cakeresume.task_jobs_cakeresume
    # This will automatically use the 'test_db' as configured at the top of the script.

    from crawler.database.connection import initialize_database
    initialize_database()

    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]
    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_CAKERESUME,
        statuses=statuses_to_fetch,
        limit=10,
    )
    for url in urls_to_process:
        logger.info("Starting to process URL from the database.", url=url)
        fetch_url_data_cakeresume(url)


================================================
FILE: crawler/project_cakeresume/task_urls_cakeresume.py
================================================
import os
import structlog
from collections import deque
from bs4 import BeautifulSoup

# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

from crawler.worker import app
from crawler.database.schemas import SourcePlatform, UrlCategoryPydantic, CategorySourcePydantic
from crawler.database.repository import upsert_urls, upsert_url_categories, get_all_categories_for_platform
from crawler.project_cakeresume.client_cakeresume import fetch_cakeresume_job_urls
from crawler.config import (
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_cakeresume.config_cakeresume import (
    URL_CRAWLER_ORDER_BY_CAKERESUME,
    JOB_DETAIL_BASE_URL_CAKERESUME,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_cakeresume_category_urls(job_category: dict, url_limit: int = 0) -> None:
    """
    Celery task: Iterates through all pages of a specified CakeResume job category, fetches job URLs,
    and stores them in the database.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    global_job_url_set = set()
    current_batch_urls = []
    current_batch_url_categories = []
    recent_counts = deque(maxlen=4)

    current_page = 0
    max_page = 1

    logger.info(
        "Task started: crawling CakeResume job category URLs.", job_category_code=job_category_code, url_limit=url_limit
    )

    while True:
        if url_limit > 0 and len(global_job_url_set) >= url_limit:
            logger.info("URL limit reached. Ending task early.", job_category_code=job_category_code, url_limit=url_limit, collected_urls=len(global_job_url_set))
            break

        if current_page % 5 == 0:
            logger.info(
                "Current page being processed.",
                page=current_page,
                job_category_code=job_category_code,
            )

        html_content = fetch_cakeresume_job_urls(
            KEYWORDS="",
            CATEGORY=job_category_code,
            ORDER=URL_CRAWLER_ORDER_BY_CAKERESUME,
            PAGE_NUM=current_page,
        )

        if html_content is None:
            logger.error(
                "Failed to retrieve job URLs from CakeResume.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        soup = BeautifulSoup(html_content, 'html.parser')
        job_urls_on_page = soup.find_all('a', class_='JobSearchItem_jobTitle__bu6yO')

        if not job_urls_on_page:
            logger.info("No more job URLs found for this category and page.", page=current_page, job_category_code=job_category_code)
            break

        for job_url_item in job_urls_on_page:
            job_link_suffix = job_url_item.get('href')
            if job_link_suffix:
                full_job_link = f"{JOB_DETAIL_BASE_URL_CAKERESUME}{job_link_suffix}"
                if full_job_link not in global_job_url_set:
                    global_job_url_set.add(full_job_link)
                    current_batch_urls.append(full_job_link)
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=full_job_link,
                        source_category_id=job_category_code,
                    ).model_dump()
                )

        pagination_items = soup.find_all('a', class_='Pagination_itemNumber___enNq')
        if pagination_items:
            try:
                max_page = int(pagination_items[-1].text)
            except ValueError:
                logger.warning("Could not parse max_page from pagination items.", job_category_code=job_category_code)

        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "Batch upload size reached. Starting URL and URL-Category upload.",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_urls(SourcePlatform.PLATFORM_CAKERESUME, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            current_batch_urls.clear()
            current_batch_url_categories.clear()

        total_jobs = len(global_job_url_set)
        recent_counts.append(total_jobs)
        if len(recent_counts) == recent_counts.maxlen and len(set(recent_counts)) == 1:
            logger.info(
                "No new data found consecutively. Ending task early.",
                max_len=recent_counts.maxlen,
                job_category_code=job_category_code,
            )
            break

        current_page += 1
        if current_page > max_page:
            logger.info("Reached max page. Ending task.", current_page=current_page, max_page=max_page, job_category_code=job_category_code)
            break

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining raw job URLs to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_urls(SourcePlatform.PLATFORM_CAKERESUME, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No URLs collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code)


if __name__ == "__main__":
    # To run this script for local testing, execute:
    # python -m crawler.project_cakeresume.task_urls_cakeresume
    # This will automatically use the 'test_db' as configured at the top of the script.

    from crawler.database.connection import initialize_database
    initialize_database()

    job_category_lists = get_all_categories_for_platform(SourcePlatform.PLATFORM_CAKERESUME)

    for job_category in job_category_lists:
        logger.info("Dispatching crawl_and_store_cakeresume_category_urls task for local testing.", job_category_code=job_category.source_category_id)
        crawl_and_store_cakeresume_category_urls(job_category.model_dump())


================================================
FILE: crawler/project_yes123/client_yes123.py
================================================
import random
import time
from typing import Any, Dict, Optional

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import urllib.parse

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_yes123.config_yes123 import (
    HEADERS_YES123,
    JOB_LISTING_BASE_URL_YES123,
    JOB_CAT_URL_YES123,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_web_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[str]: # Return HTML content as string
    """
    通用的網頁請求函式，處理隨機延遲、請求發送、和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before web request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        return response.text
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during web request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except Exception as e:
        logger.error(
            "Unexpected error during web request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None

def fetch_yes123_category_data(
    url: str = JOB_CAT_URL_YES123, headers: Dict[str, str] = HEADERS_YES123
) -> Optional[str]:
    """
    從 yes123 獲取職務分類的原始數據 (HTML 內容)。
    """
    return _make_web_request("GET", url, headers=headers, log_context={"api_type": "yes123_category_data"})

def yes123_url(
    KEYWORDS: str = "",
    CATEGORY: str = "",
    ORDER: str = "new",
    PAGE_NUM: int = 1,
) -> str:
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 yes123 求職網的完整職缺網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別代碼。
    ORDER (str, optional): 排序方式。預設為 "new" (最新)。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。

    返回:
    str: 生成的 yes123 求職網址。
    """
    base_url = JOB_LISTING_BASE_URL_YES123
    params = {
        "p": PAGE_NUM,
        "s_key": KEYWORDS,
        "job_kind": CATEGORY,
        "order": ORDER,
    }
    query_string = urllib.parse.urlencode(params)
    return f"{base_url}?{query_string}"

def fetch_yes123_job_urls(
    KEYWORDS: str = "",
    CATEGORY: str = "",
    ORDER: str = "new",
    PAGE_NUM: int = 1,
) -> Optional[str]: # Returns HTML content of the job listing page
    """
    從 yes123 獲取職缺 URL 列表的原始數據 (HTML 內容)。
    """
    url = yes123_url(KEYWORDS, CATEGORY, ORDER, PAGE_NUM)
    return _make_web_request(
        "GET",
        url,
        headers=HEADERS_YES123,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "yes123_job_urls",
            "keywords": KEYWORDS,
            "category": CATEGORY,
            "page": PAGE_NUM,
        },
    )

def fetch_yes123_job_data(job_url: str) -> Optional[str]: # Returns HTML content of the job detail page
    """
    從 yes123 職缺頁面抓取單一 URL 的資料 (HTML 內容)。
    """
    return _make_web_request(
        "GET",
        job_url,
        headers=HEADERS_YES123,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "yes123_job_detail",
            "url": job_url,
        },
    )



================================================
FILE: crawler/project_yes123/config_yes123.py
================================================
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# yes123 平台相關設定
WEB_NAME_YES123 = config_section.get("WEB_NAME_YES123", "yes123_求職網")
JOB_CAT_URL_YES123 = config_section.get("JOB_CAT_URL_YES123", "https://www.yes123.com.tw/admin/job_refer_data.asp?sno=20090101_jobkind_001")
JOB_LISTING_BASE_URL_YES123 = config_section.get("JOB_LISTING_BASE_URL_YES123", "https://www.yes123.com.tw/findjob/job_list.asp")
JOB_DETAIL_BASE_URL_YES123 = config_section.get("JOB_DETAIL_BASE_URL_YES123", "https://www.yes123.com.tw/findjob/job_detail.asp")

HEADERS_YES123 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.yes123.com.tw",
}

URL_CRAWLER_PAGE_SIZE_YES123 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_YES123", "20"))
URL_CRAWLER_ORDER_BY_YES123 = config_section.get("URL_CRAWLER_ORDER_BY_YES123", "new") # "new" or other options



================================================
FILE: crawler/project_yes123/producer_category_yes123.py
================================================
from crawler.project_yes123.task_category_yes123 import fetch_and_sync_yes123_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_yes123.config_yes123 import JOB_CAT_URL_YES123

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_yes123_categories.s(JOB_CAT_URL_YES123).apply_async(queue='producer_category_yes123')
logger.info("send task_category_yes123 url", url=JOB_CAT_URL_YES123, queue='producer_category_yes123')


================================================
FILE: crawler/project_yes123/producer_jobs_yes123.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_yes123.task_jobs_yes123 import fetch_url_data_yes123
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_yes123_job_urls():
    """
    從資料庫讀取待處理或失敗的 yes123 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 yes123 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_YES123,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 yes123 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 yes123 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 yes123 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_yes123.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_yes123")

        logger.info(
            "已成功分發一批 yes123 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_yes123"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_yes123/producer_urls_yes123.py
================================================
from crawler.database.repository import get_all_categories_for_platform
from crawler.project_yes123.task_urls_yes123 import crawl_and_store_yes123_category_urls
from crawler.database.models import SourcePlatform
import structlog

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Starting URL task distribution for all yes123 categories.")

all_yes123_categories = get_all_categories_for_platform(SourcePlatform.PLATFORM_YES123)

if all_yes123_categories:
    logger.info("Found categories for PLATFORM_YES123.", count=len(all_yes123_categories))
    root_categories = [
        cat for cat in all_yes123_categories if cat.parent_source_id is None
    ]

    if root_categories:
        logger.info(
            "Found root categories for PLATFORM_YES123.", count=len(root_categories)
        )

        for category_info in root_categories:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id)
            crawl_and_store_yes123_category_urls.delay(category_info.model_dump())
    else:
        logger.info("No root categories found for PLATFORM_YES123.")
else:
    logger.info("No categories found for PLATFORM_YES123.")


================================================
FILE: crawler/project_yes123/task_category_yes123.py
================================================
import os
# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

import structlog
from bs4 import BeautifulSoup
import re
from crawler.worker import app
from crawler.database.schemas import SourcePlatform
from crawler.database.repository import get_source_categories, sync_source_categories
from crawler.project_yes123.config_yes123 import JOB_CAT_URL_YES123
from crawler.project_yes123.client_yes123 import fetch_yes123_category_data

logger = structlog.get_logger(__name__)


def flatten_yes123_categories(html_content):
    """
    Parses the HTML content from yes123 category page and flattens the category structure.
    """
    flattened = []
    soup = BeautifulSoup(html_content, 'html.parser')

    category_table = soup.find('table', class_='table_01')

    if not category_table:
        logger.error("Could not find category table in yes123 HTML.")
        return flattened

    for row in category_table.find_all('tr'):
        for td in row.find_all('td'):
            link = td.find('a')
            if link and 'href' in link.attrs:
                href = link['href']
                match = re.search(r'job_kind=(\d+)', href)
                if match:
                    source_category_id = match.group(1)
                    source_category_name = link.get_text(strip=True)
                    
                    parent_source_id = None
                    if len(source_category_id) == 9 and source_category_id.endswith('001'):
                        parent_source_id = source_category_id[:6]
                    elif len(source_category_id) == 6 and source_category_id.endswith('00'):
                        parent_source_id = source_category_id[:3]

                    flattened.append({
                        "parent_source_id": parent_source_id,
                        "source_category_id": source_category_id,
                        "source_category_name": source_category_name,
                        "source_platform": SourcePlatform.PLATFORM_YES123.value,
                    })
    return flattened


@app.task()
def fetch_and_sync_yes123_categories(url_JobCat: str = JOB_CAT_URL_YES123):
    logger.info("Starting yes123 category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = get_source_categories(SourcePlatform.PLATFORM_YES123)

        html_content = fetch_yes123_category_data(url_JobCat)
        if html_content is None:
            logger.error("Failed to fetch yes123 category data from web.", url=url_JobCat)
            return

        flattened_data = flatten_yes123_categories(html_content)

        if not flattened_data:
            logger.warning("No categories found in yes123 category data.", url=url_JobCat)
            return

        if not existing_categories:
            logger.info("yes123 category database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            sync_source_categories(SourcePlatform.PLATFORM_YES123, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_YES123.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            logger.info(
                "Found new or updated yes123 categories to sync.",
                count=len(categories_to_sync),
            )
            sync_source_categories(SourcePlatform.PLATFORM_YES123, categories_to_sync)
        else:
            logger.info("No new or updated yes123 categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during yes123 category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    # python -m crawler.project_yes123.task_category_yes123
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    logger.info("Dispatching fetch_and_sync_yes123_categories task for local testing.", url=JOB_CAT_URL_YES123)
    fetch_and_sync_yes123_categories(JOB_CAT_URL_YES123)



================================================
FILE: crawler/project_yes123/task_jobs_yes123.py
================================================
import os
# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

import structlog
from typing import Optional
import re
from datetime import datetime
from bs4 import BeautifulSoup

from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform, JobPydantic, JobStatus, JobType, SalaryType
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_yes123.client_yes123 import fetch_yes123_job_data
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

# yes123 的工作類型到我們內部 JobType Enum 的映射
# 根據 notebook 中的 '工作性質' 欄位
JOB_TYPE_MAPPING_YES123 = {
    "全職": JobType.FULL_TIME,
    "兼職": JobType.PART_TIME,
    "派遣": JobType.CONTRACT,
    "工讀": JobType.INTERNSHIP, # Assuming工讀 is internship
    "約聘": JobType.TEMPORARY,
    "其他": JobType.OTHER, # Added for cases where job_type is None
}


def parse_salary(
    salary_text: str,
) -> (Optional[int], Optional[int], Optional[SalaryType]):
    salary_min, salary_max, salary_type = None, None, None
    text = salary_text.replace(",", "").lower()

    # 月薪
    match_monthly = re.search(r"月薪([0-9,]+)(?:元)?(?:[至~])?([0-9,]+)?元?", text) or re.search(
        r"月薪([0-9,]+)元以上", text
    )
    if match_monthly:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly.group(1).replace(",", ""))
        if match_monthly.group(2):
            salary_max = int(match_monthly.group(2).replace(",", ""))
        return salary_min, salary_max, salary_type

    # 年薪
    match_yearly = re.search(r"年薪([0-9,]+)萬(?:[至~])?([0-9,]+)?萬?", text) or re.search(
        r"年薪([0-9,]+)萬以上", text
    )
    if match_yearly:
        salary_type = SalaryType.YEARLY
        salary_min = int(match_yearly.group(1).replace(",", "")) * 10000
        if match_yearly.group(2):
            salary_max = int(match_yearly.group(2).replace(",", "")) * 10000
        return salary_min, salary_max, salary_type

    # 時薪
    match_hourly = re.search(r"時薪([0-9,]+)元", text)
    if match_hourly:
        salary_type = SalaryType.HOURLY
        salary_min = int(match_hourly.group(1).replace(",", ""))
        salary_max = int(match_hourly.group(1).replace(",", ""))
        return salary_min, salary_max, salary_type

    # 日薪
    match_daily = re.search(r"日薪([0-9,]+)元", text)
    if match_daily:
        salary_type = SalaryType.DAILY
        salary_min = int(match_daily.group(1).replace(",", ""))
        salary_max = int(match_daily.group(1).replace(",", ""))
        return salary_min, salary_max, salary_type

    # 論件計酬
    if "論件計酬" in text:
        salary_type = SalaryType.BY_CASE
        return None, None, salary_type

    # 面議
    if "面議" in text:
        salary_type = SalaryType.NEGOTIABLE
        return None, None, salary_type

    return salary_min, salary_max, salary_type


def parse_yes123_job_data_to_pydantic(html_content: str, url: str) -> Optional[JobPydantic]:
    """
    從 yes123 職缺頁面的 HTML 內容解析並轉換為 JobPydantic 物件。
    """
    try:
        # yes123 的 job_id 通常是 URL 中的 p_id 參數
        job_id_match = re.search(r'p_id=(\d+)', url)
        job_id = job_id_match.group(1) if job_id_match else None

        if not job_id:
            logger.error("Failed to extract job_id from URL for parsing.", url=url)
            return None

        soup = BeautifulSoup(html_content, 'html.parser')

        # --- Extracting data based on yes123_人力銀行_crawl.ipynb ---
        # This part needs careful mapping from the notebook's parsing logic

        # Title
        title_element = soup.select_one('#content > div.job_content > div.job_title > h1')
        title = title_element.get_text(strip=True) if title_element else None

        # Company Name and URL
        company_name_element = soup.select_one('#content > div.job_content > div.job_title > div.comp_name > a')
        company_name = company_name_element.get_text(strip=True) if company_name_element else None
        company_url = company_name_element['href'] if company_name_element and 'href' in company_name_element.attrs else None
        # yes123 doesn't seem to have a direct company_source_id in the detail page HTML
        company_source_id = None

        # Job Details Table (工作條件)
        job_detail_table = soup.select_one('#content > div.job_content > div.job_detail > div.job_detail_content > div.job_detail_table')
        job_detail_map = {}
        if job_detail_table:
            for row in job_detail_table.find_all('tr'):
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True)
                    value = td.get_text(strip=True)
                    job_detail_map[key] = value

        # Description (工作內容)
        description_element = soup.select_one('#content > div.job_content > div.job_detail > div.job_detail_content > div:nth-child(1) > div.job_detail_content_text')
        description = description_element.get_text(separator='\n', strip=True) if description_element else None

        # 補充 description: 如果 description 為空，嘗試從其他地方獲取
        if not description:
            # 嘗試從其他可能的元素中提取描述，例如 job_detail_map 中的某些鍵
            # 這裡需要根據 yes123 網頁的實際結構來判斷
            # 這裡的 job_detail_map 已經在上面初始化並填充，所以可以使用
            description = job_detail_map.get('工作內容') or job_detail_map.get('職務說明')

        # Extracting from job_detail_map
        salary_text = job_detail_map.get('薪資待遇')
        salary_min, salary_max, salary_type = parse_salary(salary_text or "")

        job_type_str = job_detail_map.get('工作性質')
        job_type = JOB_TYPE_MAPPING_YES123.get(job_type_str) if job_type_str else None
        if job_type is None:
            job_type = JobType.OTHER # Default to OTHER if not mapped

        location_text = job_detail_map.get('工作地點')
        experience_required_text = job_detail_map.get('工作經驗')
        education_required_text = job_detail_map.get('學歷要求')

        # 將 None 轉換為 "不拘"
        if experience_required_text is None:
            experience_required_text = "不拘"
        if education_required_text is None:
            education_required_text = "不拘"

        # Posted At (發佈日期)
        posted_at = None
        posted_at_text = job_detail_map.get('刊登日期')
        if posted_at_text:
            try:
                # Assuming format like '2025/07/30'
                posted_at = datetime.strptime(posted_at_text, "%Y/%m/%d")
            except ValueError:
                logger.warning("Could not parse posted_at date format.", posted_at_text=posted_at_text, job_id=job_id)

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_YES123,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=company_source_id,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing yes123 job HTML to Pydantic.",
            error=e,
            url=url,
            exc_info=True,
        )
        return None


@app.task()
def fetch_url_data_yes123(url: str) -> Optional[dict]:
    job_id = None
    try:
        job_id_match = re.search(r'p_id=(\d+)', url)
        job_id = job_id_match.group(1) if job_id_match else None

        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        html_content = fetch_yes123_job_data(url)
        if html_content is None:
            logger.error("Failed to fetch job data from yes123 web.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        job_pydantic_data = parse_yes123_job_data_to_pydantic(html_content, url)

        if not job_pydantic_data:
            logger.error(
                "Failed to parse job data to Pydantic.",
                job_id=job_id,
                url=url,
            )
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        upsert_jobs([job_pydantic_data])

        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when processing yes123 job data.",
            error=e,
            job_id=job_id if 'job_id' in locals() else "N/A",
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    # python -m crawler.project_yes123.task_jobs_yes123
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]
    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_YES123,
        statuses=statuses_to_fetch,
        limit=10,
    )
    for url in urls_to_process:
        logger.info("Starting to process URL from the database.", url=url)
        fetch_url_data_yes123(url)


================================================
FILE: crawler/project_yes123/task_urls_yes123.py
================================================
import os
# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
# --- End Local Test Environment Setup ---

import structlog
from collections import deque
from bs4 import BeautifulSoup
import re
import urllib.parse
from crawler.worker import app
from crawler.database.schemas import SourcePlatform, UrlCategoryPydantic, CategorySourcePydantic
from crawler.database.repository import upsert_urls, upsert_url_categories, get_all_categories_for_platform
from crawler.project_yes123.client_yes123 import fetch_yes123_job_urls
from crawler.config import URL_CRAWLER_UPLOAD_BATCH_SIZE
from crawler.project_yes123.config_yes123 import (
    URL_CRAWLER_ORDER_BY_YES123,
    JOB_DETAIL_BASE_URL_YES123,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_yes123_category_urls(job_category: dict, url_limit: int = 0) -> None:
    """
    Celery task: Iterates through all pages of a specified yes123 job category, fetches job URLs,
    and stores them in the database.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    global_job_url_set = set()
    current_batch_urls = []
    current_batch_url_categories = []
    recent_counts = deque(maxlen=4)

    current_page = 1

    logger.info(
        "Task started: crawling yes123 job category URLs.", job_category_code=job_category_code, url_limit=url_limit
    )

    while True:
        if url_limit > 0 and len(global_job_url_set) >= url_limit:
            logger.info("URL limit reached. Ending task early.", job_category_code=job_category_code, url_limit=url_limit, collected_urls=len(global_job_url_set))
            break

        if current_page % 5 == 1:
            logger.info(
                "Current page being processed.",
                page=current_page,
                job_category_code=job_category_code,
            )

        html_content = fetch_yes123_job_urls(
            KEYWORDS="",
            CATEGORY=job_category_code,
            ORDER=URL_CRAWLER_ORDER_BY_YES123,
            PAGE_NUM=current_page,
        )

        if html_content is None:
            logger.error(
                "Failed to retrieve job URLs from yes123.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        soup = BeautifulSoup(html_content, 'html.parser')
        job_links_elements = soup.find_all('a', href=re.compile(r'job_detail\.asp\?p_id='))

        if not job_links_elements:
            logger.info("No job links found on this page. Checking for next page.", page=current_page, job_category_code=job_category_code)
            next_page_link = soup.find('a', text=re.compile(r'下一頁|Next'))
            if not next_page_link:
                logger.info("No next page link found. Ending task.", job_category_code=job_category_code)
                break
            current_page += 1
            continue

        for link_element in job_links_elements:
            job_link_suffix = link_element.get('href')
            if job_link_suffix:
                full_job_link = urllib.parse.urljoin(JOB_DETAIL_BASE_URL_YES123, job_link_suffix)
                if full_job_link not in global_job_url_set:
                    global_job_url_set.add(full_job_link)
                    current_batch_urls.append(full_job_link)
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=full_job_link,
                        source_category_id=job_category_code,
                    ).model_dump()
                )

        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "Batch upload size reached. Starting URL and URL-Category upload.",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_urls(SourcePlatform.PLATFORM_YES123, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            current_batch_urls.clear()
            current_batch_url_categories.clear()

        total_jobs = len(global_job_url_set)
        recent_counts.append(total_jobs)
        if len(recent_counts) == recent_counts.maxlen and len(set(recent_counts)) == 1:
            logger.info(
                "No new data found consecutively. Ending task early.",
                max_len=recent_counts.maxlen,
                job_category_code=job_category_code,
            )
            break

        current_page += 1

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining raw job URLs to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_urls(SourcePlatform.PLATFORM_YES123, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No URLs collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code)


if __name__ == "__main__":
    # python -m crawler.project_yes123.task_urls_yes123
    
    # --- Database Initialization for Local Test ---
    from crawler.database.connection import initialize_database
    initialize_database()
    # --- End Database Initialization ---

    job_category_lists = get_all_categories_for_platform(SourcePlatform.PLATFORM_YES123)

    for job_category in job_category_lists:
        logger.info("Dispatching crawl_and_store_yes123_category_urls task for local testing.", job_category_code=job_category.source_category_id)
        crawl_and_store_yes123_category_urls(job_category.model_dump())


================================================
FILE: crawler/project_yes123/yes123_人力銀行_crawl.ipynb
================================================
Error processing notebook: Invalid JSON in notebook: /home/soldier/crawler_system/crawler/project_yes123/yes123_人力銀行_crawl.ipynb


================================================
FILE: crawler/utils/salary_parser.py
================================================
import re

def parse_salary_text(salary_text):
    """
    Parses a salary text string and extracts min/max salary values.
    Returns a tuple (min_salary, max_salary).
    Returns (None, None) if parsing fails.
    """
    if not salary_text:
        return None, None

    # Clean the input string: remove commas, '元', '約', '以上', '起' etc.
    cleaned_text = salary_text.replace(',', '').replace('元', '').replace('約', '').strip()

    # 1. Try to parse as a range (e.g., "40000~70000", "40000-70000", "40000到70000")
    range_match = re.search(r'(\d+)\s*[~-到]\s*(\d+)', cleaned_text)
    if range_match:
        min_val = int(range_match.group(1))
        max_val = int(range_match.group(2))
        return min_val, max_val

    # 2. Try to parse with "以上" (e.g., "55000以上")
    above_match = re.search(r'(\d+)\s*以上', cleaned_text)
    if above_match:
        min_val = int(above_match.group(1))
        return min_val, None # Max is open-ended

    # 3. Try to parse with '萬' (e.g., "4萬", "3萬5", "4萬元或以上")
    # This regex handles "X萬" and "X萬Y" where Y is optional
    wan_match = re.search(r'(\d+)\s*萬(\d*)', cleaned_text)
    if wan_match:
        base_val = int(wan_match.group(1)) * 10000
        if wan_match.group(2): # If there's a second part like '5' in '3萬5'
            # Assuming 'X萬Y' means X*10000 + Y*1000 (e.g., 3萬5 = 35000)
            # Or it could be X*10000 + Y (e.g., 3萬500 = 35000)
            # Let's assume Y is in thousands if it's a single digit, otherwise it's the full number
            try:
                second_part = int(wan_match.group(2))
                if second_part < 10: # Likely a single digit like '5' in '3萬5'
                    base_val += second_part * 1000
                else: # Likely a full number like '500' in '3萬500'
                    base_val += second_part
            except ValueError:
                pass # No valid second part number
        return base_val, None # Max is open-ended for '萬' format unless specified

    # 4. Try to parse as a single number (e.g., "45000")
    single_match = re.search(r'(\d+)', cleaned_text)
    if single_match:
        single_val = int(single_match.group(1))
        return single_val, None

    return None, None # Could not parse



================================================
FILE: docs/development_manual.md
================================================
# Crawler System 開發手冊

## 1. 總體哲學 (Philosophy)

本專案所有開發與重構工作，應遵循以下核心哲學：

- **清晰性 (Clarity)**：程式碼首先是寫給人看的，其次才是給機器執行的。優先選擇清晰、易於理解的寫法，避免過度炫技或使用晦澀的語法。
- **單一職責 (Single Responsibility)**：每個模組、每個類別、每個函式都應該只有一個明確的職責。這使得程式碼更容易測試、重用和維護。
- **穩定性 (Robustness)**：應用程式應具備容錯能力，並透過**嚴格的測試**來確保其穩定性。對於外部依賴（如資料庫、訊息佇列），必須有適當的重試和錯誤處理機制。
- **配置外部化 (Externalized Configuration)**：程式碼本身不應包含任何環境特定的設定（如密碼、主機位址）。所有設定應透過外部設定檔管理。

---

## 2. 環境設定 (Environment Setup)

### 2.1. 必要工具

- **Python**: 版本定義於 `.python-version`。
- **uv**: 用於管理 Python 虛擬環境和套件。
- **Docker & Docker Compose**: 用於啟動外部服務（MySQL, RabbitMQ）。

### 2.2. 初始化步驟

1.  **初始化專案 (使用 uv)**:
    ```bash
    uv init
    ```

2.  **啟動基礎服務**: 
    ```bash
    # 啟動 MySQL 和 RabbitMQ 服務
    docker-compose -f mysql-network.yml up -d
    docker-compose -f rabbitmq-network.yml up -d
    ```

3.  **建立虛擬環境並安裝依賴**: 
    ```bash
    # 建立 .venv 虛擬環境
    uv venv

    # 啟用虛擬環境
    source .venv/bin/activate

    # 安裝專案依賴
    uv pip install -r requirements.txt
    ```

3.  **設定環境變數**: 
    複製 `local.ini.example` (如果有的話) 為 `local.ini`，並根據本地開發需求修改。`APP_ENV` 環境變數用於切換不同的設定區塊。

---

## 3. 設定檔管理 (Configuration)

- **`local.ini`**: 這是唯一的設定來源 (Single Source of Truth)。它被分為不同的區塊，例如 `[DEV]`, `[DOCKER]`, `[PROD]`。
- **`crawler/config.py`**: 這是讀取 `local.ini` 的唯一模組。專案中任何其他地方需要設定值時，都應該**直接從 `crawler.config` 匯入**，而不是自己重新讀取 `.ini` 檔案。
- **`APP_ENV` 環境變數**: 這個環境變數決定了 `config.py` 要讀取 `local.ini` 中的哪一個區塊。預設為 `DOCKER`。

### 3.1. 核心爬蟲設定

以下是 `local.ini` 中一些影響爬蟲行為的關鍵設定：

-   **`PRODUCER_BATCH_SIZE`**:
    -   **作用**: 限制 Producer (例如 `producer_jobs_104`) 每次從資料庫讀取並分發的任務數量。
    -   **用途**: 在本地測試時，可以設定較小的值 (例如 `20`)，以控制單次測試的資料量，避免一次性處理過多任務。在生產環境中，可以根據系統資源和任務量設定較大的值。

-   **`PRODUCER_DISPATCH_INTERVAL_SECONDS`**:
    -   **作用**: 設定**持續運行**的 Producer (使用 `while True` 迴圈) 在分發完一批任務後，等待多久才開始下一批。
    -   **用途**: 用於控制常駐型 Producer 的任務分發頻率。對於**排程驅動、單次執行**的 Producer (如目前的 `producer_jobs_104`)，此設定**無效**，因為腳本執行完畢後即會終止。

-   **`URL_CRAWLER_SLEEP_MIN_SECONDS` / `URL_CRAWLER_SLEEP_MAX_SECONDS`**:
    -   **作用**: 定義 Worker 在每次發送 API 請求前，隨機暫停的最小和最大秒數。
    -   **用途**: 這是**避免被目標網站 API 封鎖的關鍵設定**。透過引入隨機延遲，模擬人類的瀏覽行為，降低請求頻率，有效規避 `429 Too Many Requests` 等反爬機制。在測試或生產環境中，應根據目標網站的限制策略進行調整。

---

## 4. 核心編碼原則 (Core Coding Principles)

### 4.1. 單一職責原則 (SRP)

- **模組層級**: `crawler/database/connection.py` 只負責資料庫連接，`crawler/config.py` 只負責設定讀取。
- **函式層級**: 一個函式只做一件事情。例如，`get_engine` 只負責取得引擎，而不應該包含建立資料庫的邏輯。

### 4.2. DRY (Don't Repeat Yourself)

- **避免重複程式碼**: 如果一段邏輯在兩個以上的地方出現，就應該將它抽像成一個函式或類別。
- **範例**: 專案中所有讀取設定的地方都應從 `crawler.config` 匯入，這就是 DRY 的體現。

### 4.3. 日誌記錄 (Logging)

- **使用 `structlog`**: 全面使用 `structlog` 進行結構化日誌記錄，而不是 `print()`。
- **日誌級別**:
    - `logger.debug()`: 用於開發時的詳細除錯資訊。
    - `logger.info()`: 用於記錄關鍵的業務流程節點（例如「服務啟動」、「收到新任務」）。
    - `logger.warning()`: 用於記錄可預期的、但需要注意的異常情況（例如「設定檔缺少某個非關鍵值，使用預設值」）。
    - `logger.error()`: 用於記錄發生了錯誤，但應用程式仍可繼續運行的情況（例如「處理單一任務失敗，但 worker 會繼續接收下一個任務」）。
    - `logger.critical()`: 用於記錄導致應用程式無法繼續運行的致命錯誤（例如「資料庫連接失敗」）。
- **包含上下文**: 在記錄日誌時，盡可能帶上關鍵的上下文資訊，例如 `logger.info("任務處理完成", task_id=123, duration_ms=500)`。

---

## 5. 測試策略 (Testing Strategy)

本專案高度重視程式碼品質與穩定性，因此測試是開發流程中不可或缺的一環。所有程式碼在提交前都必須經過適當的測試。

### 5.1. 本地測試 (Local Testing)

為了方便在本地開發環境中快速驗證 `task` 的功能，我們採用了基於環境變數的測試模式。

- **核心機制**: 透過在執行腳本前設定 `CRAWLER_DB_NAME` 環境變數，可以讓應用程式連線到指定的測試資料庫（例如 `test_db`），而不是 `local.ini` 中設定的預設資料庫。
- **標準實踐**: 在每個 `task_*.py` 檔案的頂部，都加入了以下標準化的程式碼區塊：
  ```python
  import os
  if __name__ == "__main__":
      os.environ['CRAWLER_DB_NAME'] = 'test_db'
  ```
- **如何運作**: 當你使用 `python -m crawler.project_xxx.task_yyy` 執行一個 `task` 檔案時，這個區塊會被觸發，從而確保所有後續的資料庫操作都發生在 `test_db` 中。這使得本地測試既簡單又安全，完全不會影響到正式的開發資料庫。

### 5.2. 測試工具

-   **`pytest`**: 作為主要的測試框架，提供豐富的功能和靈活的擴展性。
-   **`unittest.mock`**: 用於單元測試中模擬外部依賴，確保測試的隔離性。

### 5.3. 測試流程與規範

1.  **編寫測試**：為新功能或修復的 Bug 編寫相應的測試案例。
2.  **運行測試**：在提交程式碼前，必須運行所有相關測試，並確保其通過。
    ```bash
    # 運行所有測試 (如果配置正確)
    python -m pytest

    # 運行特定測試檔案
    python -m pytest tests/path/to/your_test_file.py
    ```
3.  **測試覆蓋率**：鼓勵提高測試覆蓋率，但更重要的是測試的品質和有效性。

---

## 6. 資料庫互動 (Database Interaction)

本專案的資料庫互動遵循清晰的結構化原則，以確保程式碼的可維護性和資料的完整性。

### 6.1. 模組職責

- **`connection.py`**: 唯一的資料庫連線管理模組。它會根據 `CRAWLER_DB_NAME` 環境變數自動判斷應連線至正式資料庫還是測試資料庫。所有資料庫 Session 的取得都必須透過此模組的 `get_session()`。
- **`models.py`**: 只包含 SQLAlchemy 的 ORM 模型定義（例如 `Job`, `Url` 等），負責定義資料庫的資料表結構。
- **`schemas.py`**: 只包含 Pydantic 的資料驗證模型（例如 `JobPydantic`, `UrlPydantic`），負責定義應用程式內部流動的資料結構，並提供資料驗證。
- **`repository.py`**: 資料庫操作的唯一入口（Repository Pattern）。所有對資料庫的 CRUD (Create, Read, Update, Delete) 操作都應封裝在此模組的函式中。

### 6.2. ORM 與 Session 管理

-   **使用 `get_session`**: 所有對資料庫的讀寫操作，都必須透過 `crawler.database.connection.get_session` 的上下文管理器來完成。
    ```python
    from crawler.database.connection import get_session
    from crawler.database.models import MyORMModel
    from crawler.database.schemas import MyPydanticModel

    with get_session() as session:
        # 從資料庫讀取資料後，應立即轉換為 Pydantic 模型
        orm_object = session.query(MyORMModel).first()
        if orm_object:
            pydantic_instance = MyPydanticModel.model_validate(orm_object)
            # 現在你可以安全地使用 pydantic_instance

        # 寫入資料庫時，應使用 Pydantic 模型定義的資料
        new_data = MyPydanticModel(field1="value1", field2="value2")
        session.add(MyORMModel(**new_data.model_dump())) # 將 Pydantic 轉換為 ORM 可接受的格式
        # session.commit() 和 session.rollback() 會由 get_session 自動處理
    ```
-   **`tb_jobs` 唯一約束**: `tb_jobs` 表格現在包含 `(source_platform, url)` 的唯一約束。這確保了對於同一平台和 URL 的職缺，資料庫中只會保留一筆最新記錄，避免重複數據。當插入重複的 `(source_platform, url)` 組合時，現有記錄將會被更新。
-   **禁止直接使用 `engine.execute()`**: 除非是像 `initialize_database` 這樣的一次性管理腳本，否則業務邏輯中應避免直接使用 `engine`。

### 6.3. 爬取狀態生命週期 (Crawl Status Lifecycle)

為了確保任務不被重複抓取且具備重試能力，URL 的爬取狀態遵循以下生命週期：

1.  **`PENDING`**:
    -   **定義**: URL 已被收集，等待 Producer 分發。這是 URL 的初始狀態。
    -   **觸發**: `producer_category_*` 或 `producer_urls_*` 首次將 URL 存入資料庫時。

2.  **`QUEUED`**:
    -   **定義**: Producer 已從資料庫讀取此 URL，並準備將其作為任務發送到訊息佇列 (RabbitMQ)。
    -   **觸發**: `producer_jobs_*` 讀取到 `PENDING` 或 `FAILED` 的 URL 後，會**立即**將其狀態更新為 `QUEUED`，以防止其他 Producer 實例重複選取。

3.  **`PROCESSING`**:
    -   **定義**: Worker 已從訊息佇列接收到任務，正在進行資料抓取和處理。
    -   **觸發**: `worker.py` 中的 `fetch_url_data_*` 任務開始執行時，會將 URL 狀態更新為 `PROCESSING`。

4.  **`SUCCESS`**:
    -   **定義**: Worker 已成功完成資料抓取和儲存。
    -   **觸發**: `fetch_url_data_*` 任務成功執行完畢。

5.  **`FAILED`**:
    -   **定義**: Worker 在處理過程中遇到錯誤 (例如 API 請求失敗、資料驗證錯誤)。
    -   **觸發**: `fetch_url_data_*` 任務執行期間發生異常。失敗的任務將在未來的某個時間點由 Producer 重新選取並分發。

這個狀態機能夠確保系統的穩定性和資料處理的原子性。

### 6.4. 地理編碼處理 (Geocoding Processing)

地理編碼任務現在是獨立於職缺資料抓取和儲存流程的。這意味著 `task_jobs_*.py` 任務不再直接觸發地理編碼。你需要另外建立一個任務來處理地理編碼，例如一個定時任務，它會從資料庫中讀取尚未進行地理編碼的職缺，然後呼叫 `geocode_job_location` 進行處理。

### 6.5. 透過 Pandas 直接連線資料庫 (僅限讀取或特定用途)

在某些特定場景下，例如進行資料分析或快速查詢時，你可能希望直接透過 Pandas 連線到資料庫。此時，你可以使用 `sqlalchemy` 的 `create_engine` 搭配專案的設定來建立連接。

**注意**：這種方式通常用於讀取資料，對於寫入操作，仍建議使用 `get_session` 和 ORM，並透過 Pydantic 模型來確保事務的完整性和一致性。

1.  **安裝必要套件**: 
    ```bash
    uv pip install pandas sqlalchemy mysql-connector-python
    ```

2.  **範例程式碼**: 
    ```python
    import pandas as pd
    from sqlalchemy import create_engine
    from crawler.config import (
        MYSQL_HOST,
        MYSQL_PORT,
        MYSQL_ACCOUNT,
        MYSQL_PASSWORD,
        MYSQL_DATABASE,
    )

    # 建立資料庫連接 URL
    # 使用 mysql+pymysql 驅動
    db_url = (
        f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}"
    )

    # 建立 SQLAlchemy 引擎
    engine = create_engine(db_url)

    try:
        # 使用 pandas 讀取資料表
        # 替換 'your_table_name' 為你實際的資料表名稱
        df = pd.read_sql("SELECT * FROM your_table_name", engine)

        # 顯示 DataFrame 的前幾行
        print(df.head())

    except Exception as e:
        print(f"連線或查詢時發生錯誤: {e}")

    finally:
        # 關閉引擎連接池
        engine.dispose()
    ```

---

## 7. 執行爬蟲 (Running the Crawler)

為了確保 Python 的 `import` 路徑正確，應從專案根目錄使用 `-m` 參數來執行模組。

- **啟動 Producer**:
  ```bash
   # 啟動 104 爬蟲的 Producer
   python -m crawler.project_104.producer_category_104
   python -m crawler.project_104.producer_urls_104
   python -m crawler.project_104.producer_jobs_104

   # 啟動 1111 爬蟲的 Producer
   python -m crawler.project_1111.producer_category_1111
   python -m crawler.project_1111.producer_urls_1111
   python -m crawler.project_1111.producer_jobs_1111

   # 啟動 CakeResume 爬蟲的 Producer
   python -m crawler.project_cakeresume.producer_category_cakeresume
   python -m crawler.project_cakeresume.producer_urls_cakeresume
   python -m crawler.project_cakeresume.producer_jobs_cakeresume

   # 啟動 yes123 爬蟲的 Producer
   python -m crawler.project_yes123.producer_category_yes123
   python -m crawler.project_yes123.producer_urls_yes123
   python -m crawler.project_yes123.producer_jobs_yes123
  ```
- **啟動 Worker**:
  ```bash
   celery -A crawler.worker worker --loglevel=info
  ```

---

## 8. 程式碼風格與檢查 (Linting & Formatting)

為了確保程式碼的一致性、可讀性和品質，本專案強制執行自動化的程式碼風格檢查和格式化。

### 8.1. 為什麼需要程式碼風格與檢查？

-   **提高可讀性**：統一的風格讓所有開發者更容易閱讀和理解程式碼。
-   **減少錯誤**：Linter 可以捕捉潛在的錯誤、不一致的行為和不良的程式碼實踐。
-   **加速開發**：減少程式碼審查中關於風格的討論，讓開發者專注於業務邏輯。
-   **自動化**：透過工具自動執行，減少人工干預。

### 8.2. 推薦工具

-   **`ruff` (Linter & Formatter)**: 一個極速的 Python Linter 和 Formatter，旨在取代 `Flake8`, `isort`, `pylint`, `black` 等多個工具，提供統一的程式碼檢查和格式化體驗。

### 8.3. 安裝與使用

請確保你的虛擬環境已啟用。

1.  **安裝工具**:
    ```bash
    uv pip install ruff
    ```

2.  **配置 `ruff`**:
    `ruff` 的配置通常放在 `pyproject.toml` 中。請確保 `pyproject.toml` 中包含以下或類似的配置：
    ```toml
    [tool.ruff]
    line-length = 120
    select = ["E", "F", "W", "I", "N", "D", "UP", "ANN", "ASYNC", "B", "C4", "DTZ", "ERA", "ISC", "ICN", "PIE", "PT", "RSE", "RET", "SIM", "TID", "ARG", "PLC", "PLE", "PLR", "PLW", "TRY", "PERF"]
    ignore = [] # ruff format 會處理行長度，所以不需要忽略 E501

    [tool.ruff.per-file-ignores]
    "__init__.py" = ["F401"] # 忽略 __init__.py 中未使用的 import 警告
    "tests/*" = ["S101"] # 忽略測試檔案中的 assert 警告
    ```
    （**注意**：上述 `select` 和 `ignore` 列表僅為範例，應根據專案實際需求進行調整。）

3.  **執行檢查與格式化**: 

    -   **格式化 (使用 `ruff`)**:
        ```bash
        ruff format .
        ```
        這會自動格式化專案中的所有 Python 檔案。

    -   **檢查 (使用 `ruff`)**:
        ```bash
        ruff check .
        ```
        這會檢查程式碼中的潛在問題。如果發現問題，`ruff` 會提供建議。

    -   **自動修復 (使用 `ruff`)**:
        ```bash
        ruff check . --fix
        ```
        `ruff` 可以自動修復大部分簡單的問題。

### 8.4. 開發流程整合

強烈建議在提交程式碼前執行 `ruff format .` 和 `ruff check . --fix`。

未來可以考慮整合 `pre-commit` hooks 或 CI/CD 流程，在程式碼提交或推送到遠端倉庫時自動執行這些檢查，以確保程式碼品質。



================================================
FILE: docs/project_104_docker_manual.md
================================================
# 專案 104 Docker 化操作手冊

本文件旨在說明如何將 `crawler_jobs` 專案打包成 Docker 映像檔，並如何使用 Docker Compose 管理整個爬蟲系統。

## 1. 前置準備 (Prerequisites)

請確保您的系統已安裝 Docker 和 Docker Compose (或 Docker CLI 內建的 `compose` 外掛)。

## 2. 環境設定 (Environment Setup)

### 2.1. 產生環境變數檔案 (`.env`)

專案使用 `local.ini` 來管理不同環境的設定。在 Docker 環境中，我們需要將 `local.ini` 中的 `[DOCKER]` 區塊設定轉換為 Docker Compose 可讀取的 `.env` 檔案。

在專案根目錄下執行：

```bash
APP_ENV=DOCKER python genenv.py
```

這將會根據 `local.ini` 中的 `[DOCKER]` 區塊產生一個 `.env` 檔案。

### 2.2. 建立共用網路

為了讓所有 Docker 容器能夠互相通訊，我們需要建立一個共用的 Docker 網路。

```bash
docker network create my_network || true
```
`|| true` 確保即使網路已存在也不會報錯。

## 3. Docker 映像檔建置 (Build)

我們使用多階段建置 (Multi-stage Build) 的 `Dockerfile` 來優化映像檔的大小和建置效率。

### 3.1. 建置指令

在專案的根目錄下，執行以下指令來建置映像檔：

```bash
# -f 指定 Dockerfile 的路徑
# -t 為映像檔命名並加上標籤 (tag)，格式為 <your-dockerhub-username>/<image-name>:<version>
# . 表示建置上下文 (build context) 為當前目錄
docker build -f Dockerfile -t benitorhuang/crawler_jobs:0.0.2 .
```

### 3.2. 驗證建置結果

建置完成後，你可以使用以下指令來查看本機的所有映像檔，確認 `benitorhuang/crawler_jobs:0.0.2` 是否已成功建立。

```bash
docker images
```

## 4. 啟動與管理服務 (Docker Compose)

我們使用 Docker Compose 來同時管理多個服務 (MySQL, RabbitMQ, Worker, Producer)。

### 4.1. 啟動核心服務 (MySQL, RabbitMQ, Flower)

首先，啟動資料庫和訊息佇列服務。這些服務通常會長時間運行。

```bash
# 啟動 MySQL 和 phpMyAdmin
docker compose -f mysql-network.yml up -d

# 啟動 RabbitMQ 和 Flower (Celery 監控工具)
docker compose -f rabbitmq-network.yml up -d
```
`-d` 參數表示在背景以分離模式 (detached mode) 執行。

### 4.2. 啟動應用服務 (Worker, Producer)

接下來，啟動我們的爬蟲應用服務。

#### 4.2.1. 啟動 Worker

Worker 是長時間運行的背景服務，它會持續監聽並處理來自 RabbitMQ 的任務。

```bash
# 啟動 Worker 服務
docker compose -f docker-compose-worker-network.yml up -d
```
**注意**：Worker 容器會監聽 `producer_jobs_104`、`producer_category_104`、`producer_urls_104`、`producer_jobs_1111`、`producer_category_1111`、`producer_urls_1111`、`producer_jobs_cakeresume`、`producer_category_cakeresume`、`producer_urls_cakeresume`、`producer_jobs_yes123`、`producer_category_yes123` 和 `producer_urls_yes123` 佇列。

#### 4.2.2. 執行 Producer

Producer 的職責是讀取資料庫中的 URL 並分派任務。它是一個短時間執行的腳本，執行完畢後容器就會停止。

```bash
# 啟動所有 Producer 服務
docker compose -f docker-compose-producer-network.yml up
```

**小量測試範例**：

如果你想進行小量測試，例如只抓取特定數量的 URL 或分類，可以透過修改 Producer 的 `command` 來實現。這需要你直接在 `docker-compose-producer-network.yml` 中修改對應服務的 `command` 行，或者在執行時覆蓋它。

**範例：只分派 1 個分類的 URL 抓取任務 (並限制每個任務只抓取 5 個 URL)**

修改 `docker-compose-producer-network.yml` 中 `producer_104_urls` 服務的 `command`：

```yaml
services:
  producer_104_urls:
    # ... 其他設定 ...
    command: python -m crawler.project_104.producer_urls_104 --limit 1 --url-limit 5
    # ... 其他設定 ...
```

然後運行：

```bash
docker compose -f docker-compose-producer-network.yml up producer_104_urls
```

**注意**：`--limit` 參數用於限制分派的分類數量，`--url-limit` 參數用於限制每個分類任務抓取的 URL 數量。

### 4.3. 查看服務日誌

你可以使用以下指令來查看特定服務的日誌輸出：

```bash
# 查看 Worker 服務的日誌
docker logs -f crawler_system-crawler_104-1

# 查看 Producer 服務的日誌 (如果它還在運行或剛結束)
docker logs -f crawler_system-producer_104_jobs-1
docker logs -f crawler_system-producer_104_category-1
docker logs -f crawler_system-producer_104_urls-1

# 查看 RabbitMQ 服務的日誌
docker logs -f crawler_system-rabbitmq-1
```
**提示**：`crawler_system-` 是 Docker Compose 預設的專案名稱前綴。服務名稱加上 `-1` 是容器的預設名稱。

### 4.4. 停止所有服務

當你完成測試或開發後，可以使用以下指令停止所有由 Docker Compose 啟動的服務：

```bash
# 停止並移除所有服務容器、網路和卷 (如果沒有被其他服務使用)
docker compose -f mysql-network.yml -f rabbitmq-network.yml -f docker-compose-worker-network.yml -f docker-compose-producer-network.yml down
```
**注意**：`down` 命令會停止並移除容器。如果你想保留容器，只停止它們，可以使用 `stop` 命令。

## 5. 推送至 Docker Hub (Optional)

如果你希望將此映像檔分享給團隊或部署到其他環境，可以將其推送到 Docker Hub。

```bash
# 首先登入 Docker Hub
docker login

# 推送映像檔
docker push benitorhuang/crawler_jobs:0.0.2
```

## 6. Docker 環境與本地環境差異

### 6.1. 服務間通訊

在 Docker 環境中，各服務（如 Producer, Worker, MySQL, RabbitMQ）透過 Docker 網路和服務名稱進行通訊。例如，RabbitMQ 的主機名設定為 `rabbitmq`，MySQL 的主機名設定為 `mysql`。

### 6.2. 環境變數管理

Docker 環境透過 `genenv.py` 腳本，根據 `local.ini` 中的 `[DOCKER]` 區塊生成 `.env` 檔案，供 Docker Compose 使用。這確保了 Docker 容器內部使用正確的服務位址和配置。

### 6.3. 執行方式

在 Docker 環境中，Producer、Worker 和 Celery 服務都是作為 Docker 容器運行，並由 Docker Compose 進行編排和管理。這與本地環境中直接透過 `python -m` 或 `celery -A` 命令執行的方式不同。

### 6.4. 資料庫連接

Docker 環境中的服務會連接到 Docker 網路中的 MySQL 容器。而本地環境中的 Python 腳本則會連接到 `local.ini` 中 `[DEV]` 區塊設定的 `127.0.0.1` 上的 MySQL 服務。


================================================
FILE: docs/project_104_local_test_plan.md
================================================
# Project 104 本地測試計畫

本文件旨在提供 `project_104` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_category_104
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 104 的類別資料。

### 測試 `task_urls_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_urls_104
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_jobs_104
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_category_104
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_104_categories` 任務被接收並成功執行。

### 測試 `producer_urls_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_urls_104
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_jobs_104
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_104` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```



================================================
FILE: docs/project_1111_local_test_plan.md
================================================
# Project 1111 本地測試計畫

本文件旨在提供 `project_1111` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_category_1111
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 1111 的類別資料。

### 測試 `task_urls_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_urls_1111
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_jobs_1111
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_category_1111
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_1111_categories` 任務被接收並成功執行。

### 測試 `producer_urls_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_urls_1111
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_1111_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_jobs_1111
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_1111` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```



================================================
FILE: docs/project_cakeresume_local_test_plan.md
================================================
# Project CakeResume 本地測試計畫

本文件旨在提供 `project_cakeresume` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_cakeresume.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_cakeresume.task_category_cakeresume
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 CakeResume 的類別資料。

### 測試 `task_urls_cakeresume.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_cakeresume.task_urls_cakeresume
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_cakeresume.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_cakeresume.task_jobs_cakeresume
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_cakeresume`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_cakeresume.producer_category_cakeresume
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_cakeresume_categories` 任務被接收並成功執行。

### 測試 `producer_urls_cakeresume`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_cakeresume.producer_urls_cakeresume
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_cakeresume_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_cakeresume`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_cakeresume.producer_jobs_cakeresume
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_cakeresume` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```



================================================
FILE: docs/project_yes123_local_test_plan.md
================================================
# Project yes123 本地測試計畫

本文件旨在提供 `project_yes123` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_category_yes123
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 yes123 的類別資料。

### 測試 `task_urls_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_urls_yes123
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_jobs_yes123
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_category_yes123
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_yes123_categories` 任務被接收並成功執行。

### 測試 `producer_urls_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_urls_yes123
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_yes123_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_jobs_yes123
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_yes123` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```


